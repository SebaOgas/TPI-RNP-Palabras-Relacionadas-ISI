<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/DeepLearning/4PyTorchAvanzado/1modelospersonalizados.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"/></a>
Modelos Personalizados en Pytorch

Hasta ahora, hemos introducido algunos conceptos básicos de machine learning, avanzando hacia modelos de deep learning completamente funcionales. Para llegar tan lejos tan rápido, recurrimos a Pytorch, un framework de Deep Learning, pero pasamos por alto los detalles más avanzados sobre cómo funciona.

En esta clase, abriremos el telón, profundizaremos en los componentes clave del cómputo en deep learning, a saber, la construcción de modelos, el acceso a los parámetros y la inicialización, el diseño de capas y bloques personalizados, la lectura y escritura de modelos en el disco y el aprovechamiento de las GPU para lograr resultados con aceleraciones espectaculares. Estos conocimientos nos llevarán de usuario final a usuario avanzado, brindándonos las herramientas necesarias para aprovechar los beneficios de un framework maduro de aprendizaje profundo mientras conservamos la flexibilidad para implementar modelos más complejos, ¡incluidos los que inventemos nosotros mismos!

Bloques

Cuando introdujimos las redes neuronales por primera vez, nos enfocamos en modelos lineales con una sola salida.

Aquí, todo el modelo consta de una sola neurona. Tenga en cuenta que una sola neurona
toma algún conjunto de entradas;
genera una salida escalar correspondiente; y
tiene un conjunto de parámetros asociados que pueden actualizarse para optimizar alguna función objetivo de interés.

Luego, una vez que comenzamos a pensar en redes con múltiples salidas, aprovechamos la aritmética vectorizada para caracterizar una capa completa de neuronas. Al igual que las neuronas individuales, las capas
toma algún conjunto de entradas;
genera una salida escalar correspondiente; y
tiene un conjunto de parámetros asociados que pueden actualizarse para optimizar alguna función objetivo de interés.

Cuando trabajamos con la regresión softmax, una sola capa era en sí misma el modelo. Sin embargo, incluso cuando posteriormente introdujimos los MLP, aún podíamos pensar que el modelo conservaba esta misma estructura básica.
toma algún conjunto de entradas;
genera una salida escalar correspondiente; y
tiene un conjunto de parámetros asociados que se pueden actualizar para optimizar alguna función objetiva de interés.



Si bien podría pensar que las neuronas, las capas y los modelos
darnos suficientes abstracciones para seguir con nuestro negocio,
Resulta que a menudo nos resulta conveniente hablar de componentes que son más grandes que una capa individual pero más pequeños que el modelo completo.

La mayoría de las arquitecturas muy populares en visión artificial,
Posee cientos de capas. Estas capas consisten en patrones repetidos de grupos de capas. Implementar una red de este tipo una capa a la vez puede volverse tedioso. Esta preocupación no es solo hipotética, tales patrones de diseño son comunes en la práctica.

Arquitecturas similares en las que las capas se organizan en varios patrones repetitivos ahora son omnipresentes en otros dominios, incluido el procesamiento del lenguaje natural y el habla.

Para implementar estas redes complejas, presentamos el concepto de una red neuronal bloque. ¡Un bloque podría describir una sola capa, un componente que consta de varias capas o el modelo completo en sí! Una ventaja de trabajar con la abstracción de bloques es que se pueden combinar en artefactos más grandes, a menudo recursivamente. Esto se ilustra en :numref:fig_blocks. Al definir el código para generar bloques de complejidad arbitraria bajo demanda, podemos escribir un código sorprendentemente compacto y aun así implementar redes neuronales complejas.



Desde el punto de vista de la programación, un bloque está representado por una clase. Cualquier subclase debe definir una función de propagación directa que transforme su entrada en salida y debe almacenar los parámetros necesarios. Tenga en cuenta que algunos bloques no requieren ningún parámetro en absoluto. Finalmente, un bloque debe poseer una función de retropropagación,
para fines de cálculo de gradientes. Afortunadamente, debido a la magia detrás de escena proporcionada por la diferenciación automática (introducida en :numref:sec_autograd) al definir nuestro propio bloque, solo debemos preocuparnos por los parámetros y la función de propagación hacia adelante.

Para comenzar, revisamos el código que usamos para implementar MLPs (:numref:secmlpconcise). El siguiente código genera una red con una capa oculta completamente conectada con 256 unidades y activación de ReLU, seguida de una capa de salida completamente conectada con 10 unidades (sin función de activación).

En este ejemplo, construimos nuestro modelo instanciando un nn.Sequential, con capas en el orden en que deberían ejecutarse pasadas como argumentos. En resumen, (nn.Sequential define un tipo especial de Módulo), la clase que presenta un bloque en PyTorch. Mantiene una lista ordenada de 'Módulos' constituyentes. Tenga en cuenta que cada una de las dos capas completamente conectadas es una instancia de la clase 'Lineal', que a su vez es una subclase de 'Módulo'. La función de propagación hacia adelante ("forward") también es notablemente simple: encadena cada bloque en la lista, pasando la salida de cada uno como entrada al siguiente.
Tenga en cuenta que hasta ahora, hemos estado invocando nuestros modelos a través de la construcción net(X) para obtener sus resultados. En realidad, esto es solo una abreviatura de net.call(X).

Un bloque personalizado

Quizás la forma más fácil de desarrollar la intuición sobre cómo funciona un bloque es implementar uno nosotros mismos. Antes de implementar nuestro propio bloque personalizado, resumimos brevemente la funcionalidad básica que debe proporcionar cada bloque:

Ingerir datos de entrada como argumentos para su función de propagación directa.
Generar una salida haciendo que la función de propagación directa devuelva un valor. Tenga en cuenta que la salida puede tener una forma diferente a la entrada. Por ejemplo, la primera capa completamente conectada en nuestro modelo anterior ingiere una entrada de dimensión 20 pero devuelve una salida de dimensión 256.
Calcular el gradiente de su salida con respecto a su entrada, al que se puede acceder a través de su función de backpropagation. Por lo general, esto sucede automáticamente.
Almacenar y proporcionar acceso a aquellos parámetros necesarios para ejecutar el cálculo de propagación directa.
Inicializar los parámetros del modelo según sea necesario.

En el siguiente fragmento, codificamos un bloque desde cero
correspondiente a un MLP con una capa oculta con 256 unidades ocultas y una capa de salida de 10 dimensiones. Tenga en cuenta que la clase MLP a continuación hereda de la clase Module que representa un bloque. Confiaremos en gran medida en las funciones de la clase principal, proporcionando solo nuestro propio constructor (la función init en Python) y la función forward de propagación hacia adelante.

Centrémonos primero en la función de propagación directa. Tenga en cuenta que toma X como entrada, calcula la representación oculta con la función de activación aplicada y emite sus logits. En esta implementación MLP, ambas capas son variables de instancia. Para ver por qué esto es razonable, imagine instanciar dos MLP, net1 y net2, y entrenarlos con diferentes datos. Naturalmente, los esperaríamos
para representar dos modelos aprendidos diferentes.

Creamos una instancia de las capas de MLP en el constructor y posteriormente invocamos estas capas en cada llamada a la función de propagación directa. Tenga en cuenta algunos detalles clave. Primero, nuestra función init personalizada invoca la función init de la clase principal a través de super().init(), ahorrándonos el dolor de volver a establecer el código repetitivo aplicable a la mayoría de los bloques. Luego instanciamos nuestras dos capas densas, asignándolas a self.hidden y self.out. Tenga en cuenta que, a menos que implementemos un nuevo operador, no debemos preocuparnos por la función de retropropagación o la inicialización de parámetros. El sistema generará estas funciones automáticamente. Probemos esto.

Una virtud clave de la abstracción de bloques es su versatilidad. Podemos hacer una subclase de un bloque para crear capas (como la clase de capa completamente conectada), modelos completos (como la clase MLP anterior) o varios componentes de complejidad intermedia. Explotamos esta versatilidad a lo largo de las siguientes clases y cursos, como cuando tratamos las redes neuronales convolucionales.

El Bloque Sequential

Ahora podemos echar un vistazo más de cerca a cómo funciona la clase Sequential. Recuerde que Sequential fue diseñado para conectar en cadena otros bloques. Para construir nuestro propio MySequential simplificado, solo necesitamos definir dos funciones clave:
Una función para agregar bloques uno por uno a una lista.
Una función de propagación directa para pasar una entrada a través de la cadena de bloques, en el mismo orden en que se agregaron.

La siguiente clase MySequential ofrece la misma funcionalidad que la clase Sequential predeterminada.

En el método init, agregamos cada bloque al diccionario ordenado modules uno por uno. Quizás se pregunte por qué cada Module posee un atributo modules y por qué lo usamos en lugar de simplemente definir una lista de Python nosotros mismos. En resumen, la principal ventaja de modules es que durante la inicialización de los parámetros de nuestro bloque, el sistema sabe buscar dentro del diccionario modules para encontrar subbloques cuyos parámetros también necesitan inicializarse.

Cuando se invoca la función de propagación directa de MySequential, cada bloque agregado se ejecuta en el orden en que se agregaron. Ahora podemos volver a implementar un MLP usando nuestra clase MySequential.

Tenga en cuenta que este uso de MySequential es idéntico al código que escribimos anteriormente para la clase Sequential.

Ejecutando código en la función de propagación directa

La clase Sequential facilita la construcción de modelos, permitiéndonos ensamblar nuevas arquitecturas sin tener que definir nuestra propia clase. Sin embargo, no todas las arquitecturas son simples cadenas serializadas. Cuando se requiera mayor flexibilidad, querremos definir nuestros propios bloques. Por ejemplo, podríamos querer ejecutar flujos de control de Python dentro de la función de propagación directa. Además, podríamos querer realizar operaciones matemáticas arbitrarias, que no se basan simplemente en capas de redes neuronales predefinidas.

Es posible que haya notado que hasta ahora, todas las operaciones en nuestras redes han actuado sobre las activaciones de nuestra red y sus parámetros. A veces, sin embargo, es posible que queramos incorporar términos que no son el resultado de capas anteriores ni parámetros actualizables. Los llamamos parámetros constantes. Digamos, por ejemplo, que queremos una capa que calcule la función , donde  es la entrada,  es nuestro parámetro y  es una constante especificada que no se actualiza durante la optimización. Así que implementamos una clase FixedHiddenMLP de la siguiente manera.

En este modelo FixedHiddenMLP, implementamos una capa oculta cuyos pesos (self.rand_weight) se inicializan aleatoriamente en la creación de instancias y luego son constantes. Este peso no es un parámetro del modelo y, por lo tanto, la retropropagación nunca lo actualiza. Luego, la red pasa la salida de esta capa "fija" a través de una capa completamente conectada.

Tenga en cuenta que antes de devolver la salida, nuestro modelo hizo algo inusual. Ejecutamos un ciclo while, probando con la condición de que su norma  sea mayor que  y dividiendo nuestro vector de salida por  hasta que satisfaga la condición. Finalmente, devolvimos la suma de las entradas en X. Hasta donde sabemos, ninguna red neuronal estándar realiza esta operación. Tenga en cuenta que esta operación en particular puede no ser útil en ninguna tarea del mundo real. Nuestro objetivo es solo mostrarle cómo integrar código arbitrario en el flujo de los cálculos de su red neuronal.

Capas personalizadas

Un factor detrás del éxito del aprendizaje profundo es la disponibilidad de una amplia gama de capas que se pueden componer de formas creativas para diseñar arquitecturas adecuadas para una amplia variedad de tareas. Por ejemplo, los investigadores han inventado capas específicamente para manejar imágenes, texto, recorrer datos secuenciales y realizar programación dinámica. Tarde o temprano, encontrará o inventará una capa que aún no existe en el marco de aprendizaje profundo. En estos casos, debe crear una capa personalizada. En esta sección, le mostramos cómo.

Capas sin Parámetros

Para empezar, construimos una capa personalizada que no tiene parámetros propios. La siguiente clase CenteredLayer simplemente
resta la media de su entrada. Para construirlo, simplemente necesitamos heredar de la clase base Layer e implementar la función forward

Verifiquemos que nuestra capa funcione según lo previsto alimentandola con algunos datos.

Ahora podemos incorporar nuestra capa como un componente en la construcción de modelos más complejos.

Como verificación de cordura adicional, podemos enviar datos aleatorios a través de la red y verificar que la media sea de hecho 0. Debido a que estamos tratando con números de coma flotante, aún podemos ver un número muy pequeño distinto de cero debido a la cuantización.

Capas con parámetros

Ahora que sabemos cómo definir capas simples, pasemos a definir capas con parámetros que se pueden ajustar a través del entrenamiento. Podemos usar la clase nn.Parameter para crear parámetros, que brinda algunas funciones básicas de mantenimiento. En particular, rige el acceso, la inicialización, el uso compartido, el guardado y la carga de los parámetros del modelo.
De esta forma, entre otros beneficios, no necesitaremos escribir rutinas de serialización personalizadas para cada capa personalizada.

Ahora implementemos nuestra propia versión de la capa densa. Recuerde que esta capa requiere dos parámetros, uno para representar los pesos y otro para el sesgo. En esta implementación, usamos la función de activación ReLU prefabricada. Esta capa requiere argumentos de entrada: in_units y units, que denotan el número de entradas y salidas, respectivamente.

A continuación, instanciamos la clase MyLinear
y acceder a sus parámetros.

También podemos construir modelos usando capas personalizadas.

Gestión de parámetros

Una vez que hemos elegido una arquitectura y establecido nuestros hiperparámetros, procedemos al ciclo de entrenamiento, donde nuestro objetivo es encontrar valores de parámetros que minimicen nuestra función de pérdida. Después del entrenamiento, necesitaremos estos parámetros para poder hacer futuras predicciones. Además, a veces desearemos extraer los parámetros para reutilizarlos en otro contexto, para guardar nuestro modelo en el disco para que pueda ejecutarse en otro software, o para examinarlo con la esperanza de obtener una comprensión científica.

La mayoría de las veces, podremos ignorar los detalles esenciales de cómo se declaran y manipulan los parámetros, confiando en marcos de aprendizaje profundo para hacer el trabajo pesado. Sin embargo, cuando nos alejamos de las arquitecturas apiladas con capas estándar, a veces necesitaremos entrar en la maleza de declarar y manipular parámetros. En esta sección, cubrimos lo siguiente:
Acceso a parámetros para depuración, diagnóstico y visualizaciones.
Inicialización de parámetros.
Compartir parámetros entre diferentes componentes del modelo.

Comenzamos enfocándonos en un MLP con una capa oculta.

Acceso a parámetros

Comencemos con cómo acceder a los parámetros de los modelos que ya conoce. Cuando un modelo se define a través de la clase Sequential, primero podemos acceder a cualquier capa indexando el modelo como si fuera una lista. Los parámetros de cada capa están convenientemente ubicados en su atributo. Podemos inspeccionar los parámetros de la segunda capa completamente conectada de la siguiente manera.

La salida nos dice algunas cosas importantes. Primero, esta capa completamente conectada contiene dos parámetros, correspondientes a los pesos y sesgos de esa capa, respectivamente. Ambos se almacenan como flotadores de precisión simples (float32). Tenga en cuenta que los nombres de los parámetros nos permiten identificar de forma única los parámetros de cada capa, incluso en una red que contiene cientos de capas.

Indexado de parámteros

Tenga en cuenta que cada parámetro se representa como una instancia de la clase de parámetro. Para hacer algo útil con los parámetros, primero debemos acceder a los valores numéricos subyacentes. Hay varias maneras de hacer esto. Algunos son más simples, mientras que otros son más generales. El siguiente código extrae el sesgo de la segunda capa de la red neuronal, que devuelve una instancia de la clase Parameter y además accede al valor de ese parámetro.

Los parámetros son objetos complejos que contienen valores, gradientes e información adicional. Es por eso que necesitamos solicitar el valor explícitamente. Además del valor, cada parámetro también nos permite acceder al gradiente. Debido a que aún no hemos invocado la retropropagación para esta red, se encuentra en su estado inicial.

Todos los parámetros a la vez

Cuando necesitamos realizar operaciones en todos los parámetros, acceder a ellos uno por uno puede volverse tedioso. La situación puede volverse especialmente difícil de manejar cuando trabajamos con bloques más complejos (por ejemplo, bloques anidados), ya que tendríamos que recurrir a todo el árbol para extraer los parámetros de cada subbloque. A continuación, demostramos el acceso a los parámetros de la primera capa densa frente al acceso a todas las capas.

Esto nos proporciona otra forma de acceder a los parámetros de la red de la siguiente manera.

Recopilación de parámetros de bloques anidados

Veamos cómo funcionan las convenciones de nomenclatura de parámetros si anidamos varios bloques uno dentro de otro. Para eso, primero definimos una función que produce bloques (una fábrica de bloques, por así decirlo) y luego los combinamos dentro de bloques aún más grandes.

Ahora que hemos diseñado la red,
veamos cómo está organizada.

Dado que las capas están anidadas jerárquicamente, también podemos acceder a ellas como si indexáramos a través de listas anidadas. Por ejemplo, podemos acceder al primer bloque principal, dentro de él al segundo sub-bloque, y dentro de este al sesgo de la primera capa, de la siguiente manera.

Inicialización de parámetros

Ahora que sabemos cómo acceder a los parámetros, veamos cómo inicializarlos correctamente. El marco de aprendizaje profundo proporciona inicializaciones aleatorias predeterminadas para sus capas. Sin embargo, a menudo queremos inicializar nuestros pesos de acuerdo con varios otros protocolos. El marco proporciona los protocolos más utilizados y también permite crear un inicializador personalizado.

De forma predeterminada, PyTorch inicializa las matrices de peso y sesgo de manera uniforme dibujando desde un rango que se calcula de acuerdo con la dimensión de entrada y salida. El módulo nn.init de PyTorch proporciona una variedad de métodos de inicialización preestablecidos.

Inicialización integrada

Comencemos llamando a los inicializadores incorporados. El siguiente código inicializa todos los parámetros de ponderación como variables aleatorias gaussianas con una desviación estándar de 0,01, mientras que los parámetros de sesgo se borran a cero.

También podemos inicializar todos los parámetros a un valor constante dado (por ejemplo, 1).

También podemos aplicar diferentes inicializadores para ciertos bloques. Por ejemplo, a continuación inicializamos la primera capa con el inicializador Xavier e inicializamos la segunda capa a un valor constante de 42.

Inicialización personalizada

A veces, el marco de aprendizaje profundo no proporciona los métodos de inicialización que necesitamos. En el siguiente ejemplo, definimos un inicializador para cualquier parámetro de peso  usando la siguiente distribución extraña:

Nuevamente, implementamos una función my_init para aplicar a net.

Tenga en cuenta que siempre tenemos la opción de configurar los parámetros directamente.

Parámetros vinculados

A menudo, queremos compartir parámetros en varias capas. Veamos cómo hacer esto con elegancia. A continuación, asignamos una capa densa y luego usamos sus parámetros específicamente para establecer los de otra capa.

Este ejemplo muestra que los parámetros de la segunda y tercera capa están vinculados. No solo son iguales, están representados por el mismo tensor exacto. Así, si cambiamos uno de los parámetros, el otro también cambia. Quizás se pregunte, cuando los parámetros están vinculados, ¿qué sucede con los gradientes? Dado que los parámetros del modelo contienen gradientes, los gradientes de la segunda capa oculta y la tercera capa oculta se suman durante la retropropagación.

Archivos de E/S

Hasta ahora, discutimos cómo procesar datos y cómo construir, entrenar y probar modelos de aprendizaje profundo. Sin embargo, en algún momento, con suerte, estaremos lo suficientemente contentos con los modelos aprendidos que querremos guardar los resultados para su uso posterior en varios contextos (quizás incluso para hacer predicciones en producción). Además, cuando se ejecuta un proceso de entrenamiento largo, la mejor práctica es guardar periódicamente los resultados intermedios (puntos de control) para garantizar que no perdamos varios días de cómputo si nos tropezamos con el enchufe de nuestro servidor. Por lo tanto, es hora de aprender a cargar y almacenar tanto vectores de peso individuales como modelos completos. Esta sección aborda ambos temas.

Cargando y guardando tensores

Para tensores individuales, podemos invocar directamente las funciones load y save para leerlos y escribirlos respectivamente. Ambas funciones requieren que proporcionemos un nombre, y save requiere como entrada la variable que se guardará.

Ahora podemos volver a leer los datos del archivo almacenado y cargarlos de vuelta en la memoria.

Podemos almacenar una lista de tensores y volver a leerlos en la memoria.

Incluso podemos escribir y leer un diccionario que mapea desde cadenas a tensores. Esto es conveniente cuando queremos leer o escribir todos los pesos en un modelo.

[Cargar y guardar parámetros del modelo]

Guardar vectores de peso individuales (u otros tensores) es útil, pero se vuelve muy tedioso si queremos guardar (y luego cargar) un modelo completo. Después de todo, es posible que tengamos cientos de grupos de parámetros esparcidos por todas partes. Por esta razón, el framework de deep learning proporciona funcionalidades integradas para cargar y guardar redes enteras. Un detalle importante a tener en cuenta es que esto guarda los parámetros del modelo y no todo el modelo. Por ejemplo, si tenemos un MLP de 3 capas, debemos especificar la arquitectura por separado. La razón de esto es que los propios modelos pueden contener código arbitrario, por lo que no se pueden serializar de forma natural. Por lo tanto, para restablecer un modelo, necesitamos generar la arquitectura en código y luego cargar los parámetros desde el disco. (Empecemos con nuestro familiar MLP.)

A continuación, almacenamos los parámetros del modelo como un archivo con el nombre "mlp.params".

Para recuperar el modelo, instanciamos un clon del modelo MLP original. En lugar de inicializar aleatoriamente los parámetros del modelo, leemos directamente los parámetros almacenados en el archivo.

Dado que ambas instancias tienen los mismos parámetros de modelo, el resultado computacional de la misma entrada X debería ser el mismo. Verifiquemos esto.