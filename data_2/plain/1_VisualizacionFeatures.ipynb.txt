<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/4TransferLearning/1_VisualizacionFeatures.ipynb"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>

Generalización de las Features Aprendidas por ImageNet

El impacto de ImageNet en el curso de la investigación del machine learning difícilmente puede exagerarse. El conjunto de datos se publicó originalmente en 2009 y evolucionó rápidamente hasta convertirse en ImageNet Large Scale Visual Recognition Challenge (ILSVRC). En 2012, la red neuronal profunda presentada por Alex Krizhevsky, Ilya Sutskever y Geoffrey Hinton se desempeñó un 41 % mejor que el siguiente mejor competidor, lo que demuestra que el deep learning era una estrategia viable para el machine learning y podría decirse que desencadenó la explosión en su investigación.

El éxito de ImageNet destacó que en la era del deep learning, los datos eran al menos tan importantes como los algoritmos. El conjunto de datos de ImageNet no solo permitió esa importante demostración de 2012 del poder del deep learning, sino que también permitió un avance de importancia similar en el transfer learning: los investigadores pronto se dieron cuenta de que los pesos aprendidos en modelos de última generación para ImageNet podrían usarse para inicializar modelos para conjuntos de datos completamente diferentes y mejorar significativamente el rendimiento. Este enfoque de "fine tuning" permitió lograr un buen desempeño con tan solo un ejemplo positivo por categoría.

Para determinar cómo sería una ImageNet para el lenguaje, primero tenemos que identificar qué hace que ImageNet sea buena para el transfer learning. Estudios previos solo han arrojado luz parcial sobre esta pregunta: reducir la cantidad de ejemplos por clase o la cantidad de clases solo da como resultado una pequeña caída en el rendimiento, mientras que las clases detalladas y más datos no siempre son mejores.

En lugar de mirar los datos directamente, es más prudente sondear lo que aprenden los modelos entrenados en los datos. Es bien sabido que las características de las redes neuronales profundas entrenadas en ImageNet pasan de lo general a lo específico de la tarea desde la primera hasta la última capa: las capas inferiores aprenden a modelar características de bajo nivel, como bordes, mientras que las capas superiores modelan conceptos de nivel superior, como patrones y partes u objetos completos como se puede ver en la siguiente figura.

Es importante destacar que el conocimiento de los bordes, las estructuras y la composición visual de los objetos es relevante para muchas tareas de CV, lo que arroja luz sobre por qué estas capas son transferibles. Una propiedad clave de un conjunto de datos como ImageNet es, por lo tanto, alentar a un modelo a aprender características que probablemente se generalizarán a nuevas tareas en el dominio del problema.

Más allá de esto, es difícil hacer más generalizaciones sobre por qué la transferencia desde ImageNet funciona tan bien. Por ejemplo, otra posible ventaja del conjunto de datos de ImageNet es la calidad de los datos. Los creadores de ImageNet hicieron todo lo posible para garantizar anotaciones confiables y consistentes. Sin embargo, el trabajo en supervisión a distancia sirve como contrapunto, lo que indica que grandes cantidades de datos débilmente etiquetados a menudo pueden ser suficientes. De hecho, recientemente los investigadores de Facebook demostraron que podían pre-entrenar un modelo prediciendo hashtags en miles de millones de imágenes de redes sociales con precisión de última generación en ImageNet.

Sin más ideas concretas, nos quedan dos propiedades deseables clave:
El dataset debe ser lo suficientemente grande, es decir, del orden de millones de ejemplos de entrenamiento.
Debe ser representativo del espacio de problemas de la disciplina.

Los resultados empíricos y teóricos en el aprendizaje de múltiples tareas indican que es probable que un sesgo que se aprende en un número suficiente de tareas se generalice a tareas invisibles extraídas del mismo entorno. Visto a través de la lente del aprendizaje multitarea, un modelo entrenado en ImageNet aprende una gran cantidad de tareas de clasificación binaria (una para cada clase). Es probable que estas tareas, todas extraídas del espacio de imágenes naturales del mundo real, sean representativas de muchas otras tareas de CV.

Visualización de las Features

Existe una sensación creciente de que las redes neuronales deben ser interpretables para los humanos. El campo de la interpretabilidad de redes neuronales se ha formado en respuesta a estas preocupaciones. A medida que madura, dos hilos principales de investigación han comenzado a fusionarse: la visualización de características y la atribución.

La visualización de características responde preguntas sobre qué es lo que las redes, o ciertas partes de las redes, buscan en una foto. Esto lo hacen mediante la generación de ejemplos.

La atribución estudia qué parte de una imágen es responsable de que la red se active de una manera particular.

Visualización mediante optimización

Las redes neuronales son, en términos generales, diferenciables con respecto a sus entradas. Si queremos averiguar qué tipo de entrada causaría un cierto comportamiento, ya sea una activación de una capa interna o el comportamiento de salida final, podemos usar derivadas para ajustar iterativamente la entrada hacia ese objetivo.

Independientemente de si estamos buscando la imágen dentro del dataset que mejor representa lo que espera un sector de la red o si estamos optimizando imagenes desde cero, la pregunta central que debemos hacernos para encontrar los ejemplos es qué sector de la red queremos analizar. Tenemos una gran variedad de opciones.

Si queremos comprender las features individuales, podemos buscar ejemplos en los que tengan valores altos, ya sea para una neurona en una posición individual o para un canal completo.

Si queremos entender una capa como un todo, podemos usar el objetivo DeepDream, buscando imágenes que la capa encuentre “interesantes”.

Y si queremos crear ejemplos de clases de salida a partir de un clasificador, tenemos dos opciones: optimizar los logits de clase antes del softmax u optimizar las probabilidades de clase después del softmax. Uno puede ver los logits como la evidencia para cada clase y las probabilidades como la probabilidad de cada clase dada la evidencia. Desafortunadamente, la forma más fácil de aumentar la probabilidad que softmax otorga a una clase es a menudo hacer que las alternativas sean poco probables en lugar de hacer que la clase de interés sea probable. Según nuestra experiencia, la optimización de los logits previos a softmax produce imágenes de mejor calidad visual. Si bien la explicación estándar es que maximizar la probabilidad no funciona muy bien porque simplemente puede reducir la evidencia para otras clases, una hipótesis alternativa es que es más difícil de optimizar a través de la función softmax.

¿Por qué visualizar por optimización?

La optimización puede darnos un ejemplo de entrada que provoque el exactamente el comportamiento deseado por la red, pero ¿por qué molestarse con eso? ¿No podríamos simplemente buscar en el conjunto de datos ejemplos que provoquen el comportamiento deseado?

Resulta que el enfoque de optimización puede ser una forma poderosa de comprender lo que realmente busca un modelo, porque separa las cosas que causan el comportamiento de las cosas que simplemente se correlacionan con las causas. Por ejemplo, considere las siguientes neuronas visualizadas con ejemplos de conjuntos de datos y optimización:

La optimización también tiene la ventaja de la flexibilidad. Por ejemplo, si queremos estudiar cómo las neuronas representan información de forma conjunta, podemos preguntarnos fácilmente cómo debería ser diferente un ejemplo en particular para que se active una neurona adicional. Esta flexibilidad también puede ser útil para visualizar cómo evolucionan las características a medida que se entrena la red. Si nos limitáramos a comprender el modelo en los ejemplos fijos en nuestro conjunto de datos, temas como estos serían mucho más difíciles de explorar.

Implementación

Microscope de OpenAI

Microscope es un sitio web que contiene una colección de visualizaciones de cada capa y neurona significativa de ocho "organismos modelo" de visión que a menudo se estudian en interpretabilidad. Microscope facilita el análisis de las features que se forman dentro de estas redes neuronales, con el objetivo de ayudar a la comunidad investigadora a avanzar hacia la comprensión de estos complicados sistemas.