<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/CV/6_Segmentacion/Segmentacion.ipynb"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>

Segmentación Semántica

Cuando hablamos de detección de objetos en la clase anterior, vimos que se usaban bounding boxes rectangulares para etiquetar y predecir objetos en las imágenes. En esta hablaremos de la segmentación semántica, que se enfoca en cómo dividir una imagen en regiones que pertenecen a diferentes clases semánticas. A diferencia de la detección de objetos, la segmentación semántica reconoce y comprende lo que hay en las imágenes a nivel de píxel: su etiquetado y predicción de regiones semánticas están a nivel de píxel. La siguiente figura muestra las etiquetas del perro, el gato y el fondo de la imagen en segmentación semántica. En comparación con la detección de objetos, los bordes a nivel de píxel etiquetados en la segmentación semántica son obviamente más detallados.

Segmentación de imágenes y segmentación de instancias

También hay dos tareas importantes en el campo de la visión artificial que son similares a la segmentación semántica, a saber, la segmentación de imágenes y la segmentación de instancias. Los distinguiremos brevemente de la segmentación semántica de la siguiente manera.
La segmentación de imágenes divide una imagen en varias regiones constituyentes. Los métodos para este tipo de problemas suelen hacer uso de la correlación entre píxeles de la imagen. No necesita información de etiqueta sobre los píxeles de la imagen durante el entrenamiento y no puede garantizar que las regiones segmentadas tengan la semántica que esperamos obtener durante la predicción.
La segmentación de instancias también se denomina detección y segmentación simultáneas. Estudia cómo reconocer las regiones a nivel de píxeles de cada instancia de objeto en una imagen. A diferencia de la segmentación semántica, la segmentación de instancias necesita distinguir no solo la semántica, sino también diferentes instancias de objetos. Por ejemplo, si hay dos perros en la imagen, la segmentación de instancias debe distinguir a cuál de los dos perros pertenece un píxel.

El Dataset de segmentación semántica Pascal VOC2012

Uno de los conjuntos de datos de segmentación semántica más importantes es Pascal VOC2012.
A continuación, vamos a echar un vistazo a este conjunto de datos.

The tar file of the dataset is about 2 GB,
so it may take a while to download the file.
The extracted dataset is located at ../data/VOCdevkit/VOC2012.

Después de ingresar a la carpeta data/VOCdevkit/VOC2012,
podemos ver los diferentes componentes del conjunto de datos.
La carpeta ImageSets/Segmentation contiene archivos de texto
que especifican ejemplos de entrenamiento y prueba, mientras que las carpetas JPEGImages y SegmentationClass almacenan la imagen de entrada y la etiqueta para cada ejemplo, respectivamente.

La etiqueta aquí también está en formato de imagen, con el mismo tamaño que su imagen de entrada etiquetada. Además, los píxeles con el mismo color en cualquier imagen de etiqueta pertenecen a la misma clase semántica. Lo siguiente define la función readvocimages para leer todas las imágenes y etiquetas de entrada en la memoria.

Dibujamos las primeras cinco imágenes de entrada y sus etiquetas. En las imágenes de las etiquetas, el blanco y el negro representan los bordes y el fondo, respectivamente, mientras que los demás colores corresponden a diferentes clases.

Next, we enumerate the RGB color values and class names for all the labels in this dataset.

Con las dos constantes definidas anteriormente, podemos encontrar convenientemente el índice de clase para cada píxel en una etiqueta. Definimos la función voccolormap2label para crear la asignación de los valores de color RGB anteriores a los índices de clase, y la función voclabel_indices para asignar cualquier valor RGB a sus índices de clase en este conjunto de datos Pascal VOC2012.

Por ejemplo, en la primera imagen de ejemplo, el índice de clase para la parte delantera del avión es 1, mientras que el índice de fondo es 0.

Preprocesamiento de datos
En experimentos anteriores, hemos tenido que reescalar las imágenes para ajustarnos a la forma de entrada requerida por el modelo. Sin embargo, en la segmentación semántica, hacerlo requiere volver a escalar las clases de píxeles previstas para la forma original de la imagen de entrada. Tal cambio de escala puede ser inexacto, especialmente para regiones segmentadas con diferentes clases. Para evitar este problema, recortamos la imagen a una forma fija en lugar de cambiar la escala. Específicamente, usando el RandomCrop que vimos la primera vez que hicimos image augmentation, recortamos la misma área de la imagen de entrada y la etiqueta.

Dataset Personalizado de Segmentación Semántica

Definimos una clase de conjunto de datos de segmentación semántica personalizada VOCSegDataset heredando la clase Dataset proporcionada por las API de alto nivel. Al implementar la función getitem, podemos acceder arbitrariamente a la imagen de entrada indexada como idx en el conjunto de datos y al índice de clase de cada píxel en esta imagen. Dado que algunas imágenes en el conjunto de datos tienen un tamaño más pequeño que el tamaño de salida del recorte aleatorio, estos ejemplos se filtran mediante una función de "filtro" personalizada. Además, también definimos la función normalize_image para estandarizar los valores de los tres canales RGB de las imágenes de entrada.

Lectura del Dataset

Usamos la clase VOCSegDataset personalizada para crear instancias del conjunto de entrenamiento y el conjunto de prueba, respectivamente. Supongamos que especificamos que la forma de salida de las imágenes recortadas aleatoriamente es . A continuación, podemos ver la cantidad de ejemplos que se retienen en el conjunto de entrenamiento y el conjunto de prueba.

Al establecer el tamaño del lote en 64, definimos el iterador de datos para el conjunto de entrenamiento. Imprimamos la forma del primer minilote. A diferencia de la clasificación de imágenes o la detección de objetos, las etiquetas aquí son tensores tridimensionales.

Convolución transpuesta

Las capas de CNN que hemos visto hasta ahora, como las capas convolucionales  y las capas de pooling, normalmente reducen (reducen) las dimensiones espaciales (alto y ancho) de la entrada, o las mantienen sin cambios. En la segmentación semántica, que clasifica a nivel de píxel, será conveniente que las dimensiones espaciales de la entrada y la salida sean las mismas. Por ejemplo, la dimensión del canal en un píxel de salida puede contener los resultados de clasificación para el píxel de entrada en la misma posición espacial.

Para lograr esto, especialmente después de que las capas CNN reducen las dimensiones espaciales, debemos usar otro tipo de capas CNN que sean capaces de aumentar las dimensiones espaciales de los mapas de características intermedias (upsampling). En esta sección, presentaremos la convolución transpuesta, para invertir las operaciones de downsampling por la convolución.

Operación básica

Ignorando los canales por ahora, comencemos con
la operación básica de convolución transpuesta
con stride 1 y sin padding. Supongamos que
nos dan un tensor de entrada 
y un kernel . Deslizar la ventana del kernel con un stride de 1  veces en cada fila y  veces en cada columna, produce un total de  resultados intermedios.

Cada resultado intermedio es un tensor  que se inicializan como ceros. Para calcular cada tensor intermedio, cada elemento en el tensor de entrada se multiplica por el kernel para que el tensor  resultante reemplace una porción en cada tensor intermedio. Tenga en cuenta que la posición de la porción reemplazada en cada tensor intermedio corresponde a la posición del elemento en el tensor de entrada utilizado para el cálculo. Al final, todos los resultados intermedios se suman para producir la salida.

Podemos implementar esta operación básica de convolución transpuesta trans_conv para una matriz de entrada X y una matriz de kernel K.

A diferencia de la convolución normal que reduce los elementos de entrada a través del kernel, la convolución transpuesta propaga los elementos de entrada mediante el kernel, produciendo así una salida mayor que la entrada. Podemos construir el tensor de entrada X y el tensor kernel K de la figura para validar la salida de la implementación anterior de la operación de convolución transpuesta bidimensional básica.

Alternatively,
when the input X and kernel K are both
four-dimensional tensors,
we can [use high-level APIs to obtain the same results].

Padding, Strides y Multiples Canales

A diferencia de la convolución regular donde se le aplica el padding a la entrada, en la convolución transpuesta se le aplica a la salida. Por ejemplo, al especificar el padding a ambos lados de la altura y el ancho como 1, la primera y la última fila y columna se eliminarán de la salida de convolución transpuesta.

Esto puede parecer contraintuitivo... ¿Por qué el padding (relleno en inglés) eliminaría dimensiones en vez de agregarlas? La respuesta viene del uso que se les da a estas convoluciones transpuestas. PyTorch (así como el resto de los frameworks de Deep Learning) sabe que la única razón por la que usarías una de estas convoluciones transpuestas, es para anular la reducción de dimensionalidad introducida por convoluciones regulares previas en la red. Por lo tanto, el padding de las convoluciones transpuestas está diseñado para hacer lo contrario de lo que hace el padding en las convoluciones regulares. De esta manera, sabemos que para contrarrestar los efectos de una convolución regular con padding 2, simplemente debemos agregar una convolución transpuesta con padding 2.

Siguiendo con la lógica anterior, el stride en la convolución transpuesta se especifica para saltearno píxeles en los resultados intermedios, no en la entrada.

Usando los mismos tensores de entrada y kernel de la figura anterior, cambiar el stride de 1 a 2 aumenta tanto la altura como el ancho de los tensores intermedios, de ahí el tensor de salida de la figura siguiente

El siguiente fragmento de código puede validar la salida de la convolución transpuesta para un stride de 2.

Para múltiples canales de entrada y salida, la convolución transpuesta funciona de la misma manera que la convolución normal. Suponga que la entrada tiene canales  y que la convolución transpuesta asigna un  kernel  a cada canal de entrada. Cuando se especifican múltiples canales de salida, tendremos un kernel  para cada canal de salida.

Como en todo, si alimentamos  en una capa convolucional  para generar  y creamos una capa convolucional transpuesta  con los mismos hiperparámetros que  excepto que el número de canales de salida es el número de canales en , entonces  tendrá la misma forma que . Esto se puede ilustrar en el siguiente ejemplo.

Conexión con las Matrices Transpuestas

La convolución transpuesta lleva el nombre de la matriz transpuesta. Para explicar por qué, primero veamos cómo implementar convoluciones usando multiplicaciones de matrices. En el siguiente ejemplo, definimos una entrada X de  y un kernel de convolución K de , y luego usamos la función corr2d para calcular la salida de convolución Y.

A continuación, reescribimos el kernel de convolución K como una matriz de pesos rala W que contiene muchos ceros. La forma de la matriz de peso es (, ), donde los elementos distintos de cero provienen del kernel de convolución K.

Luego, concatenamos la entrada X fila por fila para obtener un vector de longitud 16. Luego, la multiplicación de la matriz de W y la vectorizada X da un vector de longitud 4. Después de remodelarlo, podemos obtener el mismo resultado Y  de la operación de convolución original anterior: acabamos de implementar convoluciones usando multiplicaciones de matrices.

Del mismo modo, podemos implementar convoluciones transpuestas usando multiplicaciones de matrices. En el siguiente ejemplo, tomamos la salida Y de  de la convolución regular anterior como entrada a la convolución transpuesta. Para implementar esta operación multiplicando matrices, solo necesitamos transponer la matriz de pesos W con la nueva forma .

Convolución Transpuesta vs Deconvolución

La capa convolucional transpuesta también se conoce (erróneamente) como la capa deconvolucional.

Una capa deconvolucional invierte el funcionamiento de una capa convolucional estándar, es decir, si la salida generada a través de una capa convolucional estándar se deconvoluciona, se recupera la entrada original. La capa convolucional transpuesta es similar a la capa deconvolucional en el sentido de que la dimensión espacial generada por ambas es la misma. La convolución transpuesta no invierte la convolución estándar por valores, sino solo por dimensiones.

Fully Convolutional Networks

Como se discutió anteriormente, la segmentación semántica clasifica las imágenes a nivel de píxeles. Una red totalmente convolucional (Fully Convolutional Network - FCN) utiliza una red neuronal convolucional para transformar los píxeles de la imagen en clases de píxeles. A diferencia de las CNN que usamos anteriormente para la clasificación de imágenes o la detección de objetos, una red totalmente convolucional transforma la altura y el ancho de los mapas de activación intermedios de vuelta a los de la imagen de entrada: esto se logra mediante la capa convolucional transpuesta presentada en la sección anterior. Como resultado, la salida de clasificación y la imagen de entrada tienen una correspondencia uno a uno en el nivel de píxel: la dimensión del canal en cualquier píxel de salida contiene los resultados de clasificación para el píxel de entrada en la misma posición espacial.

El Modelo

Aquí describimos el diseño básico del modelo de red totalmente convolucional. Como se muestra en la figura, este modelo primero usa una CNN para extraer las características de la imagen, luego transforma el número de canales en el número de clases a través de una capa convolucional y finalmente transforma la altura y el ancho de los mapas de características en los de la imagen de entrada a través de la convolución transpuesta. Como resultado, la salida del modelo tiene la misma altura y anchura que la imagen de entrada, donde el canal de salida contiene las clases predichas para el píxel de entrada en la misma posición espacial.

A continuación, usamos un modelo ResNet-18 preentrenado en el conjunto de datos de ImageNet para extraer las características de la imagen y denotar la instancia del modelo como pretrained_net. Las últimas capas de este modelo incluyen una capa de average pooling global y una capa densas: estas no son necesarias en la red totalmente convolucional.

A continuación, creamos la instancia de red totalmente convolucional net. Copia todas las capas previamente entrenadas en ResNet-18, excepto la capa de averge pooling global final y la capa densa que están más cerca de la salida.

Dada una entrada con una altura y un ancho de 320 y 480 respectivamente, la propagación directa de net reduce la altura y el ancho de entrada a 1/32 del original, es decir, 10 y 15.

A continuación, usamos una capa convolucional  para transformar la cantidad de canales de salida en la cantidad de clases (21) del conjunto de datos Pascal VOC2012.

Finalmente, necesitamos aumentar la altura y el ancho de los mapas de características 32 veces para volver a cambiarlos a la altura y el ancho de la imagen de entrada.

Recuerde cómo calcular la forma de salida de una capa convolucional:

Si cambiáramos todas las capas convolucionales por una sola, podríamos decir que esa capa tendría un kernel de , un stride de  y un padding de , ya que:

En general, podemos ver que para stride , padding  (asumiendo que  es un número entero), y la altura y el ancho del kernel ,
la convolución transpuesta aumentará la altura y el ancho de la entrada  veces.

Inicializar las capas convolucionales transpuestas

Ya sabemos que las capas convolucionales transpuestas pueden aumentar la altura y el ancho de los mapas de características. En el procesamiento de imágenes, es posible que necesitemos escalar una imagen, es decir, hacemos upsampling. La interpolación bilineal es una de las técnicas de upsampling más utilizadas. También se usa a menudo para inicializar capas convolucionales transpuestas.

Para explicar la interpolación bilineal, digamos que dada una imagen de entrada, queremos calcular cada píxel de la imagen de salida muestreada. Para calcular el píxel de la imagen de salida en la coordenada , primero asigne  a la coordenada  en la imagen de entrada, por ejemplo, según la relación entre el tamaño de entrada y el tamaño de salida. Tenga en cuenta que  y  mapeados son números reales. Luego, busque los cuatro píxeles más cercanos a la coordenada  en la imagen de entrada. Finalmente, el píxel de la imagen de salida en la coordenada  se calcula en base a estos cuatro píxeles más cercanos en la imagen de entrada y su distancia relativa desde .

El upsampling de la interpolación bilineal se puede implementar mediante la capa convolucional transpuesta con el kernel construido por la siguiente función bilinearkernel. Debido a las limitaciones de espacio, solo proporcionamos la implementación de la función bilinearkernel a continuación sin discusiones sobre el diseño de su algoritmo.

Experimentemos con el upsampling de la interpolación bilineal que se implementa mediante una capa convolucional transpuesta. Construimos una capa convolucional transpuesta que duplica la altura y el ancho, e inicializamos su kernel con la función bilinear_kernel.

Leemos la imagen X y asignamos la salida del upsampling a Y. Para imprimir la imagen, necesitamos ajustar la posición de la dimensión del canal.

Como podemos ver, la capa convolucional transpuesta aumenta tanto el alto como el ancho de la imagen por un factor de dos. Excepto por las diferentes escalas en coordenadas, la imagen ampliada por interpolación bilineal y la imagen original tienen el mismo aspecto.

En una red totalmente convolucional, inicializamos la capa convolucional transpuesta con upsampling de interpolación bilineal. Para la capa convolucional , usamos la inicialización de Xavier.

Entrenamiento

Ahora podemos entrenar nuestra red totalmente convolucional. La función de pérdida y el cálculo del accuracy aquí no son esencialmente diferentes de los de la clasificación de imágenes de capítulos anteriores.

Debido a que usamos los canales de la salida de la capa convolucional transpuesta para predecir la clase de cada píxel, la dimensión del canal se especifica en el cálculo de la función de pérdida. Además, la precisión se calcula en función de la corrección de la clase predicha para todos los píxeles.

Predicción

Al predecir, necesitamos estandarizar la imagen de entrada en cada canal y transformar la imagen al formato de entrada de cuatro dimensiones requerido por la CNN.

Para visualizar la clase prevista de cada píxel, mapeamos la clase prevista de nuevo a su color de etiqueta en el conjunto de datos.

Las imágenes en el conjunto de datos de prueba varían en tamaño y forma. Dado que el modelo utiliza una capa convolucional transpuesta con un paso de 32, cuando la altura o el ancho de una imagen de entrada es indivisible por 32, la altura o el ancho de salida de la capa convolucional transpuesta se desviará de la forma de la imagen de entrada. Para solucionar este problema, podemos recortar varias áreas rectangulares con una altura y un ancho que sean múltiplos enteros de 32 en la imagen y realizar la propagación hacia adelante en los píxeles de estas áreas por separado. Tenga en cuenta que la unión de estas áreas rectangulares debe cubrir completamente la imagen de entrada. Cuando un píxel está cubierto por múltiples áreas rectangulares, el promedio de las salidas de convolución transpuestas en áreas separadas para este mismo píxel se puede ingresar a la operación softmax para predecir la clase.

Para simplificar, solo leemos algunas imágenes de prueba más grandes y recortamos un área de  para la predicción a partir de la esquina superior izquierda de una imagen. Para estas imágenes de prueba, imprimimos sus áreas recortadas, los resultados de la predicción y la verdad sobre el terreno fila por fila.