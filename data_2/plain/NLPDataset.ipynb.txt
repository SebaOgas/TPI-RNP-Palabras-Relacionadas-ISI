Construcción de un Dataset para Procesamiento de Lenguaje Natural

En este notebook vamos a aprender a trabajar con texto para generar un dataset que nos permita entrenar modelos de Procesamiento de Lenguaje Natural. Para esto vamos a ver un ejemplo en una tarea particular, pero los conceptos necesarios para construir este dataset son aplicables a todas las tareas del rubro.

Inferencia del lenguaje natural

La Inferencia del lenguaje natural estudia si una hipótesis
se puede inferir de una premisa, donde ambas son una secuencia de texto.
En otras palabras, la inferencia del lenguaje natural determina la relación lógica entre un par de secuencias de texto.
Estas relaciones suelen clasificarse en tres tipos:

 Implicación* (entailment): la hipótesis se puede inferir de la premisa.
 Contradicción* (contradiction): la negación de la hipótesis se puede inferir de la premisa.
 Neutral*: todos los demás casos.

La inferencia del lenguaje natural también se conoce como tarea de reconocimiento de vinculación textual.

Por ejemplo, el siguiente par se etiquetará como Implicación porque  "showing affection" en la hipótesis se puede inferir de "hugging one another" en la premisa.
Premise: Two women are hugging each other.
Hypothesis: Two women are showing affection.

El siguiente es un ejemplo de contradicción ya que "running the coding example" indica "not sleeping" en lugar de "sleeping".
Premise: A man is running the coding example from Dive into Deep Learning.
Hypothesis: The man is sleeping.

El tercer ejemplo muestra una relación de neutralidad porque ni "famous" ni "not famous" pueden inferirse del hecho de que "are performing for us".
Premise: The musicians are performing for us.
Hypothesis: The musicians are famous.

La inferencia del lenguaje natural ha sido un tema central para comprender el lenguaje natural. Disfruta de amplias aplicaciones que van desde la recuperación de información hasta la respuesta a preguntas de dominio abierto. Para estudiar este problema, comenzaremos investigando un conjunto de datos de referencia de inferencia de lenguaje natural popular.

The Stanford Natural Language Inference (SNLI) Dataset

El corpus de inferencia del lenguaje natural de Stanford (SNLI) es una colección de más de 500 000 pares de oraciones etiquetadas en inglés.
Descargamos y almacenamos el conjunto de datos SNLI extraído en la ruta ../data/snli_1.0.

El dataset está estructurado como un archivo separado por tabs. Usaremos pandas para leerlo.

Leyendo el dataset

El conjunto de datos SNLI original contiene información mucho más rica de la que realmente necesitamos en nuestros experimentos. Por lo tanto, definimos una función read_snli para extraer solo parte del conjunto de datos y luego devolver listas de premisas, hipótesis y sus etiquetas.

Ahora imprimamos los primeros 3 pares de premisa e hipótesis, así como sus etiquetas ("0", "1" y "2" corresponden a "entailment", "contradiction" y "neutral", respectivamente).

El conjunto de entrenamiento tiene alrededor de 550000 pares,
y el conjunto de prueba tiene alrededor de 10000 pares.
A continuación se muestra que
las tres etiquetas están equilibradas en
tanto el conjunto de entrenamiento como el de prueba.

Tokenización con Spacy

La tokenización es la tarea de dividir un texto en segmentos significativos, llamados tokens. La entrada al tokenizador es un texto Unicode y la salida es un objeto Doc de Spacy.

La tokenización de spaCy no es destructiva, lo que significa que siempre podrás reconstruir la entrada original a partir de la salida tokenizada. La información de los espacios en blanco se conserva en los tokens y no se agrega ni elimina ninguna información durante la tokenización. Este es una especie de principio básico del objeto Doc de spaCy: doc.text == input_text siempre debe ser verdadero.

Durante el procesamiento, spaCy primero tokeniza el texto, es decir, lo segmenta en palabras, signos de puntuación, etc. Esto se hace aplicando reglas específicas de cada idioma. Por ejemplo, la puntuación al final de una frase debe separarse, mientras que “EE.UU.” debería seguir siendo un sólo token.

Acontinuación vamos a descargar los modelos para los idiomas Español e Inglés.

Creación del Vocabulario

Estos tokens siguen siendo cadenas. Sin embargo, las entradas a nuestros modelos deben consistir en última instancia en entradas numéricas. A continuación, presentamos una clase para construir vocabularios, es decir, objetos que asocian cada valor de token distinto con un índice único. Primero, determinamos el conjunto de tokens únicos en nuestro corpus de entrenamiento. Luego asignamos un índice numérico a cada token único. Los elementos raros del vocabulario a menudo se eliminan por conveniencia. Cada vez que nos encontramos con un token en el tiempo de entrenamiento o prueba que no se había visto previamente o se había eliminado del vocabulario, la representamos por un token especial “<unk>”, lo que significa que este es un valor desconocido.

Definición de una clase para cargar el conjunto de datos

A continuación definimos una clase para cargar el conjunto de datos SNLI heredando de la clase Dataset. El argumento num_steps en el constructor de clases especifica la longitud de una secuencia de texto para que cada minilote de secuencias tenga la misma forma.
En otras palabras, los tokens después de los primeros numsteps en una secuencia más larga se recortan, mientras que los tokens especiales “&lt;pad&gt;” se agregarán a secuencias más cortas hasta que su longitud se convierta en "numsteps".
Al implementar la función getitem, podemos acceder arbitrariamente a la premisa, hipótesis y etiqueta con el índice idx.

Juntando todo

Ahora podemos invocar la función read_snli y la clase SNLIDataset para descargar el conjunto de datos SNLI y devolver instancias de DataLoader para los conjuntos de entrenamiento y prueba, junto con el vocabulario del conjunto de entrenamiento.
Es de destacar que debemos utilizar el vocabulario construido a partir del conjunto de entrenamiento como el del conjunto de prueba.
Como resultado, cualquier token nuevo del conjunto de prueba será desconocido para el modelo entrenado en el conjunto de entrenamiento.

Aquí configuramos el tamaño del lote en 128 y la longitud de la secuencia en 50, e invocamos la función loaddatasnli para obtener los iteradores de datos y el vocabulario. Luego imprimimos el tamaño del vocabulario.

Ahora imprimimos la forma del primer minibatch. Tenemos dos entradas X[0] y X[1] que representan pares de premisas e hipótesis.