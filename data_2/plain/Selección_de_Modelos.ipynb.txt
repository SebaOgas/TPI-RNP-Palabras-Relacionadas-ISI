<a href="https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/DeepLearning/5EvaluacionModelos/2SeleccionModelos.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"/></a>

Selección de modelos

En el aprendizaje automático, generalmente seleccionamos nuestro modelo final después de evaluar varios modelos candidatos. Este proceso se llama selección de modelo. A veces los modelos sujetos a comparación
son de naturaleza fundamentalmente diferente
(por ejemplo, árboles de decisión frente a modelos lineales). En otras ocasiones, estamos comparando miembros de la misma clase de modelos que han sido entrenados con diferentes configuraciones de hiperparámetros.

Con los MLP, por ejemplo, es posible que deseemos comparar modelos con diferentes números de capas ocultas, diferentes números de unidades ocultas y varias opciones de funciones de activación aplicadas a cada capa oculta. Para determinar cuál es el mejor entre nuestros modelos candidatos, generalmente emplearemos un conjunto de datos de validación.

Conjunto de datos de validación

En principio, no deberíamos tocar nuestro conjunto de prueba hasta que hayamos elegido todos nuestros hiperparámetros.
Si utilizáramos los datos de prueba en el proceso de selección del modelo, existe el riesgo de que podamos sobreajustar los datos de prueba. Entonces estaríamos en serios problemas. Si sobreajustamos nuestros datos de entrenamiento, siempre existe la evaluación de los datos de prueba para mantenernos honestos. Pero si sobreajustamos los datos de prueba, ¿cómo lo sabríamos?

Por lo tanto, nunca debemos confiar en los datos de prueba para la selección del modelo. Y, sin embargo, tampoco podemos confiar únicamente en los datos de entrenamiento para la selección del modelo porque no podemos estimar el error de generalización en los mismos datos que usamos para entrenar el modelo.

En aplicaciones prácticas, la imagen se vuelve más turbia. Si bien, idealmente, solo tocaríamos los datos de prueba una vez, para evaluar el mejor modelo o para comparar una pequeña cantidad de modelos entre sí, los datos de prueba del mundo real rara vez se descartan después de un solo uso. Rara vez podemos permitirnos un nuevo conjunto de prueba para cada ronda de experimentos.

La práctica común para abordar este problema
es dividir nuestros datos de tres maneras, incorporando un conjunto de datos de validación (o conjunto de validación) además de los conjuntos de datos de entrenamiento y prueba.

Un buen ejemplo para distinguir entre conjunto de prueba y de validación es lo que hace la plataforma Kaggle en sus competencias de aprendizaje automático. En sus inicios, Kaggle era solamente una plataforma de concursos donde las empresas publican problemas y los participantes compiten para construir el mejor algoritmo, generalmente con premios en efectivo. La organización d elos concursos consiste en:
el organizador debe separar su dataset en un conjunto de entrenamiento (que será publicado) y un conjunto de prueba (cuyas features serán publicadas, pero las etiquetas permanecerán ocultas).
Los participantes podrán descargar los datos de entrenamiento y deberán elegir un modelo para presentar en la competencia. Para eso, deberán llevar adelante una selección de modelos generando un conjunto de validación a partir de los datos de entrenamiento.
Una vez seleccionado el modelo que mejor funcione con los datos de validación, se alimenta dicho modelo con las features del conjunto de prueba para obtener las etiquetas de prueba predichas por el modelo.
Se entregan las etiquetas de prueba predichas y el organizador las compara con las reales. El ganador es el modelo que menos erroes haya cometido.

De esta manera, los conjuntos de prueba y validación están bien diferenciados. El primero se usa para elegir el mejor modelo y el segundo se usa para evaluar el modelo elegido con datos que nunca vio en el entrenamiento.

A menos que se indique explícitamente lo contrario, en los experimentos de este curso en realidad estamos trabajando con lo que correctamente debería llamarse datos de entrenamiento y datos de validación, sin verdaderos conjuntos de prueba. Por lo tanto, reportado en cada experimento es realmente un accuracy de validación y no un verdadero accuracy del conjunto de pruebas.

-fold cross-validation

Cuando los datos de entrenamiento son escasos, es posible que ni siquiera podamos permitirnos mantener suficientes datos para constituir un conjunto de validación adecuado. Una solución popular a este problema es emplear -fold cross-validation. Aquí, los datos de entrenamiento originales se dividen en  subconjuntos que no se superponen. Luego, el entrenamiento y la validación del modelo se ejecutan  veces, cada vez entrenando en  subconjuntos y validando en un subconjunto diferente (el que no se usó para entrenar en esa ronda).
Finalmente, los errores de entrenamiento y validación se estiman promediando los resultados de los experimentos de .

Model
Definamos una red neuronal simple para el conjunto de datos MNIST.

Función para reiniciar pesos
Necesitamos restablecer los pesos del modelo para que cada fold de cross validation comience desde un estado inicial aleatorio y no aprenda de los folds anteriores. Podemos llamar a reset_weights() en todos los módulos hijos.

Modificamos ligeramente los pipelines de entrenamiento para que sea más ordenado... Todas las lineas para calcular la pérdida y mejorar los parámetros las ponemos en la función train y todas las que se encargan de calcular el accuracy, en la función test.

Dataset
Necesitamos concatenar las partes de entrenamiento y prueba del dataset MNIST, que usaremos para entrenar el modelo. Hacer K-fold implica que nosotros mismos generemos las divisiones, por lo que no queremos que PyTorch lo haga por nosotros.

Clase KFold

KFold es una clase de la librería sklearn que nos puede ayudar a hacer cross validation. Para eso debemos instanciar el objeto kfold indicando la cantidad de folds que queremos en el atributo n_splits del constructor.

La clase KFold tiene un método llamado split() que es un iterator que recibe el dataset a separar y devuelve un tupla con dos listas de índices. La primera es la lista de índices de entrenamiento y la segunda es la lista de índices de testeo de ese fold.

Ahora podemos generar los folds y entrenar nuestro modelo. Lo vamos a hacer definiendo un loop que itere sobre los folds especificando la lista de identificadores de los ejemplos de entrenamiento y validación para ese fold en particular.

Dentro del loop hacemos un print del id del fold. Después, entrenamos muestreando los elementos de train y test con un SubsetRandomSampler. A esta clase se le puede pasar una lista con los índices de los elementos que debe muestrear del dataset.