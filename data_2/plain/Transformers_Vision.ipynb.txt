Transformers para Visi√≥n (ViT)

La arquitectura del tranformer se propuso inicialmente para el aprendizaje de secuencia a secuencia, como la traducci√≥n autom√°tica. Con gran eficacia, los tranformers se convirtieron posteriormente en el modelo de elecci√≥n en diversas tareas de procesamiento del lenguaje natural.
Sin embargo, en el campo de la visi√≥n artificial la arquitectura dominante
se hab√≠a basado en las CNN. ¬øPodemos adaptar tranformers para modelar datos de imagen? Esta pregunta ha despertado un gran inter√©s en la comunidad de visi√≥n artificial. Un paper del 2020 demostr√≥ te√≥ricamente que la autoatenci√≥n puede aprender a comportarse de manera similar a la convoluci√≥n. Emp√≠ricamente, se tomaron parches de  de las im√°genes como entrada, pero el peque√±o tama√±o del parche hace que el modelo solo sea aplicable a datos de im√°genes con resoluciones bajas.

Sin restricciones espec√≠ficas sobre el tama√±o del parche, los tranformers de visi√≥n (ViT) extraen parches de las im√°genes y los introducen en un encoder de tranformer para obtener una representaci√≥n global, que finalmente se transformar√° para la clasificaci√≥n. En particular, los tranformers muestran una mejor escalabilidad que las CNN: cuando se entrenan modelos m√°s grandes en conjuntos de datos m√°s grandes, los tranformers de visi√≥n superan a los ResNet por un margen significativo. Similar al panorama del dise√±o de arquitectura de red en el procesamiento del lenguaje natural, los tranformers tambi√©n cambiaron las reglas del juego en la visi√≥n por computadora.

CNN vs ViT: Sesgo Inductivo

El sesgo inductivo es un t√©rmino utilizado en el aprendizaje autom√°tico para describir el conjunto de suposiciones que utiliza un algoritmo de aprendizaje para hacer predicciones. En t√©rminos m√°s simples, el sesgo inductivo es como un atajo que ayuda a un modelo de aprendizaje autom√°tico a hacer conjeturas basadas en la informaci√≥n que ha visto hasta ahora.

Aqu√≠ hay un par de sesgos inductivos que observamos en las CNN:
Equivarianza traslacional: un objeto puede aparecer en cualquier parte de la imagen y las CNN pueden detectar sus caracter√≠sticas.
Localidad: los p√≠xeles de una imagen interact√∫an principalmente con los p√≠xeles circundantes para generar features.

Estos sesgos inductivos no existen en los ViT porque originalmente fueron pensados para trabajar con secuencias de palabras. Entonces, ¬øc√≥mo se desempe√±an tan bien? Es porque su dise√±o basado en los mecanismos de atenci√≥n son altamente escalables debido a su alto grado de paralelizaci√≥n. Por tanto, superan la necesidad de estos sesgos inductivos si se los entrena con cantidades supermasivas de im√°genes.

Entrenando un ViT desde cero

La siguiente figura representa la arquitectura modelo de los tranformers de visi√≥n. Esta arquitectura consta de una base que parchea las im√°genes, un cuerpo basado en el encoder de un tranformer multicapa y una cabeza que transforma la representaci√≥n global en la etiqueta de salida.

Considere una imagen de entrada con altura , ancho  y  canales. Especificando la altura y el ancho del parche como ,
la imagen se divide en una secuencia de  parches,
donde cada parche se aplana a un vector de longitud .
De esta forma, los parches de imagen pueden ser tratados de manera similar a los tokens en secuencias de texto por encoderes de tranformeres. Un token especial ‚Äú&lt;cls&gt;‚Äù (clasificaci√≥n) y los  parches de imagen aplanados se proyectan linealmente en una secuencia de vectores , sumados con embeddings posicionales que se pueden aprender. El encoder de tranformer multicapa transforma los vectores de entrada  en la misma cantidad de representaciones de vectores de salida de la misma longitud. Funciona exactamente de la misma manera que el encoder del tranformer original, solo que difiere en la posici√≥n de normalizaci√≥n. Dado que el token ‚Äú&lt;cls&gt;‚Äù  atiende a todos los parches de imagen a trav√©s de la autoatenci√≥n su representaci√≥n desde la salida del encoder del tranformer
se transformar√° en la etiqueta de salida.

Patch Embedding

Para implementar un transformer de visi√≥n, comencemos con los embeddings de los parches. Dividir una imagen en parches y proyectar linealmente estos parches aplanados se puede simplificar como una sola operaci√≥n de convoluci√≥n, donde tanto el tama√±o del kernel como el tama√±o del stride se establecen en el tama√±o del parche.

En el siguiente ejemplo, tomando im√°genes con una altura y un ancho de imgsize como entrada, se generan (imgsize//patchsize)**2 parches que se proyectan linealmente en vectores de longitud numhiddens.

Encoder del Transformer de Vision

El MLP del encoder transformer de visi√≥n es ligeramente diferente del a la red feed forward posicional del encoder del transformer original. Primero, aqu√≠ la funci√≥n de activaci√≥n usa la unidad lineal de error gaussiano (GELU), que puede considerarse como una versi√≥n m√°s suave de ReLU. En segundo lugar, el dropout se aplica a la salida de cada capa densa en el MLP para la regularizaci√≥n.

La implementaci√≥n del bloque del encoder del transformer de visi√≥n simplemente sigue el dise√±o de prenormalizaci√≥n, donde la normalizaci√≥n se aplica justo antes de la atenci√≥n multiples cabezales o el MLP. A diferencia de la posnormalizaci√≥n, donde la normalizaci√≥n se coloca justo despu√©s de las conexiones residuales, la prenormalizaci√≥n conduce a un entrenamiento m√°s efectivo o eficiente para los transformers.

Igual que en el transformer original, cualquier bloque de encoder de transformer de visi√≥n no cambia su forma de entrada.

Juntar todo

El paso hacia adelante de los transformers de visi√≥n es sencillo. Primero, las im√°genes de entrada se introducen en una instancia PatchEmbedding, cuya salida se concatena con el embedding del token ‚Äú&lt;cls&gt;‚Äù. Se suman los embeddings posicionales aprendibles antes del dropout. Luego, la salida se alimenta al encoder del transformer que apila las instancias num_blks de la clase ViTBlock. Finalmente, la representaci√≥n del token ‚Äú&lt;cls&gt;‚Äù token es proyectado por la cabeza de la red.

Entrenamiento

Entrenar un transformer de visi√≥n en el conjunto de datos Fashion-MNIST es similar a c√≥mo se entrenaron las CNN en clases anteriores.

Puede notar que para datasets peque√±os como Fashion-MNIST, nuestro transformer de visi√≥n implementado no supera a ResNet. Se pueden realizar observaciones similares incluso en el conjunto de datos de ImageNet (1,2 millones de im√°genes). Esto se debe a que los transformers carecen de esos principios √∫tiles en la convoluci√≥n, como la localidad y la invariancia a la traslaci√≥n. Sin embargo, el panorama cambia cuando se entrenan modelos m√°s grandes en datasets m√°s grandes (por ejemplo, 300 millones de im√°genes), donde los transformers de visi√≥n superan a las ResNets por un amplio margen en la clasificaci√≥n de im√°genes, lo que demuestra la superioridad intr√≠nseca de los transformers en escalabilidad.

Fine Tuning sobre un ViT pre-entrenado

El fine-tuning es una t√©cnica en la que un modelo previamente entrenado, que ya ha aprendido caracter√≠sticas de una tarea, se utiliza como punto de partida para una tarea similar. Esto ahorra tiempo y recursos al aprovechar el conocimiento existente del modelo en lugar de entrenar un modelo nuevo desde cero.

En esta secci√≥n, vamos a ver c√≥mo podemos aplicar fine-tuning para la clasificaci√≥n de im√°genes con un Vision Transformer en un dataset de nuestra elecci√≥n.

En el fine-tuning, no necesitamos actualizar los par√°metros de todo el modelo. Dado que nuestro ViT ha aprendido representaciones de caracter√≠sticas de millones de im√°genes, podemos optar por entrenar las √∫ltimas capas de nuestro modelo para que funcione bien en nuestro nuevo conjunto de datos.

Para este tutorial, usaremos el modelo google/vit-base-patch16-224 del Hugging Face hub.

Comencemos importando algunos m√≥dulos y funciones necesarios.

Ahora, carguemos nuestro dataset de clasificaci√≥n de im√°genes.

Para este tutorial, usaremos el dataset de mascotas Oxford-IIIT. Es una colecci√≥n de im√°genes diferentes de 37 razas de perros y gatos. Usaremos la biblioteca Hugging Face Datasets para cargar nuestro conjunto de datos f√°cilmente desde el hub.

El conjunto de datos contiene las siguientes caracter√≠sticas:
ruta: una ruta al archivo
etiqueta: la raza del animal
perro: indica si el animal es un perro o no
imagen: una imagen en formato PIL

Veamos algunas im√°genes de muestra de nuestro dataset.

Para cualquier conjunto de datos que usemos con la biblioteca datasets, podemos mezclarlo usando shuffle() y seleccionar cualquier muestra usando el m√©todo select().

Preprocesamiento de nuestro conjunto de datos

Cuando se trata de datasets de im√°genes, el preprocesamiento implica varios pasos. Esto incluye transformaciones como cambiar el tama√±o de todas las im√°genes para que tengan las mismas dimensiones, normalizar y escalar los valores de p√≠xeles a un rango uniforme. Tambi√©n podemos hacer augmentatio de im√°genes aplicando giros aleatorios, rotaciones, perspectivas, etc.

Antes de aplicar nuestras transformaciones, dividamos nuestro conjunto de datos en 3 partes para entrenamiento, validaci√≥n y un conjunto de pruebas oculto para evaluar el rendimiento de nuestro modelo. Podemos utilizar el m√©todo incorporado traintestsplit para hacerlo.

Dado que solo tenemos una divisi√≥n de "entrenamiento" en nuestro dataset original, usaremos el 80% para entrenamiento y el 10% para "validaci√≥n" y el 10% restante como nuestra divisi√≥n de "prueba".

Es importante tener en cuenta que ning√∫n modelo puede comprender las etiquetas en su formato de string. Por lo tanto, los asignamos a sus contrapartes enteras. Como hay 37 etiquetas, las etiquetas se asignar√°n a un n√∫mero del 0 al 36.

Crearemos dos asignaciones, label2id y id2label para convertir las etiquetas a sus ID y viceversa. Esto tambi√©n ser√° √∫til cuando inicialicemos nuestro modelo para actualizar su configuraci√≥n.

Image Processor

Para aplicar las transformaciones correctas en nuestras im√°genes, usaremos AutoImageProcessor que aplicar√° las transformaciones de acuerdo con el modelo que usaremos. Podemos verificar su configuraci√≥n para ver qu√© transformaciones se aplicar√°n.

Para aplicar las transformaciones a un lote en el momento del entrenamiento, podemos crear una funci√≥n que preprocesar√° el lote. El Trainer llamar√° a esta funci√≥n cuando la agreguemos al conjunto de datos usando with_transform.

En el momento del entrenamiento, debemos aplicar las transformaciones en un lote de muestras. Para manejar los lotes, crearemos una funci√≥n "transforms" que se encargar√° de lo siguiente:
Convertir todas las im√°genes a RGB: es posible que algunas im√°genes de su conjunto de datos sean en escala de grises o transparentes (RGBA).
Convertir las etiquetas de las cadenas a n√∫meros enteros: usando el mapa label2id.
Aplicar transformaciones de im√°genes: pasamos las im√°genes por el processor para procesarlas y convertirlas al formato PyTorch.

Las caracter√≠sticas del conjunto de datos resultantes ser√°n:
py
{
     'pixel_values': torch.Tensor,
     'etiquetas': List
}

Emparejaremos la funci√≥n con nuestro conjunto de datos usando el m√©todo with_transform().

Data Collation
Al proceso de agrupar nuestros datos en el formato correcto se lo denomina Data Collation. Para pixel_values, la forma de entrada para el modelo debe ser (batch, channels, alto, ancho) y para labels, la forma debe ser (batch,)

Veamos c√≥mo calcular las m√©tricas.

Podemos utilizar la biblioteca evaluate de Hugging Face para calcular las m√©tricas. Para la clasificaci√≥n de im√°genes, podemos utilizar la m√©trica de accuracy.

Cargando nuestro modelo

Usaremos ViTForImageClassification para cargar nuestro modelo pre-entrenado.

Necesitamos actualizar la capa de clasificaci√≥n final para generar predicciones iguales a la cantidad de etiquetas en nuestro conjunto de datos.
Lo haremos pasando el argumento num_labels junto con nuestras asignaciones de etiquetas id2label y label2id.

Tambi√©n necesitamos pasar ignoremismatchedsizes = True para compensar el cambio en el n√∫mero de par√°metros en la capa de clasificaci√≥n.

Aqu√≠ est√° la arquitectura de nuestro modelo.

Como no vamos a actualizar todo el modelo, podemos "congelar" todos los par√°metros excepto la nueva capa classifier estableciendo requires_grad en False para los par√°metros de cada capa.

Podemos comprobar cu√°ntos par√°metros hay en el modelo junto con cu√°ntos se van a entrenar ahora.

Comencemos nuestro entrenamiento üöÄÔ∏èüöÄÔ∏èüöÄÔ∏è

Vamos a usar Hugging Face Trainer para entrenar nuestro modelo. De esta manera, es m√°s simple elegir los argumentos de entrenamiento, como el tama√±o del lote, la tasa de aprendizaje, la cantidad de √©pocas, las opciones de logging, etc.

Con respecto a la clasificaci√≥n de im√°genes, necesitamos configurar removeunusedcolumns=False para evitar que se elimine la columna image de nuestro conjunto de datos, ya que es la que se utiliza para crear nuestras entradas de pixel_values.

Evaluamos en nuestro conjunto de datos de prueba

¬°Veamos algunas de las predicciones hechas por nuestro nuevo modelo!