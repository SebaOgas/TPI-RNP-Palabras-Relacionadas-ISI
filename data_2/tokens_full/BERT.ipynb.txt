<
a
href="https://colab.research.google.com
/
github
/
institutohumai
/
cursos-python
/
blob
/
master
/
NLP/7_BERT
/
BERT.ipynb
"
>
<
img
src='https://colab.research.google.com
/
assets
/
colab-badge.svg
'
/
>
<
/a
>



BERT



El
a√±o
2018
fue
 
un
importante
punto
de
inflexi√≥n
para
los
modelos
de
aprendizaje
autom√°tico
que
manejan
texto
(
o
,
m√°s
exactamente
,
Procesamiento
del
Lenguaje
Natural
o
NLP
para
abreviar
)
.
Nuestra
comprensi√≥n
de
la
mejor
forma
de
representar
palabras
y
oraciones
comprendiendo
los
significados
y
las
relaciones
subyacentes
est√°
evolucionando
r√°pidamente
.
Adem√°s
,
la
comunidad
de
NLP
ha
estado
presentando
componentes
incre√≠blemente
poderosos
que
puedes
descargar
y
usar
libremente
en
tus
propios
modelos
y
versiones



ULM-FiT
no
tiene
nada
que
ver
con
Cookie
Monster
pero
no
se
me
ocurri√≥
nada
mejor
üôÇ



Uno
de
los
√∫ltimos
hitos
en
este
desarrollo
es
el
lanzamiento
de
BERT
,
un
evento
descrito
como
el
comienzo
de
una
nueva
era
en
la
NLP
.
BERT
es
un
modelo
que
rompi√≥
varios
r√©cords
relativos
a
la
forma
en
la
que
estos
modelos
pueden
manejar
tareas
basadas
en
el
lenguaje
.
Poco
despu√©s
del
lanzamiento
del
documento
que
presenta
Bert
,
el
equipo
liber√≥
el
c√≥digo
del
modelo
y
puso
a
libre
disposici√≥n
la
descarga
de
versiones
del
modelo
pre-entrenadas
con
conjuntos
de
datos
masivos
.
Este
es
un
desarrollo
trascendental
,
ya
que
permite
a
cualquier
persona
desarrollar
un
modelo
de
aprendizaje
autom√°tico
que
involucre
procesamiento
del
lenguaje
,
usando
este
motor
como
un
componente
f√°cilmente
disponible
,
ahorrando
el
tiempo
,
la
energ√≠a
,
el
conocimiento
y
los
recursos
que
habr√≠a
tenido
que
destinar
a
entrenar
un
modelo
de
procesamiento
del
lenguaje
construido
desde
cero
.



Estos
son
los
2
pasos
a
seguir
para
usar
BERT
.
Primero
descargas
el
modelo
previamente
entrenado
con
datos
no
anotados
(
PASO
1
)
y
luego
te
concentras
en
ajustarlo
(
PASO
2
)
.
[
Fuente
de
la
imagen
]
.



BERT
se
basa
en
algunas
buenas
ideas
que
han
ido
surgiendo
recientemente
en
la
comunidad
de
NLP
,
y
que
incluyen
,
entre
otros
,
el
aprendizaje
semi-supervisado
(
de
Andrew
Dai
y
Quoc
Le
)
,
ELMo
(
de
Matthew
Peters
e
investigadores
de
AI2
y
UW
CSE
)
,
ULMFiT
(
del
fundador
de
fast.ai
Jeremy
Howard
y
Sebastian
Ruder
)
,
transformer
OpenAI
(
de
los
investigadores
de
OpenAI
Radford
,
Narasimhan
,
Salimans
y
Sutskever
)
y
los
Transformer
(
de
Vaswani
et
alia
.
)
.



Conviene
conocer
bien
algunos
conceptos
esenciales
para
comprender
correctamente
qu√©
es
BERT
.
Pero
comencemos
por
ver
las
formas
en
que
puedes
usar
BERT
,
antes
de
ver
los
conceptos
involucrados
en
el
modelo
en
s√≠
.


Clasificaci√≥n
de
oraciones



La
forma
m√°s
directa
de
emplear
BERT
es
usarlo
para
clasificar
un
fragmento
de
texto
.
El
modelo
tendr√≠a
este
aspecto
:



Para
entrenar
un
modelo
de
este
tipo
,
principalmente
se
tiene
que
entrenar
el
clasificador
,
con
cambios
m√≠nimos
en
el
modelo
BERT
durante
la
fase
de
entrenamiento
.
Este
proceso
de
entrenamiento
se
llama
Fine-Tunning
y
tiene
sus
ra√≠ces
en
el
Aprendizaje
Secuencial
Semi-supervisado
y
ULM-FiT.



Para
las
personas
que
no
est√°n
versadas
en
el
tema
,
dado
que
hablamos
de
clasificadores
,
estamos
en
el
dominio
del
aprendizaje
supervisado
,
dentro
del
campo
del
aprendizaje
autom√°tico
.
Lo
que
significar√≠a
que
necesitamos
un
conjunto
de
datos
etiquetados
para
entrenar
dicho
modelo
.
Para
este
ejemplo
de
clasificador
de
spam
,
el
conjunto
de
datos
etiquetado
ser√≠a
una
lista
de
mensajes
de
correo
electr√≥nico
y
una
etiqueta
(
‚Äú
spam
‚Äù
o
‚Äú
no
spam
‚Äù
para
cada
mensaje
)
.



Otros
ejemplos
de
este
uso
podr√≠an
incluir
:


An√°lisis
de
sentimientos


Entrada
:
Rese√±a
de
pel√≠cula
/
producto
.
Salida
:
¬ø
la
revisi√≥n
es
positiva
o
negativa
?


Conjunto
de
datos
de
ejemplo
:
SST


Comprobaci√≥n
de
hechos


Entrada
:
oraci√≥n
.
Salida
:
‚Äú
Declaraci√≥n
‚Äù
o
‚Äú
No
declaraci√≥n
‚Äù


Ejemplo
m√°s
ambicioso
/
futurista
:


Entrada
:
Declaraci√≥n
.
Salida
:
‚Äú
Verdadera
‚Äù
o
‚Äú
Falsa
‚Äù


Arquitectura
del
modelo



Ahora
que
sabemos
en
que
podemos
aplica
BERT
,
echemos
un
vistazo
m√°s
de
cerca
a
c√≥mo
funciona
.



El
paper
original
del
BERT
presenta
dos
tama√±os
del
modelo
:


BERT
BASE
:
comparable
en
tama√±o
al
Transformer
OpenAI
(
para
comparar
rendimiento
)


BERT
LARGE
:
un
modelo
rid√≠culamente
enorme
que
logr√≥
los
incre√≠bles
resultados
rese√±ados
en
el
paper
original



BERT
es
b√°sicamente
una
pila
de
transformers
encoders
entrenados
.
Es
posible
que
sea
necesario
revisar
la
clase
de
Transformes
para
continuar



Ambos
tama√±os
de
modelos
BERT
tienen
una
gran
cantidad
de
capas
de
encoder
:
12
para
la
versi√≥n
b√°sica
y
24
para
la
versi√≥n
grande
.
Estos
tambi√©n
tienen
densas
m√°s
grandes
(
768
y
1024
unidades
ocultas
respectivamente
)
y
m√°s
cabezales
de
atenci√≥n
(
12
y
16
respectivamente
)
que
la
configuraci√≥n
predeterminada
en
la
implementaci√≥n
de
referencia
del
Transformer
en
el
paper
inicial
(
6
capas
de
codificador
,
512
unidades
ocultas
,
y
8
cabezales
de
atenci√≥n
)
.



|Caracter√≠sticas|Transformer|BERT
base|BERT
large|


|:--|:-:|:-:|:-:|


|Cantidad
de
capas|6|12|24|


|Longitud
de
estados
ocultos|512|768|1024|


|Cantidad
de
cabezales
de
atenci√≥n|8|12|16|


Entradas
del
modelo



El
primer
token
de
entrada
viene
con
un
token
[
CLS
]
especial
por
motivos
que
se
aclarar√°n
m√°s
adelante
.
CLS
aqu√≠
significa
Clasificaci√≥n
.



Al
igual
que
el
encoder
vanilla
del
transformer
,
BERT
toma
una
secuencia
de
palabras
como
entrada
que
avancia
hacia
arriba
en
la
pila
.
Cada
capa
aplica
auto
atenci√≥n
,
pasa
sus
resultados
a
trav√©s
de
una
red
de
avance
y
luego
los
devuelve
al
siguiente
encoder
.



En
t√©rminos
de
arquitectura
,
es
id√©ntica
a
la
del
Transformer
hasta
este
punto
(
aparte
del
tama√±o
,
que
no
deja
de
ser
una
configuraciones
a
elecci√≥n
de
cada
uno
)
.
Es
en
la
salida
donde
empezamos
a
ver
c√≥mo
cambian
las
cosas
.


Salidas
del
modelo



Cada
posici√≥n
genera
un
vector
de
tama√±o
hidden_size
(
768
en
BERT
Base
)
.
Para
el
ejemplo
de
clasificaci√≥n
de
oraciones
que
vimos
anteriormente
,
nos
enfocamos
en
la
salida
de
solo
la
primera
posici√≥n
(
a
la
que
le
pasamos
el
token
[
CLS
]
especial
)
.



Ese
vector
ahora
se
puede
usar
como
entrada
para
un
clasificador
de
nuestra
elecci√≥n
.
El
documento
logra
excelentes
resultados
usando
√∫nicamente
una
red
neuronal
de
una
sola
capa
como
clasificador
.



Si
tienes
m√°s
etiquetas
(
por
ejemplo
,
si
es
un
servicio
de
correo
electr√≥nico
que
etiqueta
los
correos
electr√≥nicos
como
‚Äú
spam
‚Äù
,
‚Äú
no
spam
‚Äù
,
‚Äú
social
‚Äù
y
‚Äú
promoci√≥n
‚Äù
)
,
simplemente
modifica
la
red
clasificadora
para
tener
m√°s
neuronas
de
salida
que
luego
pasar
por
softmax
.


Una
nueva
era
de
integraci√≥n



Estos
nuevos
desarrollos
traen
consigo
un
nuevo
cambio
en
la
forma
en
que
se
codifican
las
palabras
.
Hasta
ahora
,
los
embeddings
de
palabras
han
sido
una
gran
herramienta
en
la
forma
en
que
los
principales
modelos
de
NLP
tratan
el
lenguaje
.
M√©todos
como
Word2Vec
y
Glove
se
han
utilizado
ampliamente
para
este
tipo
de
tareas
.
Recapitulemos
c√≥mo
se
usan
antes
de
se√±alar
lo
que
ahora
ha
cambiado
.



Resumen
de
embeddings
de
palabras



Para
que
las
palabras
sean
procesadas
por
modelos
de
aprendizaje
autom√°tico
,
necesitan
alguna
forma
de
representaci√≥n
num√©rica
que
los
modelos
puedan
usar
en
sus
c√°lculos
.
Word2Vec
demostr√≥
que
podemos
usar
un
vector
(
una
lista
de
n√∫meros
)
para
representar
correctamente
las
palabras
de
una
manera
que
captura
las
relaciones
sem√°nticas
o
relacionadas
con
el
significado
(
por
ejemplo
,
la
capacidad
de
saber
si
las
palabras
son
similares
u
opuestas
,
o
que
un
par
de
palabras
como
‚Äú
Estocolmo
‚Äù
y
‚Äú
Suecia
‚Äù
tienen
entre
ellos
la
misma
relaci√≥n
que
tienen
entre
ellos
‚Äú
El
Cairo
‚Äù
y
‚Äú
Egipto
‚Äù
)
,
as√≠
como
relaciones
sint√°cticas
o
basadas
en
la
gram√°tica
(
por
ejemplo
,
la
relaci√≥n
entre
‚Äú
ten√≠a
‚Äù
y
‚Äú
tengo
‚Äù
es
la
mismo
que
entre
‚Äú
era
‚Äù
y
‚Äú
es
‚Äù
)
.



La
comunidad
r√°pidamente
se
dio
cuenta
de
que
era
mucho
mejor
idea
usar
embeddings
previamente
entrenados
con
grandes
cantidades
de
datos
de
texto
,
en
lugar
de
entrenarlas
junto
con
el
modelo
en
lo
que
con
frecuencia
eran
peque√±os
conjuntos
de
datos
.
De
esta
 
manera
uno
descarga
una
lista
de
palabras
y
sus
embeddings
a
partir
del
entrenamiento
previo
con
Word2Vec
o
GloVe
.
Este
es
un
ejemplo
del
embedding
GloVe
de
la
palabra
‚Äú
palo
‚Äù
(
con
un
tama√±o
de
vector
de
embedding
de
200
)



Embedding
GloVe
de
la
palabra
‚Äú
palo
‚Äù
:
un
vector
de
200
floats
(
redondeados
a
dos
decimales
)
.
Contin√∫a
con
m√°s
de
doscientos
valores
.



Dada
su
longitud
y
la
gran
cantidad
de
n√∫meros
,
en
las
ilustraciones
de
mis
publicaciones
utilizo
esta
cuadr√≠cula
simplificada
para
representar
vectores
:



ELMo
:
el
contexto
importa



Si
estamos
usando
esta
representaci√≥n
GloVe
,
entonces
la
palabra
‚Äú
palo
‚Äù
estar√≠a
representada
por
este
vector
sin
importar
el
contexto
.
Sin
embargo
,
es
claro
que
esto
es
un
problema.(Peters
et
.
al
.
,
2017
,
McCann
et
.
al
.
,
2017
,
y
una
vez
m√°s
Peters
et
.
al
.
,
2018
en
el
art√≠culo
de
ELMo
)
.
"
Palo
"
tiene
m√∫ltiples
significados
dependiendo
de
d√≥nde
se
use
.
¬ø
Por
qu√©
no
darle
un
embedding
basado
en
el
contexto
en
el
que
se
usa
,
tanto
para
capturar
el
significado
de
la
palabra
en
ese
contexto
como
para
otra
informaci√≥n
contextual
?
‚Äù
.
Y
as√≠
nacieron
los
embeddings
de
palabras
contextualizadas
.



Los
embeddings
de
palabras
contextualizadas
pueden
dar
a
las
palabras
embeddings
diferentes
seg√∫n
el
significado
que
tengan
en
el
contexto
de
la
oraci√≥n
.



En
lugar
de
usar
un
embedding
fijo
para
cada
palabra
,
ELMo
analiza
la
oraci√≥n
completa
antes
de
asignarle
un
embedding
a
cada
palabra
.
Utiliza
un
LSTM
bidireccional
entrenado
en
una
tarea
espec√≠fica
para
poder
crear
esos
embedding
.



ELMo
supuso
un
paso
significativo
hacia
la
formaci√≥n
previa
en
el
contexto
de
la
NLP
.
El
ELMo
LSTM
se
entrenar√≠a
en
un
conjunto
de
datos
masivo
en
el
idioma
de
nuestro
conjunto
de
datos
,
y
luego
podemos
usarlo
como
componente
en
otros
modelos
que
necesitan
manejar
el
idioma
.



¬ø
Cu√°l
es
el
secreto
de
ELMo
?



ELMo
adquiri√≥
su
comprensi√≥n
del
idioma
al
ser
entrenado
para
predecir
la
siguiente
palabra
en
una
secuencia
de
palabras
,
una
tarea
llamada
Modelado
del
Lenguaje
.
Esto
es
muy
interesante
porque
tenemos
grandes
cantidades
de
datos
de
texto
de
los
que
dicho
modelo
puede
aprender
sin
necesidad
de
etiquetas
.



Uno
de
los
pasos
en
el
proceso
de
pre-entrenamiento
de
ELMo
:
dado
‚Äú
Lets
stick
to
‚Äù
como
entrada
,
predecir
la
siguiente
palabra
m√°s
probable
:
una
tarea
de
Modelado
de
Lenguaje
.
Cuando
se
entrena
en
un
gran
conjunto
de
datos
,
el
modelo
comienza
a
captar
patrones
de
lenguaje
.
Es
poco
probable
que
adivine
con
precisi√≥n
la
siguiente
palabra
en
este
ejemplo
.
De
manera
m√°s
realista
,
despu√©s
de
una
palabra
como
‚Äú
pasar
‚Äù
,
asignar√°
una
mayor
probabilidad
a
una
palabra
como
‚Äú
tiempo
‚Äù
(
para
conformar
‚Äú
pasar
tiempo
‚Äù
)
que
a
‚Äú
c√°mara
‚Äù
.



Podemos
entrever
cada
uno
de
los
pasos
de
LSTM
asomando
por
detr√°s
de
la
cabeza
de
ELMo
.
Resultan
muy
√∫tiles
para
generar
embeddings
de
que
realizar
este
entrenamiento
previo
.



ELMo
en
realidad
va
un
paso
m√°s
all√°
y
entrena
un
LSTM
bidireccional
,
de
modo
que
su
modelo
de
lenguaje
no
solo
se
hace
una
idea
de
la
palabra
siguiente
,
sino
tambi√©n
de
la
palabra
anterior
.



ELMo
llega
al
 
embedding
contextualizado
mediante
la
agrupaci√≥n
de
los
estados
ocultos
(
y
el
embedding
inicial
)
de
cierta
manera
(
concatenaci√≥n
seguida
de
suma
ponderada
)
.



ULM-FiT
:
Transferencia
de
Aprendizaje
en
PNLP



ULM-FiT
introdujo
nuevos
m√©todos
para
utilizar
de
manera
m√°s
efectiva
mucho
de
lo
que
el
modelo
aprende
durante
el
entrenamiento
previo
,
m√°s
all√°
de
los
meros
embeddings
e
embeddings
contextualizados
.
ULM-FiT
introdujo
un
nuevo
modelo
de
lenguaje
y
un
proceso
para
ajustar
efectivamente
ese
modelo
de
lenguaje
para
resolver
varias
tareas
.


El
Transformer
:
yendo
m√°s
all√°
que
los
LSTM



La
publicaci√≥n
del
paper
y
el
c√≥digo
de
Transformer
,
y
los
resultados
que
logr√≥
en
tareas
como
la
traducci√≥n
autom√°tica
,
comenzaron
a
hacer
que
algunos
pensaran
en
ellos
como
un
reemplazo
de
los
LSTM
.
Esto
se
vio
agravado
por
el
hecho
de
que
los
Transformers
manejan
las
dependencias
a
largo
plazo
mejor
que
los
LSTM
.



La
estructura
Encoder-Decoder
del
Transformer
lo
hizo
perfecto
para
la
traducci√≥n
autom√°tica
.
Pero
,
¬ø
c√≥mo
lo
usar√≠as
para
la
clasificaci√≥n
de
oraciones
?
¬ø
C√≥mo
lo
usar√≠a
para
entrenar
previamente
un
modelo
de
idioma
que
se
puede
ajustar
para
otras
tareas
(
tareas
posteriores
es
lo
que
el
campo
llama
tareas
de
aprendizaje
supervisado
que
utilizan
un
modelo
o
componente
entrenado
previamente
)
.


OpenAI
Transformer
:
pre-entrenando
un
decoder
de
Transformer
para
Modelado
de
Lenguaje



Resulta
que
no
necesitamos
un
Transformer
completo
para
adoptar
transferencia
del
aprendizaje
y
un
modelo
de
lenguaje
ajustable
para
tareas
de
NLP
.
Podemos
hacerlo
solo
con
el
decoder
del
Transformer
.
El
decoder
es
una
buena
opci√≥n
porque
es
una
opci√≥n
natural
para
el
modelado
del
lenguaje
(
predecir
la
siguiente
palabra
)
ya
que
est√°
dise√±ado
para
enmascarar
tokens
futuros
,
una
caracter√≠stica
valiosa
cuando
genera
una
traducci√≥n
palabra
por
palabra
.



El
transformer
OpenAI
est√°
integrado
por
la
pila
de
decoders
del
Transformer



El
modelo
apil√≥
doce
capas
de
decoder
.
Dado
que
no
hay
encoder
en
esta
configuraci√≥n
,
estas
capas
de
decoder
no
tendr√≠an
la
subcapa
de
atenci√≥n
de
encoder-decoder
que
tienen
las
capas
de
decoder
de
Transformer
est√°ndar
.
Sin
embargo
,
todav√≠a
tendr√≠a
la
capa
de
auto
atenci√≥n
(
enmascarada
para
que
no
alcance
su
punto
m√°ximo
en
tokens
futuros
)
.



Con
esta
estructura
,
podemos
proceder
a
entrenar
el
modelo
en
la
misma
tarea
de
modelado
de
lenguaje
:
predecir
la
siguiente
palabra
utilizando
conjuntos
de
datos
masivos
sin
etiquetar
.
¬°
M√©tele
7.000
libros
y
que
aprenda
!
Los
libros
son
excelentes
para
este
tipo
de
tareas
,
ya
que
permiten
que
el
modelo
aprenda
a
asociar
informaci√≥n
relacionada
,
incluso
si
est√°n
separados
por
mucho
texto
,
algo
que
no
se
obtiene
,
por
ejemplo
,
cuando
se
entrena
con
tweets
o
art√≠culos
.



OpenAI
Transformer
listo
para
ser
entrenado
para
predecir
la
siguiente
palabra
,
con
un
conjunto
de
datos
compuesto
por
7.000
libros
.



Transferencia
del
Aprendizaje
a
tareas
posteriores



Ahora
que
el
Transformer
OpenAI
est√°
preentrenado
y
sus
capas
se
han
ajustado
para
manejar
razonablemente
el
lenguaje
,
podemos
comenzar
a
usarlo
para
tareas
posteriores
.
Primero
veamos
la
clasificaci√≥n
de
oraciones
(
clasificar
un
mensaje
de
correo
electr√≥nico
como
‚Äú
spam
‚Äù
o
‚Äú
no
spam
‚Äù
):



C√≥mo
usar
un
Transformer
OpenAI
preentrenado
para
clasificar
oraciones



El
documento
de
OpenAI
describe
una
serie
de
transformaciones
de
entrada
para
manejar
las
entradas
para
diferentes
tipos
de
tareas
.
La
siguiente
imagen
del
paper
muestra
las
estructuras
de
los
modelos
y
las
transformaciones
de
entrada
para
llevar
a
cabo
diferentes
tareas
.



Muy
inteligente
,
no
?


BERT
:
de
Decodificadores
a
Codificadores



El
Transformer
openAI
nos
brind√≥
un
modelo
preentrenado
ajustable
con
precisi√≥n
basado
en
el
Transformer
.
Pero
faltaba
algo
en
esta
transici√≥n
de
LSTM
a
Transformers
.
El
modelo
de
lenguaje
de
ELMo
era
bidireccional
,
pero
el
Transformer
openAI
solo
entrena
un
modelo
de
lenguaje
directo
.
¬ø
Podr√≠amos
construir
un
modelo
basado
en
Transformers
cuyo
modelo
de
lenguaje
mire
tanto
hacia
adelante
como
hacia
atr√°s
(
en
la
jerga
t√©cnica
,
‚Äú
est√©
condicionado
tanto
en
el
contexto
izquierdo
como
en
el
derecho
‚Äù
)
?



Modelo
de
lenguaje
enmascarado



Todo
el
mundo
sabe
que
el
condicionamiento
bidireccional
permitir√≠a
que
cada
palabra
se
viera
indirectamente
en
un
contexto
de
varias
capas
.
Por
eso
no
se
usa
encoders
para
esta
tarea
...
¬ø
Pero
que
pasa
si
usamos
m√°scaras
?



El
proceso
de
modelado
de
lenguaje
inteligente
de
BERT
enmascara
el
15%
de
las
palabras
en
la
entrada
y
le
pide
al
modelo
que
prediga
la
palabra
que
falta
.



Es
literalmente
la
tarea
de
cualquier
libro
de
texto
de
lengua
extranjera
.
Completar
con
la
preposici√≥n
correcta
,
con
la
conjugaci√≥n
correcta
.
Solo
que
ahora
es
como
si
le
dijeramos
al
alumno
.
Podes
usar
todo
el
diccionario
.



Encontrar
la
tarea
correcta
para
entrenar
una
pila
de
codificadores
de
Transformer
es
un
obst√°culo
complejo
que
BERT
resuelve
adoptando
un
concepto
de
‚Äú
modelo
de
lenguaje
enmascarado
‚Äù
de
la
literatura
anterior
(
donde
se
denomina
tarea
Cloze
)
.



M√°s
all√°
de
enmascarar
el
15%
de
la
entrada
,
BERT
tambi√©n
mezcla
un
poco
las
cosas
para
mejorar
la
forma
en
que
el
modelo
se
ajusta
m√°s
tarde
.
A
veces
reemplaza
aleatoriamente
una
palabra
con
otra
palabra
y
le
pide
al
modelo
que
prediga
la
palabra
correcta
en
esa
posici√≥n
.


Tareas
de
dos
oraciones



Si
miramos
atr√°s
en
las
transformaciones
de
entrada
que
hace
el
Transformer
OpenAI
para
gestionar
diferentes
tareas
,
notaremos
que
algunas
tareas
requieren
que
el
modelo
diga
algo
inteligente
sobre
dos
oraciones
(
por
ejemplo
,
¬ø
son
simplemente
versiones
parafraseadas
una
de
la
otra
?
Dada
una
entrada
de
wikipedia
como
entrada
,
y
una
pregunta
sobre
esa
entrada
como
otra
entrada
,
¬ø
podemos
responder
esa
pregunta
?
)
.



Para
hacer
que
BERT
sea
mejor
en
el
manejo
de
relaciones
entre
m√∫ltiples
oraciones
,
el
proceso
de
entrenamiento
previo
incluye
una
tarea
adicional
:
dadas
dos
oraciones
(
A
y
B
)
,
¬ø
es
probable
que
B
sea
la
oraci√≥n
que
sigue
a
A
,
o
no
?



La
segunda
tarea
en
la
que
BERT
est√°
pre-entrenado
es
una
tarea
de
clasificaci√≥n
de
dos
oraciones
.
La
tokenizaci√≥n
esta
muy
simplificada
en
este
gr√°fico
,
ya
que
BERT
en
realidad
usa
WordPieces
como
tokens
en
lugar
de
palabras
,
por
lo
que
algunas
palabras
se
dividen
en
fragmentos
m√°s
peque√±os
.


Modelos
espec√≠ficos
de
tareas



El
paper
de
BERT
muestra
varias
formas
de
usar
BERT
para
diferentes
tareas
.



BERT
para
extracci√≥n
de
caracter√≠sticas



El
enfoque
de
Fine-Tuning
no
es
la
√∫nica
forma
de
utilizar
BERT
.
Al
igual
que
ELMo
,
se
puede
usar
BERT
previamente
entrenado
para
crear
embeddings
de
palabras
contextualizadas
.
Luego
,
se
puede
alimentar
estos
embeddings
a
un
modelo
existente
,
un
proceso
que
el
paper
muestra
que
produce
resultados
no
muy
lejanos
del
Fine-Tuning
de
BERT
en
tareas
como
el
reconocimiento
de
entidades
.



¬ø
Qu√©
vector
funciona
mejor
como
embeddig
contextualizada?Depende
de
la
tarea
.
El
documento
examina
seis
opciones
(
en
comparaci√≥n
con
el
modelo
ajustado
que
logr√≥
una
puntuaci√≥n
de
96,4
):
