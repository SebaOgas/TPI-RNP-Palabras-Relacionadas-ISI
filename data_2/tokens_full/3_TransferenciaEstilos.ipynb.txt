<
a
href="https://colab.research.google.com
/
github
/
institutohumai
/
cursos-python
/
blob
/
master
/
CV/4TransferLearning/3_TransferenciaEstilos.ipynb
"
>
<
img
src='https://colab.research.google.com
/
assets
/
colab-badge.svg
'
/
>
<
/a
>



Transferencia
de
Estilos



Si
es
un
entusiasta
de
la
fotografía
,
es
posible
que
esté
familiarizado
con
los
filtros
.
Los
filtros
pueden
cambiar
el
estilo
de
color
de
las
fotos
para
que
las
fotos
de
paisajes
se
vuelvan
más
nítidas
o
las
fotos
de
retratos
tengan
pieles
más
blancas
.
Sin
embargo
,
un
filtro
generalmente
solo
cambia
un
aspecto
de
la
foto
.
Para
aplicar
un
estilo
ideal
a
una
foto
,
probablemente
necesite
probar
muchas
combinaciones
de
filtros
diferentes
.
Este
proceso
es
tan
complejo
como
ajustar
los
hiperparámetros
de
un
modelo
.



En
esta
clase
,
aprovecharemos
las
representaciones
por
capas
de
una
CNN
para
aplicar
automáticamente
el
estilo
de
una
imagen
a
otra
imagen
,
es
decir
,
haremos
transferencias
de
estilos
.



Esta
tarea
necesita
dos
imágenes
de
entrada
:
una
es
la
imagen
de
contenido
y
la
otra
es
la
imagen
de
estilo
.
Usaremos
redes
neuronales
para
modificar
la
imagen
del
contenido
para
que
su
estilo
se
acerque
al
de
la
imagen
de
estilo
.
Por
ejemplo
,
la
imagen
de
contenido
en
la
figura
es
una
foto
de
un
paisaje
mientras
que
la
imagen
de
estilo
es
una
pintura
al
óleo
de
unos
robles
otoñales
.
En
la
imagen
sintetizada
de
salida
,
se
aplican
los
trazos
de
pincel
al
óleo
de
la
imagen
de
estilo
,
lo
que
da
lugar
a
colores
más
vivos
,
al
tiempo
que
conserva
la
forma
principal
de
los
objetos
en
la
imagen
de
contenido
.



Método



La
siguiente
figura
ilustra
el
método
de
transferencia
de
estilo
basado
en
CNN
con
un
ejemplo
simplificado
.
Primero
,
inicializamos
la
imagen
sintetizada
,
por
ejemplo
,
en
la
imagen
de
contenido
.
Esta
imagen
sintetizada
es
la
única
variable
que
debe
actualizarse
durante
el
proceso
de
transferencia
de
estilo
,
es
decir
,
los
parámetros
del
modelo
que
se
actualizarán
durante
el
entrenamiento
.
Luego
,
elegimos
una
CNN
preentrenada
para
extraer
las
características
de
la
imagen
y
congelamos
los
parámetros
de
su
modelo
durante
el
entrenamiento
.
Esta
CNN
profunda
utiliza
varias
capas
para
extraer
características
jerárquicas
de
las
imágenes
.
Podemos
elegir
la
salida
de
algunas
de
estas
capas
como
características
de
contenido
o
características
de
estilo
.
En
el
ejemplo
de
la
figura
,
la
red
neuronal
preentrenada
aquí
tiene
3
capas
convolucionales
,
donde
la
segunda
capa
genera
las
características
de
contenido
y
la
primera
y
la
tercera
capas
generan
las
características
de
estilo
.



A
continuación
,
calculamos
la
función
de
pérdida
de
la
transferencia
de
estilo
a
través
de
la
propagación
hacia
adelante
(
dirección
de
las
flechas
continuas
)
y
actualizamos
los
parámetros
del
modelo
(
la
imagen
sintetizada
para
la
salida
)
a
través
de
la
propagación
hacia
atrás
(
dirección
de
las
flechas
discontinuas
)
.



La
función
de
pérdida
comúnmente
utilizada
en
la
transferencia
de
estilo
consta
de
tres
partes
:


la
pérdida
de
contenido
hace
que
la
imagen
sintetizada
y
la
imagen
de
contenido
se
acerquen
en
características
de
contenido
;


la
pérdida
de
estilo
hace
que
la
imagen
sintetizada
y
la
imagen
de
estilo
se
acerquen
en
características
de
estilo
;
y


la
pérdida
de
variación
total
ayuda
a
reducir
el
ruido
en
la
imagen
sintetizada
.



Finalmente
,
cuando
finaliza
el
entrenamiento
del
modelo
,
generamos
los
parámetros
del
modelo
de
la
transferencia
de
estilo
para
generar
la
imagen
sintetizada
final
.
A
continuación
,
explicaremos
los
detalles
técnicos
de
la
transferencia
de
estilo
a
través
de
un
experimento
concreto
.



Implementación


Leer
las
imágenes
de
estilo
y
de
contenido



Primero
,
leemos
el
contenido
y
el
estilo
de
las
imágenes
.
A
partir
de
sus
ejes
de
coordenadas
impresos
,
podemos
decir
que
estas
imágenes
tienen
diferentes
tamaños
.



Preprocesamiento
y
posprocesamiento



A
continuación
,
definimos
dos
funciones
para
el
preprocesamiento
y
el
posprocesamiento
de
imágenes
.
La
función
preprocess
estandariza
cada
uno
de
los
tres
canales
RGB
de
la
imagen
de
entrada
y
transforma
los
resultados
al
formato
de
entrada
CNN
.
La
función
postprocess
restaura
los
valores
de
píxeles
en
la
imagen
de
salida
a
sus
valores
originales
antes
de
la
estandarización
.
Dado
que
la
función
de
impresión
de
imágenes
requiere
que
cada
píxel
tenga
un
valor
de
punto
flotante
de
0
a
1
,
reemplazamos
cualquier
valor
menor
que
0
o
mayor
que
1
con
0
o
1
,
respectivamente
.



Extracción
de
características



Usamos
el
modelo
VGG-19
preentrenado
en
el
conjunto
de
datos
de
ImageNet
para
extraer
las
características
de
la
imagen
.



Para
extraer
las
características
de
contenido
y
las
características
de
estilo
de
la
imagen
,
podemos
seleccionar
la
salida
de
ciertas
capas
en
la
red
VGG
.
En
términos
generales
,
cuanto
más
cerca
de
la
capa
de
entrada
,
más
fácil
extraer
detalles
de
la
imagen
y
mientras
más
lejos
,
más
fácil
extraer
la
información
global
de
la
imagen
.
Para
evitar
retener
excesivamente
los
detalles
de
la
imagen
de
contenido
en
la
imagen
sintetizada
,
elegimos
una
capa
VGG
que
está
más
cerca
de
la
salida
como
capa
de
contenido
para
generar
las
características
de
contenido
de
la
imagen
.
También
seleccionamos
la
salida
de
diferentes
capas
VGG
para
extraer
características
de
estilo
locales
y
globales
.
Estas
capas
también
se
denominan
capas
de
estilo
.



La
red
VGG
usa
5
bloques
convolucionales
.
En
este
experimento
,
elegimos
la
última
capa
convolucional
del
cuarto
bloque
convolucional
como
la
capa
de
contenido
y
la
primera
capa
convolucional
de
cada
bloque
convolucional
como
la
capa
de
estilo
.
Los
índices
de
estas
capas
se
pueden
obtener
imprimiendo
la
instancia
pretrained_net
.



Al
extraer
características
usando
capas
VGG
,
solo
necesitamos
usar
todas
aquellas
desde
la
capa
de
entrada
hasta
la
capa
de
contenido
o
la
capa
de
estilo
más
cercana
a
la
capa
de
salida
.
Construyamos
una
nueva
red
net
,
que
solo
conserve
las
capas
VGG
que
se
utilizarán
para
la
extracción
de
características
.



Dada
la
entrada
X
,
si
simplemente
invocamos
la
función
forward
net(X
)
,
solo
podemos
obtener
la
salida
de
la
última
capa
.
Dado
que
también
necesitamos
las
salidas
de
las
capas
intermedias
,
debemos
realizar
cálculos
capa
por
capa
y
mantener
las
salidas
de
las
capas
de
contenido
y
estilo
.



A
continuación
se
definen
dos
funciones
:
la
función
getcontents
extrae
características
de
contenido
de
la
imagen
de
contenido
y
la
función
getstyles
extrae
características
de
estilo
de
la
imagen
de
estilo
.
Dado
que
no
es
necesario
actualizar
los
parámetros
del
modelo
VGG
preentrenado
durante
el
entrenamiento
,
podemos
extraer
el
contenido
y
las
características
de
estilo
incluso
antes
de
que
comience
el
entrenamiento
.
Dado
que
la
imagen
sintetizada
sí
es
un
conjunto
de
parámetros
del
modelo
que
se
actualizará
para
la
transferencia
de
estilo
,
solo
podemos
extraer
el
contenido
y
las
características
de
estilo
de
la
imagen
sintetizada
llamando
a
la
función
extract_features
durante
el
entrenamiento
.



Definición
de
la
función
de
pérdida



Ahora
describiremos
la
función
de
pérdida
para
la
transferencia
de
estilo
.
La
función
de
pérdida
consiste
en
la
pérdida
de
contenido
,
la
pérdida
de
estilo
y
la
pérdida
de
variación
total
.



Pérdida
de
contenido



Similar
a
la
función
de
pérdida
en
la
regresión
lineal
,
la
pérdida
de
contenido
mide
la
diferencia
en
las
características
de
contenido
entre
la
imagen
sintetizada
y
la
imagen
de
contenido
a
través
del
error
cuadrático
medio
.



Pérdida
de
estilo



La
pérdida
de
estilo
,
similar
a
la
pérdida
de
contenido
,
también
utiliza
el
error
cuadrático
medio
para
medir
la
diferencia
de
estilo
entre
la
imagen
sintetizada
y
la
imagen
de
estilo
.
Para
expresar
la
salida
de
estilo
de
cualquier
capa
de
estilo
,
primero
usamos
la
función
extractfeatures
para
calcular
la
salida
de
la
capa
de
estilo
.
Suponiendo
que
la
salida
tiene
1
ejemplo
,
 
canales
,
altura
 
y
ancho
,
podemos
transformar
esta
salida
en
una
matriz
 
con
 
filas
y
 
columnas
.
Esta
matriz
se
puede
considerar
como
la
concatenación
de
 
vectores
,
cada
uno
de
los
cuales
tiene
una
longitud
de
.
Aquí
,
el
vector
 
representa
la
función
de
estilo
del
canal
.



Luego
crearemos
una
matriz
de
Gram
a
partir
de
estos
vectores
.
En
esta
matriz
,
el
elemento
 
en
la
fila
 
y
la
columna
 
es
el
producto
escalar
de
los
vectores
 
y
.
Esto
representa
la
correlación
de
las
características
de
estilo
de
los
canales
 
y
.



Usamos
esta
matriz
de
Gram
para
representar
la
salida
de
estilo
de
cualquier
capa
de
estilo
.
Tenga
en
cuenta
que
cuando
el
valor
de
 
es
mayor
,
es
probable
que
genere
valores
mayores
en
la
matriz
de
Gram
.
Tenga
en
cuenta
también
que
el
alto
y
el
ancho
de
la
matriz
Gram
son
ambos
el
número
de
canales
.
Para
permitir
que
la
pérdida
de
estilo
no
se
vea
afectada
por
estos
valores
,
la
siguiente
función
gram
divide
la
matriz
Gram
por
el
número
de
sus
elementos
,
es
decir
,
.



Obviamente
,
las
dos
matrices
de
Gram
que
son
entradas
de
la
pérdida
de
estilo
se
basan
en
las
salidas
de
la
capa
de
estilo
para
la
imagen
sintetizada
y
la
imagen
de
estilo
.
Aquí
se
supone
que
la
matriz
de
Gram
gram_Y
basada
en
la
imagen
de
estilo
ha
sido
precalculada
.



Pérdida
de
variación
total



A
veces
,
la
imagen
sintetizada
aprendida
tiene
mucho
ruido
de
alta
frecuencia
,
es
decir
,
píxeles
particularmente
brillantes
u
oscuros
.
Un
método
común
de
reducción
de
ruido
es
la
eliminación
de
ruido
de
variación
total
.
Denote
por
 
el
valor
del
píxel
en
la
coordenada
.
Reducir
la
pérdida
de
variación
total
definida
como



acerca
los
valores
de
los
píxeles
vecinos
en
la
imagen
sintetizada
.



Función
de
pérdida



La
función
de
pérdida
de
la
transferencia
de
estilo
es
la
suma
ponderada
de
la
pérdida
de
contenido
,
la
pérdida
de
estilo
y
la
pérdida
de
variación
total
.
Al
ajustar
estos
hiperparámetros
de
peso
,
podemos
equilibrar
la
retención
de
contenido
,
la
transferencia
de
estilo
y
la
reducción
de
ruido
en
la
imagen
sintetizada
.



Inicializar
la
imagen
sintetizada



En
la
transferencia
de
estilo
,
la
imagen
sintetizada
es
la
única
variable
que
debe
actualizarse
durante
el
entrenamiento
.
Por
lo
tanto
,
podemos
definir
un
modelo
simple
,
SynthesizedImage
,
y
tratar
a
la
imagen
sintetizada
como
los
parámetros
del
modelo
.
En
este
modelo
,
la
propagación
directa
solo
devuelve
los
parámetros
del
modelo
.



A
continuación
,
definimos
la
función
getinits
.
Esta
función
crea
una
instancia
de
modelo
de
SynthesizedImage
y
la
inicializa
en
la
imagen
X.
Las
matrices
Gram
para
la
imagen
de
estilo
en
varias
capas
de
estilo
,
stylesY_gram
,
se
calculan
antes
del
entrenamiento
.



Entrenamiento


Al
entrenar
el
modelo
para
la
transferencia
de
estilo
,
extraemos
continuamente
características
de
contenido
y
características
de
estilo
de
la
imagen
sintetizada
y
calculamos
la
función
de
pérdida
.
A
continuación
se
define
el
ciclo
de
entrenamiento
.



Ahora
comenzamos
a
entrenar
el
modelo
.
Redimensionamos
la
altura
y
el
ancho
de
las
imágenes
de
estilo
y
contenido
a
300
por
450
píxeles
.
Usamos
la
imagen
de
contenido
para
inicializar
la
imagen
sintetizada
.



Podemos
ver
que
la
imagen
sintetizada
retiene
el
escenario
y
los
objetos
de
la
imagen
de
contenido
y
transfiere
el
color
de
la
imagen
de
estilo
al
mismo
tiempo
.
Por
ejemplo
,
la
imagen
sintetizada
tiene
bloques
de
color
como
los
de
la
imagen
de
estilo
.
Algunos
de
estos
bloques
incluso
tienen
la
sutil
textura
de
las
pinceladas
.
