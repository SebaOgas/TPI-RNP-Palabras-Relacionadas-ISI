Segment
Anything
Model
(
SAM
)



El
Segment
Anything
Model
(
SAM
)
de
Meta
es
un
modelo
fundacional
de
segmentación
de
imágenes
que
destaca
por
su
capacidad
de
identificar
y
segmentar
cualquier
objeto
en
una
imagen
sin
necesidad
de
entrenamiento
adicional
o
categorías
predefinidas
.
Utiliza
una
red
neuronal
profunda
entrenada
en
una
vasta
variedad
de
imágenes
,
lo
que
le
permite
ofrecer
segmentaciones
precisas
y
versátiles
de
manera
inmediata
.
Esta
característica
lo
hace
una
herramienta
poderosa
y
flexible
para
aplicaciones
en
edición
de
imágenes
,
robótica
,
realidad
aumentada
y
análisis
de
imágenes
médicas
,
entre
otros
campos
tecnológicos
y
científicos
.



Anteriormente
,
para
resolver
cualquier
tipo
de
problema
de
segmentación
,
había
dos
clases
de
enfoques
.
El
primero
,
la
segmentación
interactiva
,
permitía
segmentar
cualquier
clase
de
objeto
,
pero
requería
que
una
persona
guiara
el
método
refinando
iterativamente
una
máscara
.
El
segundo
,
la
segmentación
automática
,
permitía
la
segmentación
de
categorías
específicas
de
objetos
definidas
de
antemano
(
por
ejemplo
,
gatos
o
sillas
)
,
pero
requería
cantidades
sustanciales
de
objetos
anotados
manualmente
para
entrenar
(
por
ejemplo
,
miles
o
incluso
decenas
de
miles
de
ejemplos
de
gatos
segmentados
)
,
junto
con
los
recursos
computacionales
y
la
experiencia
técnica
para
entrenar
el
modelo
de
segmentación
.
Ningún
enfoque
proporcionaba
una
solución
general
y
totalmente
automática
para
la
segmentación
.



SAM
es
una
generalización
de
estas
dos
clases
de
enfoques
.
Es
un
único
modelo
que
puede
realizar
fácilmente
tanto
segmentación
interactiva
como
segmentación
automática
.
La
interfaz
de
SAM
que
se
puede
orientar
mediante
prompts
(
que
se
describirán
en
breve
)
permite
usarlo
de
maneras
flexibles
que
hacen
posibles
una
amplia
gama
de
tareas
de
segmentación
simplemente
diseñando
el
prompt
adecuado
para
el
modelo
(
clics
,
cajas
,
texto
,
etcétera
)
.
Además
,
SAM
está
entrenado
en
un
conjunto
de
datos
diverso
y
de
alta
calidad
que
incluye
más
de
1000
millones
de
máscaras
(
recopiladas
como
parte
de
este
proyecto
)
,
lo
que
le
permite
generalizar
a
nuevos
tipos
de
objetos
e
imágenes
más
allá
de
lo
que
observó
durante
el
entrenamiento
.
Esta
capacidad
de
generalizar
significa
que
,
en
gran
medida
,
los
practicantes
ya
no
necesitarán
recopilar
sus
propios
datos
de
segmentación
y
ajustar
un
modelo
para
su
caso
de
uso
.



En
conjunto
,
estas
capacidades
permiten
que
SAM
generalice
tanto
a
nuevas
tareas
como
a
nuevos
dominios
.
Esta
flexibilidad
es
la
primera
de
su
tipo
para
la
segmentación
de
imágenes
.



Acá
tenés
un
video
corto
que
muestra
algunas
de
las
capacidades
de
SAM
:



(
1
)
SAM
permite
a
los
usuarios
segmentar
objetos
con
solo
un
clic
o
interactuando
mediante
clics
en
puntos
para
incluir
y
excluir
del
objeto
.
El
modelo
también
puede
recibir
un
cuadro
delimitador
como
prompt
.



(
2
)
SAM
puede
generar
múltiples
máscaras
válidas
cuando
enfrenta
ambigüedades
sobre
el
objeto
que
se
está
segmentando
,
una
capacidad
importante
y
necesaria
para
resolver
la
segmentación
en
el
mundo
real
.



(
3
)
SAM
puede
encontrar
y
enmascarar
automáticamente
todos
los
objetos
en
una
imagen
.



(
4
)
SAM
puede
generar
una
máscara
de
segmentación
para
cualquier
prompt
en
tiempo
real
después
de
precomputar
el
embbedding
de
la
imagen
,
permitiendo
la
interacción
en
tiempo
real
con
el
modelo
.



Cómo
funciona
SAM
:
Segmentación
orientada
por
prompts



En
el
procesamiento
del
lenguaje
natural
y
,
más
recientemente
,
en
la
visión
por
computadora
,
uno
de
los
desarrollos
más
emocionantes
es
el
de
los
modelos
fundacionales
que
pueden
realizar
zero-shot
y
few-shot
learning
para
nuevos
conjuntos
de
datos
y
tareas
utilizando
técnicas
de
"
prompting
"
.
Los
creadores
de
SAM
se
inspiraron
en
esta
línea
de
trabajo
.



Ellos
entrenaron
a
SAM
para
devolver
una
máscara
de
segmentación
válida
para
cualquier
prompt
,
donde
un
prompt
puede
ser
:


puntos
de
primer
plano
/
fondo
,


una
bounding
box
aproximada
,


una
máscara
aproximada
o


texto
libre
.



El
requisito
de
una
máscara
válida
simplemente
significa
que
incluso
cuando
un
prompt
es
ambiguo
y
podría
referirse
a
múltiples
objetos
(
por
ejemplo
,
un
punto
en
una
camisa
puede
indicar
la
camisa
o
la
persona
que
la
lleva
puesta
)
,
el
resultado
debe
ser
una
máscara
razonable
para
uno
de
esos
objetos
.
Esta
tarea
se
utiliza
para
preentrenar
el
modelo
y
para
resolver
tareas
generales
de
segmentación
mediante
prompting
.



Los
creadores
de
SAM
observaron
que
la
tarea
de
preentrenamiento
y
la
recolección
de
datos
interactiva
impusieron
restricciones
específicas
en
el
diseño
del
modelo
.
En
particular
,
el
modelo
necesita
funcionar
en
tiempo
real
en
un
CPU
en
un
navegador
web
para
permitir
que
los
anotadores
usen
SAM
de
manera
interactiva
en
tiempo
real
y
anoten
eficientemente
.
Aunque
la
restricción
de
tiempo
de
ejecución
implica
una
compensación
entre
la
calidad
y
el
tiempo
de
ejecución
,
encontraron
que
un
diseño
simple
produce
buenos
resultados
en
la
práctica
.



Bajo
el
capó
,
un
encoder
de
imágenes
produce
un
embedding
único
para
la
imagen
,
mientras
que
otro
encoder
liviano
convierte
cualquier
prompt
en
un
embedding
en
tiempo
real
.
Estas
dos
fuentes
de
información
se
combinan
luego
en
un
decoder
liviano
que
predice
las
máscaras
de
segmentación
.
Después
de
que
se
calcula
el
embedding
de
la
imagen
,
SAM
puede
producir
una
segmentación
en
solo
50
milisegundos
para
cualquier
prompt
en
un
navegador
web
.



Arquitectura
de
la
Red
de
SAM



La
arquitectura
de
la
red
del
Segment
Anything
Model
(
SAM
)
contiene
tres
componentes
cruciales
:
el
Image
Encoder
,
el
Prompt
Encoder
y
el
Mask
Decoder
.


Image
Encoder
:
En
el
nivel
más
alto
,
un
image
encoder
(
un
autoencoder
enmascarado
,
MAE
,
preentrenado
con
Vision
Transformer
,
ViT
)
genera
embeddings
de
imágenes
de
una
sola
vez
.
Se
aplica
antes
de
hacer
el
prompting
del
modelo
.


Prompt
Encoder
:
El
prompt
encoder
codifica
puntos
de
fondo
,
máscaras
,
bounding
boxes
o
textos
en
un
vector
de
embedding
en
tiempo
real
.
La
investigación
considera
dos
conjuntos
de
prompts
:
escasos
(
puntos
,
cuadros
,
texto
)
y
densos
(
máscaras
)
.
Los
puntos
y
boxes
se
representan
mediante
codificaciones
posicionales
y
se
suman
con
embeddings
aprendidos
para
cada
tipo
de
prompt
.
Los
prompts
de
texto
libre
se
representan
con
un
text
encoder
preexistente
de
CLIP
.
Los
prompts
densos
,
como
las
máscaras
,
se
embeben
con
convoluciones
y
se
suman
elemento
por
elemento
con
el
embedding
de
la
imagen
.


Mask
Decoder
:
Un
mask
decoder
ligero
predice
las
máscaras
de
segmentación
basándose
en
los
embeddings
tanto
del
image
encoder
como
del
prompt
encoder
.
SAM
utiliza
un
bloque
decoder
modificado
y
luego
una
cabeza
de
predicción
de
máscara
dinámica
,
inspirándose
en
bloques
de
decodificadores
de
Transformer
ya
existentes
.
Este
diseño
incorpora
mecanismos
de
self-attention
y
cross-attention
en
dos
direcciones
(
del
prompt
al
embedding
de
imagen
y
viceversa
)
para
actualizar
todos
los
embeddings
de
manera
efectiva
.





El
diseño
general
de
SAM
prioriza
la
eficiencia
,
con
el
prompt
encoder
y
el
mask
decoder
funcionando
sin
problemas
en
navegadores
web
en
aproximadamente
50
milisegundos
,
permitiendo
el
prompting
interactivo
en
tiempo
real
.



Usar
SAM
en
Colab



Antes
de
comenzar



Asegurémonos
de
que
tenés
acceso
a
la
GPU
.
Podés
usar
el
comando
nvidia-smi
para
verificarlo
.
En
caso
de
cualquier
problema
,
andá
a
Editar
-
>
Configuración
del
notebook
-
>
Acelerador
de
hardware
,
configuralo
en
GPU
,
y
después
hacé
clic
en
Guardar
.



NOTA
:
Para
que
sea
más
fácil
manejar
datasets
,
imágenes
y
modelos
,
creamos
una
constante
HOME
.



Install
Segment
Anything
Model
(
SAM
)
and
other
dependencies



Este
tutorial
fue
hecho
con
un
modelo
Segment
Anything
lanzado
por
MetaAI
en
Abril
del
2023
.
El
modelo
en
sí
es
muy
versátil
y
permite
utilizar
diferentes
modos
para
obtener
distintos
resultados
de
segmentación
.
Como
su
nombre
lo
sugiere
,
se
puede
usar
para
segmentar
cualquier
cosa
visible
en
la
imagen
,
pero
también
se
puede
seleccionar
un
punto
visible
en
el
cuadro
y
extraer
la
máscara
de
segmentación
completa
asociada
con
ese
punto
.
No
solo
eso
,
sino
que
SAM
puede
usarse
en
conjunto
con
cualquier
detector
de
objetos
para
crear
una
solución
de
segmentación
en
dos
etapas
.
Aquí
,
el
detector
produce
cuadros
delimitadores
y
SAM
convierte
esos
cuadros
delimitadores
en
segmentaciones
.



Descargar
los
Pesos
de
SAM



Similar
a
otros
modelos
,
necesitamos
descargar
nuestros
pesos
desde
un
enlace
externo
antes
de
cargarlos
en
la
memoria
.
Y
cuando
la
descarga
se
completa
,
guardamos
la
ruta
que
conduce
a
esos
pesos
en
una
variable
y
,
para
dormir
tranquilos
,
confirmamos
que
el
archivo
existe
en
nuestro
sistema
operativo
.



Descargar
Datos
de
Ejemplo



Bien
,
y
lo
último
que
haremos
antes
de
cargar
el
modelo
en
la
memoria
es
descargar
algunas
imágenes
para
tener
algunos
ejemplos
con
los
que
podamos
experimentar
.
Y
eso
es
todo
,
todo
está
listo
.
Sentite
libre
de
usar
tus
propias
imágenes
o
videos
.



Cargar
Modelo



SAM
tiene
múltiples
modos
que
podés
usar
para
la
inferencia
,
y
lo
que
vamos
a
hacer
primero
es
aprender
la
API
y
usar
todas
las
diferentes
maneras
de
generar
máscaras
.
El
primer
paso
es
la
generación
automática
de
máscaras
.
Este
es
el
modo
donde
esencialmente
creás
una
máscara
de
segmentación
para
cualquier
objeto
visible
en
la
escena
y
,
para
usarlo
,
necesitamos
importar
una
utilidad
adicional
del
paquete
segment
anything
,
que
es
SamAutomaticMaskGenerator
.



Generación
Automática
de
Máscaras



Para
ejecutar
la
generación
automática
de
máscaras
,
proporcioná
un
modelo
SAM
a
la
clase
SamAutomaticMaskGenerator
.
Establecé
la
ruta
al
checkpoint
de
SAM
a
continuación
.
Se
recomienda
ejecutar
en
CUDA
y
con
el
modelo
predeterminado
.



Generar
máscaras
con
SAM



Ahora
,
para
generar
máscaras
automáticamente
,
necesitamos
leer
una
de
nuestras
imágenes
de
ejemplo
usando
OpenCV
y
convertirla
de
BGR
a
RGB
.
Luego
,
pasamos
esta
imagen
como
argumento
del
método
generate
en
nuestro
generador
de
máscaras
.



Formato
de
Salida



SamAutomaticMaskGenerator
devuelve
una
lista
de
máscaras
,
donde
cada
máscara
es
un
diccionario
que
contiene
varias
informaciones
sobre
la
máscara
:


segmentation
-
[
np.ndarray
]
-
la
máscara
con
forma
(
W
,
H
)
y
tipo
bool


area
-
[
int
]
-
el
área
de
la
máscara
en
píxeles


bbox
-
[
List[int
]
]
-
el
cuadro
delimitador
de
la
máscara
en
formato
xywh


predicted_iou
-
[
float
]
-
la
predicción
del
modelo
sobre
la
calidad
de
la
máscara


point_coords
-
[
List[List[float
]
]
]
-
el
punto
de
entrada
muestreado
que
generó
esta
máscara


stability_score
-
[
float
]
-
una
medida
adicional
de
la
calidad
de
la
máscara


crop_box
-
[
List[int
]
]
-
el
recorte
de
la
imagen
usado
para
generar
esta
máscara
en
formato
xywh



Visualización
de
Resultados
con
Supervision



A
partir
de
la
versión
0.5.0
,
Supervision
tiene
soporte
nativo
para
SAM
.



Entonces
,
a
partir
de
la
versión
0.5.0
,
podrás
procesar
esas
máscaras
eficientemente
usando
Supervision
.
Vamos
a
ver
lo
fácil
que
es
usar
este
paquete
pip
para
anotar
nuestras
segmentaciones
en
la
imagen
.
Simplemente
creo
la
instancia
del
anotador
de
máscaras
,
convierto
nuestro
resultado
de
SAM
en
detecciones
.
Este
es
el
objeto
que
es
reconocible
por
el
resto
de
la
librería
Supervision
.
Ejecutamos
el
método
annotate
usando
nuestra
imagen
original
y
las
detecciones
.
Al
final
,
puedo
imprimir
lado
a
lado
la
imagen
original
y
la
imagen
segmentada
.
Sí
,
sé
que
se
ve
un
poco
extraño
,
pero
eso
es
porque
tenemos
varias
docenas
de
máscaras
y
realmente
no
sabemos
qué
clase
representan
.
Así
que
tratamos
de
usar
tantos
colores
como
sea
posible
para
que
sean
distinguibles
entre
sí
.



Interacción
con
los
resultados
de
segmentación



Vamos
a
explorar
cuántas
máscaras
tenemos
exactamente
en
este
momento
,
así
como
intentar
entender
qué
parte
de
la
imagen
está
representada
por
cada
una
de
ellas
.
Esta
también
es
una
oportunidad
perfecta
para
aprender
la
API
de
SAM
.



Como
mencionamos
antes
,
el
generador
de
máscaras
devuelve
una
lista
de
diccionarios
y
cada
uno
de
esos
diccionarios
contiene
una
clave
de
segmentación
.
Ahora
podemos
usar
comprensión
de
listas
para
extraer
la
clave
de
segmentación
de
cada
resultado
y
mostrarlas
todas
en
una
sola
imagen
.
Al
mismo
tiempo
,
también
ordenaremos
esas
segmentaciones
por
área
,
comenzando
desde
la
más
grande
hasta
la
más
pequeña
.



Lo
interesante
es
que
vemos
una
especie
de
duplicados
en
nuestro
conjunto
de
máscaras
,
y
eso
se
debe
a
que
SAM
espera
ambigüedad
y
te
permite
elegir
la
máscara
correcta
.
En
un
escenario
de
la
vida
real
,
eso
probablemente
significa
que
necesitas
agregar
algún
tipo
de
post-procesamiento
de
máscaras
y
seleccionar
la
estrategia
correcta
para
tu
caso
de
uso
.
Eso
significa
que
podés
elegir
la
más
pequeña
o
la
más
grande
,
o
intentar
fusionar
la
máscara
con
el
mayor
IOU
.
Depende
de
vos
.
Pero
sin
ningún
manejo
,
corres
el
riesgo
de
tener
múltiples
detecciones
que
describen
el
mismo
objeto
.



Generar
Segmentación
con
Bounding
Box



La
clase
SamPredictor
proporciona
una
interfaz
sencilla
para
hacer
prompts
al
modelo
.
Permite
al
usuario
primero
establecer
una
imagen
usando
el
método
set_image
,
que
calcula
los
embeddings
necesarios
de
la
imagen
.
Luego
,
se
pueden
proporcionar
prompts
a
través
del
método
predict
para
predecir
máscaras
de
manera
eficiente
a
partir
de
esos
prompts
.
El
modelo
puede
tomar
como
entrada
tanto
puntos
y
Bounding
Boxes
como
máscaras
de
la
iteración
anterior
de
predicción
.



Ahora
hablemos
sobre
el
uso
de
puntos
o
Bounding
Boxes
para
seleccionar
el
área
de
una
imagen
que
más
nos
interese
y
extraer
las
máscaras
relacionadas
con
esa
área
.
Para
hacerlo
,
necesitamos
importar
SamPredictor
del
paquete
segment-anything
y
,
una
vez
más
,
pasar
el
modelo
SAM
como
argumento
.



Dibujar
el
Bounding
Box



Ahora
definiremos
nuestro
Bounding
Box
.
En
lugar
de
codificarlo
manualmente
como
una
lista
en
Python
,
decidimos
usar
algo
más
interactivo
.
Así
que
ejecutaremos
un
widget
de
Jupyter
Notebook
y
ahora
podemos
usar
el
mouse
para
dibujar
un
Bounding
Box
alrededor
del
área
de
la
imagen
que
más
nos
interese
.
Cuando
accedemos
a
la
propiedad
de
bounding
boxes
del
widget
,
podemos
ver
que
nuestro
Bounding
Box
está
aquí
.



NOTA
:
Ejecutá
la
celda
de
abajo
y
usá
el
mouse
para
dibujar
un
Bounding
Box
en
la
imagen
👇



Generar
máscaras
con
SAM



NOTA
:
El
método
SamPredictor.predict
toma
un
argumento
box
de
tipo
np.ndarray
en
formato
[
xmin
,
ymin
,
xmax
,
ymax
]
.
Vamos
a
reorganizar
tus
datos
primero
.



Lamentablemente
,
esa
información
no
está
almacenada
en
la
estructura
de
datos
correcta
.
Vemos
que
tenemos
un
diccionario
con
las
propiedades
x
,
y
,
width
y
height
,
y
necesitamos
obtener
un
array
de
numpy
donde
tengamos
xmin
,
ymin
,
xmax
,
ymax
.
Así
que
necesitamos
agregar
unas
líneas
de
código
en
Python
para
convertirlo
a
la
estructura
correcta
.



Cuando
eso
esté
hecho
,
simplemente
podemos
pasar
nuestro
Bounding
Box
a
través
del
método
predict
del
mask
predictor
.
Presioná
Shift+Enter
y
obtendremos
nuestra
máscara
.
Sin
embargo
,
debemos
tener
cuidado
porque
el
mask
predictor
tiene
un
formato
de
salida
diferente
al
generador
automático
de
máscaras
que
usamos
anteriormente
.
Antes
obteníamos
una
lista
de
diccionarios
donde
cada
diccionario
describía
una
única
máscara
.
Ahora
obtenemos
una
tupla
de
tres
elementos
:
masks
,
scores
y
logits
,
y
de
esos
tres
,
los
dos
primeros
son
los
más
importantes
para
nosotros
.



Visualización
de
Resultados
con
Supervision



Para
manejar
eso
,
necesitamos
usar
un
post-procesamiento
un
poco
diferente
,
pero
cuando
presionamos
Enter
,
de
manera
similar
a
antes
,
vemos
dos
imágenes
lado
a
lado
:
la
imagen
original
a
la
izquierda
y
la
imagen
segmentada
a
la
derecha
.



Interacción
con
los
resultados
de
segmentación



Pero
,
curiosamente
,
este
no
es
el
resultado
de
simplemente
trazar
una
sola
máscara
en
la
imagen
.
El
modelo
,
una
vez
más
,
no
estaba
seguro
de
cuál
máscara
nos
interesaba
más
,
por
lo
que
devolvió
tres
de
ellas
y
nos
permitió
seleccionar
la
correcta
.
