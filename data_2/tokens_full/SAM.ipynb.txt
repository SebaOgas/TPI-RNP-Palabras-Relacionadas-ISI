Segment
Anything
Model
(
SAM
)



El
Segment
Anything
Model
(
SAM
)
de
Meta
es
un
modelo
fundacional
de
segmentaci칩n
de
im치genes
que
destaca
por
su
capacidad
de
identificar
y
segmentar
cualquier
objeto
en
una
imagen
sin
necesidad
de
entrenamiento
adicional
o
categor칤as
predefinidas
.
Utiliza
una
red
neuronal
profunda
entrenada
en
una
vasta
variedad
de
im치genes
,
lo
que
le
permite
ofrecer
segmentaciones
precisas
y
vers치tiles
de
manera
inmediata
.
Esta
caracter칤stica
lo
hace
una
herramienta
poderosa
y
flexible
para
aplicaciones
en
edici칩n
de
im치genes
,
rob칩tica
,
realidad
aumentada
y
an치lisis
de
im치genes
m칠dicas
,
entre
otros
campos
tecnol칩gicos
y
cient칤ficos
.



Anteriormente
,
para
resolver
cualquier
tipo
de
problema
de
segmentaci칩n
,
hab칤a
dos
clases
de
enfoques
.
El
primero
,
la
segmentaci칩n
interactiva
,
permit칤a
segmentar
cualquier
clase
de
objeto
,
pero
requer칤a
que
una
persona
guiara
el
m칠todo
refinando
iterativamente
una
m치scara
.
El
segundo
,
la
segmentaci칩n
autom치tica
,
permit칤a
la
segmentaci칩n
de
categor칤as
espec칤ficas
de
objetos
definidas
de
antemano
(
por
ejemplo
,
gatos
o
sillas
)
,
pero
requer칤a
cantidades
sustanciales
de
objetos
anotados
manualmente
para
entrenar
(
por
ejemplo
,
miles
o
incluso
decenas
de
miles
de
ejemplos
de
gatos
segmentados
)
,
junto
con
los
recursos
computacionales
y
la
experiencia
t칠cnica
para
entrenar
el
modelo
de
segmentaci칩n
.
Ning칰n
enfoque
proporcionaba
una
soluci칩n
general
y
totalmente
autom치tica
para
la
segmentaci칩n
.



SAM
es
una
generalizaci칩n
de
estas
dos
clases
de
enfoques
.
Es
un
칰nico
modelo
que
puede
realizar
f치cilmente
tanto
segmentaci칩n
interactiva
como
segmentaci칩n
autom치tica
.
La
interfaz
de
SAM
que
se
puede
orientar
mediante
prompts
(
que
se
describir치n
en
breve
)
permite
usarlo
de
maneras
flexibles
que
hacen
posibles
una
amplia
gama
de
tareas
de
segmentaci칩n
simplemente
dise침ando
el
prompt
adecuado
para
el
modelo
(
clics
,
cajas
,
texto
,
etc칠tera
)
.
Adem치s
,
SAM
est치
entrenado
en
un
conjunto
de
datos
diverso
y
de
alta
calidad
que
incluye
m치s
de
1000
millones
de
m치scaras
(
recopiladas
como
parte
de
este
proyecto
)
,
lo
que
le
permite
generalizar
a
nuevos
tipos
de
objetos
e
im치genes
m치s
all치
de
lo
que
observ칩
durante
el
entrenamiento
.
Esta
capacidad
de
generalizar
significa
que
,
en
gran
medida
,
los
practicantes
ya
no
necesitar치n
recopilar
sus
propios
datos
de
segmentaci칩n
y
ajustar
un
modelo
para
su
caso
de
uso
.



En
conjunto
,
estas
capacidades
permiten
que
SAM
generalice
tanto
a
nuevas
tareas
como
a
nuevos
dominios
.
Esta
flexibilidad
es
la
primera
de
su
tipo
para
la
segmentaci칩n
de
im치genes
.



Ac치
ten칠s
un
video
corto
que
muestra
algunas
de
las
capacidades
de
SAM
:



(
1
)
SAM
permite
a
los
usuarios
segmentar
objetos
con
solo
un
clic
o
interactuando
mediante
clics
en
puntos
para
incluir
y
excluir
del
objeto
.
El
modelo
tambi칠n
puede
recibir
un
cuadro
delimitador
como
prompt
.



(
2
)
SAM
puede
generar
m칰ltiples
m치scaras
v치lidas
cuando
enfrenta
ambig칲edades
sobre
el
objeto
que
se
est치
segmentando
,
una
capacidad
importante
y
necesaria
para
resolver
la
segmentaci칩n
en
el
mundo
real
.



(
3
)
SAM
puede
encontrar
y
enmascarar
autom치ticamente
todos
los
objetos
en
una
imagen
.



(
4
)
SAM
puede
generar
una
m치scara
de
segmentaci칩n
para
cualquier
prompt
en
tiempo
real
despu칠s
de
precomputar
el
embbedding
de
la
imagen
,
permitiendo
la
interacci칩n
en
tiempo
real
con
el
modelo
.



C칩mo
funciona
SAM
:
Segmentaci칩n
orientada
por
prompts



En
el
procesamiento
del
lenguaje
natural
y
,
m치s
recientemente
,
en
la
visi칩n
por
computadora
,
uno
de
los
desarrollos
m치s
emocionantes
es
el
de
los
modelos
fundacionales
que
pueden
realizar
zero-shot
y
few-shot
learning
para
nuevos
conjuntos
de
datos
y
tareas
utilizando
t칠cnicas
de
"
prompting
"
.
Los
creadores
de
SAM
se
inspiraron
en
esta
l칤nea
de
trabajo
.



Ellos
entrenaron
a
SAM
para
devolver
una
m치scara
de
segmentaci칩n
v치lida
para
cualquier
prompt
,
donde
un
prompt
puede
ser
:


puntos
de
primer
plano
/
fondo
,


una
bounding
box
aproximada
,


una
m치scara
aproximada
o


texto
libre
.



El
requisito
de
una
m치scara
v치lida
simplemente
significa
que
incluso
cuando
un
prompt
es
ambiguo
y
podr칤a
referirse
a
m칰ltiples
objetos
(
por
ejemplo
,
un
punto
en
una
camisa
puede
indicar
la
camisa
o
la
persona
que
la
lleva
puesta
)
,
el
resultado
debe
ser
una
m치scara
razonable
para
uno
de
esos
objetos
.
Esta
tarea
se
utiliza
para
preentrenar
el
modelo
y
para
resolver
tareas
generales
de
segmentaci칩n
mediante
prompting
.



Los
creadores
de
SAM
observaron
que
la
tarea
de
preentrenamiento
y
la
recolecci칩n
de
datos
interactiva
impusieron
restricciones
espec칤ficas
en
el
dise침o
del
modelo
.
En
particular
,
el
modelo
necesita
funcionar
en
tiempo
real
en
un
CPU
en
un
navegador
web
para
permitir
que
los
anotadores
usen
SAM
de
manera
interactiva
en
tiempo
real
y
anoten
eficientemente
.
Aunque
la
restricci칩n
de
tiempo
de
ejecuci칩n
implica
una
compensaci칩n
entre
la
calidad
y
el
tiempo
de
ejecuci칩n
,
encontraron
que
un
dise침o
simple
produce
buenos
resultados
en
la
pr치ctica
.



Bajo
el
cap칩
,
un
encoder
de
im치genes
produce
un
embedding
칰nico
para
la
imagen
,
mientras
que
otro
encoder
liviano
convierte
cualquier
prompt
en
un
embedding
en
tiempo
real
.
Estas
dos
fuentes
de
informaci칩n
se
combinan
luego
en
un
decoder
liviano
que
predice
las
m치scaras
de
segmentaci칩n
.
Despu칠s
de
que
se
calcula
el
embedding
de
la
imagen
,
SAM
puede
producir
una
segmentaci칩n
en
solo
50
milisegundos
para
cualquier
prompt
en
un
navegador
web
.



Arquitectura
de
la
Red
de
SAM



La
arquitectura
de
la
red
del
Segment
Anything
Model
(
SAM
)
contiene
tres
componentes
cruciales
:
el
Image
Encoder
,
el
Prompt
Encoder
y
el
Mask
Decoder
.


Image
Encoder
:
En
el
nivel
m치s
alto
,
un
image
encoder
(
un
autoencoder
enmascarado
,
MAE
,
preentrenado
con
Vision
Transformer
,
ViT
)
genera
embeddings
de
im치genes
de
una
sola
vez
.
Se
aplica
antes
de
hacer
el
prompting
del
modelo
.


Prompt
Encoder
:
El
prompt
encoder
codifica
puntos
de
fondo
,
m치scaras
,
bounding
boxes
o
textos
en
un
vector
de
embedding
en
tiempo
real
.
La
investigaci칩n
considera
dos
conjuntos
de
prompts
:
escasos
(
puntos
,
cuadros
,
texto
)
y
densos
(
m치scaras
)
.
Los
puntos
y
boxes
se
representan
mediante
codificaciones
posicionales
y
se
suman
con
embeddings
aprendidos
para
cada
tipo
de
prompt
.
Los
prompts
de
texto
libre
se
representan
con
un
text
encoder
preexistente
de
CLIP
.
Los
prompts
densos
,
como
las
m치scaras
,
se
embeben
con
convoluciones
y
se
suman
elemento
por
elemento
con
el
embedding
de
la
imagen
.


Mask
Decoder
:
Un
mask
decoder
ligero
predice
las
m치scaras
de
segmentaci칩n
bas치ndose
en
los
embeddings
tanto
del
image
encoder
como
del
prompt
encoder
.
SAM
utiliza
un
bloque
decoder
modificado
y
luego
una
cabeza
de
predicci칩n
de
m치scara
din치mica
,
inspir치ndose
en
bloques
de
decodificadores
de
Transformer
ya
existentes
.
Este
dise침o
incorpora
mecanismos
de
self-attention
y
cross-attention
en
dos
direcciones
(
del
prompt
al
embedding
de
imagen
y
viceversa
)
para
actualizar
todos
los
embeddings
de
manera
efectiva
.





El
dise침o
general
de
SAM
prioriza
la
eficiencia
,
con
el
prompt
encoder
y
el
mask
decoder
funcionando
sin
problemas
en
navegadores
web
en
aproximadamente
50
milisegundos
,
permitiendo
el
prompting
interactivo
en
tiempo
real
.



Usar
SAM
en
Colab



Antes
de
comenzar



Asegur칠monos
de
que
ten칠s
acceso
a
la
GPU
.
Pod칠s
usar
el
comando
nvidia-smi
para
verificarlo
.
En
caso
de
cualquier
problema
,
and치
a
Editar
-
>
Configuraci칩n
del
notebook
-
>
Acelerador
de
hardware
,
configuralo
en
GPU
,
y
despu칠s
hac칠
clic
en
Guardar
.



NOTA
:
Para
que
sea
m치s
f치cil
manejar
datasets
,
im치genes
y
modelos
,
creamos
una
constante
HOME
.



Install
Segment
Anything
Model
(
SAM
)
and
other
dependencies



Este
tutorial
fue
hecho
con
un
modelo
Segment
Anything
lanzado
por
MetaAI
en
Abril
del
2023
.
El
modelo
en
s칤
es
muy
vers치til
y
permite
utilizar
diferentes
modos
para
obtener
distintos
resultados
de
segmentaci칩n
.
Como
su
nombre
lo
sugiere
,
se
puede
usar
para
segmentar
cualquier
cosa
visible
en
la
imagen
,
pero
tambi칠n
se
puede
seleccionar
un
punto
visible
en
el
cuadro
y
extraer
la
m치scara
de
segmentaci칩n
completa
asociada
con
ese
punto
.
No
solo
eso
,
sino
que
SAM
puede
usarse
en
conjunto
con
cualquier
detector
de
objetos
para
crear
una
soluci칩n
de
segmentaci칩n
en
dos
etapas
.
Aqu칤
,
el
detector
produce
cuadros
delimitadores
y
SAM
convierte
esos
cuadros
delimitadores
en
segmentaciones
.



Descargar
los
Pesos
de
SAM



Similar
a
otros
modelos
,
necesitamos
descargar
nuestros
pesos
desde
un
enlace
externo
antes
de
cargarlos
en
la
memoria
.
Y
cuando
la
descarga
se
completa
,
guardamos
la
ruta
que
conduce
a
esos
pesos
en
una
variable
y
,
para
dormir
tranquilos
,
confirmamos
que
el
archivo
existe
en
nuestro
sistema
operativo
.



Descargar
Datos
de
Ejemplo



Bien
,
y
lo
칰ltimo
que
haremos
antes
de
cargar
el
modelo
en
la
memoria
es
descargar
algunas
im치genes
para
tener
algunos
ejemplos
con
los
que
podamos
experimentar
.
Y
eso
es
todo
,
todo
est치
listo
.
Sentite
libre
de
usar
tus
propias
im치genes
o
videos
.



Cargar
Modelo



SAM
tiene
m칰ltiples
modos
que
pod칠s
usar
para
la
inferencia
,
y
lo
que
vamos
a
hacer
primero
es
aprender
la
API
y
usar
todas
las
diferentes
maneras
de
generar
m치scaras
.
El
primer
paso
es
la
generaci칩n
autom치tica
de
m치scaras
.
Este
es
el
modo
donde
esencialmente
cre치s
una
m치scara
de
segmentaci칩n
para
cualquier
objeto
visible
en
la
escena
y
,
para
usarlo
,
necesitamos
importar
una
utilidad
adicional
del
paquete
segment
anything
,
que
es
SamAutomaticMaskGenerator
.



Generaci칩n
Autom치tica
de
M치scaras



Para
ejecutar
la
generaci칩n
autom치tica
de
m치scaras
,
proporcion치
un
modelo
SAM
a
la
clase
SamAutomaticMaskGenerator
.
Establec칠
la
ruta
al
checkpoint
de
SAM
a
continuaci칩n
.
Se
recomienda
ejecutar
en
CUDA
y
con
el
modelo
predeterminado
.



Generar
m치scaras
con
SAM



Ahora
,
para
generar
m치scaras
autom치ticamente
,
necesitamos
leer
una
de
nuestras
im치genes
de
ejemplo
usando
OpenCV
y
convertirla
de
BGR
a
RGB
.
Luego
,
pasamos
esta
imagen
como
argumento
del
m칠todo
generate
en
nuestro
generador
de
m치scaras
.



Formato
de
Salida



SamAutomaticMaskGenerator
devuelve
una
lista
de
m치scaras
,
donde
cada
m치scara
es
un
diccionario
que
contiene
varias
informaciones
sobre
la
m치scara
:


segmentation
-
[
np.ndarray
]
-
la
m치scara
con
forma
(
W
,
H
)
y
tipo
bool


area
-
[
int
]
-
el
치rea
de
la
m치scara
en
p칤xeles


bbox
-
[
List[int
]
]
-
el
cuadro
delimitador
de
la
m치scara
en
formato
xywh


predicted_iou
-
[
float
]
-
la
predicci칩n
del
modelo
sobre
la
calidad
de
la
m치scara


point_coords
-
[
List[List[float
]
]
]
-
el
punto
de
entrada
muestreado
que
gener칩
esta
m치scara


stability_score
-
[
float
]
-
una
medida
adicional
de
la
calidad
de
la
m치scara


crop_box
-
[
List[int
]
]
-
el
recorte
de
la
imagen
usado
para
generar
esta
m치scara
en
formato
xywh



Visualizaci칩n
de
Resultados
con
Supervision



A
partir
de
la
versi칩n
0.5.0
,
Supervision
tiene
soporte
nativo
para
SAM
.



Entonces
,
a
partir
de
la
versi칩n
0.5.0
,
podr치s
procesar
esas
m치scaras
eficientemente
usando
Supervision
.
Vamos
a
ver
lo
f치cil
que
es
usar
este
paquete
pip
para
anotar
nuestras
segmentaciones
en
la
imagen
.
Simplemente
creo
la
instancia
del
anotador
de
m치scaras
,
convierto
nuestro
resultado
de
SAM
en
detecciones
.
Este
es
el
objeto
que
es
reconocible
por
el
resto
de
la
librer칤a
Supervision
.
Ejecutamos
el
m칠todo
annotate
usando
nuestra
imagen
original
y
las
detecciones
.
Al
final
,
puedo
imprimir
lado
a
lado
la
imagen
original
y
la
imagen
segmentada
.
S칤
,
s칠
que
se
ve
un
poco
extra침o
,
pero
eso
es
porque
tenemos
varias
docenas
de
m치scaras
y
realmente
no
sabemos
qu칠
clase
representan
.
As칤
que
tratamos
de
usar
tantos
colores
como
sea
posible
para
que
sean
distinguibles
entre
s칤
.



Interacci칩n
con
los
resultados
de
segmentaci칩n



Vamos
a
explorar
cu치ntas
m치scaras
tenemos
exactamente
en
este
momento
,
as칤
como
intentar
entender
qu칠
parte
de
la
imagen
est치
representada
por
cada
una
de
ellas
.
Esta
tambi칠n
es
una
oportunidad
perfecta
para
aprender
la
API
de
SAM
.



Como
mencionamos
antes
,
el
generador
de
m치scaras
devuelve
una
lista
de
diccionarios
y
cada
uno
de
esos
diccionarios
contiene
una
clave
de
segmentaci칩n
.
Ahora
podemos
usar
comprensi칩n
de
listas
para
extraer
la
clave
de
segmentaci칩n
de
cada
resultado
y
mostrarlas
todas
en
una
sola
imagen
.
Al
mismo
tiempo
,
tambi칠n
ordenaremos
esas
segmentaciones
por
치rea
,
comenzando
desde
la
m치s
grande
hasta
la
m치s
peque침a
.



Lo
interesante
es
que
vemos
una
especie
de
duplicados
en
nuestro
conjunto
de
m치scaras
,
y
eso
se
debe
a
que
SAM
espera
ambig칲edad
y
te
permite
elegir
la
m치scara
correcta
.
En
un
escenario
de
la
vida
real
,
eso
probablemente
significa
que
necesitas
agregar
alg칰n
tipo
de
post-procesamiento
de
m치scaras
y
seleccionar
la
estrategia
correcta
para
tu
caso
de
uso
.
Eso
significa
que
pod칠s
elegir
la
m치s
peque침a
o
la
m치s
grande
,
o
intentar
fusionar
la
m치scara
con
el
mayor
IOU
.
Depende
de
vos
.
Pero
sin
ning칰n
manejo
,
corres
el
riesgo
de
tener
m칰ltiples
detecciones
que
describen
el
mismo
objeto
.



Generar
Segmentaci칩n
con
Bounding
Box



La
clase
SamPredictor
proporciona
una
interfaz
sencilla
para
hacer
prompts
al
modelo
.
Permite
al
usuario
primero
establecer
una
imagen
usando
el
m칠todo
set_image
,
que
calcula
los
embeddings
necesarios
de
la
imagen
.
Luego
,
se
pueden
proporcionar
prompts
a
trav칠s
del
m칠todo
predict
para
predecir
m치scaras
de
manera
eficiente
a
partir
de
esos
prompts
.
El
modelo
puede
tomar
como
entrada
tanto
puntos
y
Bounding
Boxes
como
m치scaras
de
la
iteraci칩n
anterior
de
predicci칩n
.



Ahora
hablemos
sobre
el
uso
de
puntos
o
Bounding
Boxes
para
seleccionar
el
치rea
de
una
imagen
que
m치s
nos
interese
y
extraer
las
m치scaras
relacionadas
con
esa
치rea
.
Para
hacerlo
,
necesitamos
importar
SamPredictor
del
paquete
segment-anything
y
,
una
vez
m치s
,
pasar
el
modelo
SAM
como
argumento
.



Dibujar
el
Bounding
Box



Ahora
definiremos
nuestro
Bounding
Box
.
En
lugar
de
codificarlo
manualmente
como
una
lista
en
Python
,
decidimos
usar
algo
m치s
interactivo
.
As칤
que
ejecutaremos
un
widget
de
Jupyter
Notebook
y
ahora
podemos
usar
el
mouse
para
dibujar
un
Bounding
Box
alrededor
del
치rea
de
la
imagen
que
m치s
nos
interese
.
Cuando
accedemos
a
la
propiedad
de
bounding
boxes
del
widget
,
podemos
ver
que
nuestro
Bounding
Box
est치
aqu칤
.



NOTA
:
Ejecut치
la
celda
de
abajo
y
us치
el
mouse
para
dibujar
un
Bounding
Box
en
la
imagen
游녢



Generar
m치scaras
con
SAM



NOTA
:
El
m칠todo
SamPredictor.predict
toma
un
argumento
box
de
tipo
np.ndarray
en
formato
[
xmin
,
ymin
,
xmax
,
ymax
]
.
Vamos
a
reorganizar
tus
datos
primero
.



Lamentablemente
,
esa
informaci칩n
no
est치
almacenada
en
la
estructura
de
datos
correcta
.
Vemos
que
tenemos
un
diccionario
con
las
propiedades
x
,
y
,
width
y
height
,
y
necesitamos
obtener
un
array
de
numpy
donde
tengamos
xmin
,
ymin
,
xmax
,
ymax
.
As칤
que
necesitamos
agregar
unas
l칤neas
de
c칩digo
en
Python
para
convertirlo
a
la
estructura
correcta
.



Cuando
eso
est칠
hecho
,
simplemente
podemos
pasar
nuestro
Bounding
Box
a
trav칠s
del
m칠todo
predict
del
mask
predictor
.
Presion치
Shift+Enter
y
obtendremos
nuestra
m치scara
.
Sin
embargo
,
debemos
tener
cuidado
porque
el
mask
predictor
tiene
un
formato
de
salida
diferente
al
generador
autom치tico
de
m치scaras
que
usamos
anteriormente
.
Antes
obten칤amos
una
lista
de
diccionarios
donde
cada
diccionario
describ칤a
una
칰nica
m치scara
.
Ahora
obtenemos
una
tupla
de
tres
elementos
:
masks
,
scores
y
logits
,
y
de
esos
tres
,
los
dos
primeros
son
los
m치s
importantes
para
nosotros
.



Visualizaci칩n
de
Resultados
con
Supervision



Para
manejar
eso
,
necesitamos
usar
un
post-procesamiento
un
poco
diferente
,
pero
cuando
presionamos
Enter
,
de
manera
similar
a
antes
,
vemos
dos
im치genes
lado
a
lado
:
la
imagen
original
a
la
izquierda
y
la
imagen
segmentada
a
la
derecha
.



Interacci칩n
con
los
resultados
de
segmentaci칩n



Pero
,
curiosamente
,
este
no
es
el
resultado
de
simplemente
trazar
una
sola
m치scara
en
la
imagen
.
El
modelo
,
una
vez
m치s
,
no
estaba
seguro
de
cu치l
m치scara
nos
interesaba
m치s
,
por
lo
que
devolvi칩
tres
de
ellas
y
nos
permiti칩
seleccionar
la
correcta
.
