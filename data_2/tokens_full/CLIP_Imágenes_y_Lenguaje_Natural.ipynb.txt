Modelo
CLIP
y
la
importancia
de
los
embeddings
multimodales



CLIP
,
que
significa
Contrastive
Language-Image
Pretraining
,
es
un
modelo
de
aprendizaje
profundo
desarrollado
por
OpenAI
en
2021
.
Los
embeddings
de
CLIP
para
imágenes
y
texto
comparten
el
mismo
espacio
,
lo
que
permite
comparaciones
directas
entre
las
dos
modalidades
.
Esto
se
logra
entrenando
el
modelo
para
acercar
imágenes
y
textos
relacionados
mientras
separa
los
no
relacionados
.
Este
artículo
explicará
cómo
funciona
CLIP
y
lo
guiará
a
través
de
un
ejemplo
para
entrenar
el
modelo
CLIP
utilizando
el
dataset
flicker30k
.



Problemas
de
los
enfoques
anteriores
de
Computer
Vision



Para
empezar
a
entender
bien
la
importancia
de
un
modelo
como
CLIP
,
primero
tenemos
que
centrarnos
en
entender
cuáles
son
los
puntos
débiles
que
hacen
tambalear
a
muchos
de
los
modelos
de
computer
vision
preexistentes
.



Aunque
estos
modelos
son
poderosos
y
versátiles
,
tienen
limitaciones
notables
.
Por
ejemplo
,
una
red
neuronal
convolucional
entrenada
para
clasificar
perros
y
gatos
puede
enfrentar
dificultades
al
clasificar
imágenes
de
caballos
.
Esto
se
debe
a
que
,
una
vez
entrenada
,
la
red
está
fijada
en
su
tarea
inicial
y
debemos
modificarla
para
abordar
nuevas
tareas
.



Esta
falta
de
flexibilidad
ha
llevado
al
desarrollo
de
una
tendencia
importante
:
trabajar
con
modelos
preentrenados
en
grandes
conjuntos
de
datos
de
imágenes
variadas
haciendo
finetuning
.
Al
utilizar
un
modelo
preentrenado
,
se
aprovechan
los
patrones
aprendidos
previamente
para
tareas
diversas
.
Por
ejemplo
,
si
se
necesita
un
clasificador
de
perros
,
gatos
y
caballos
,
simplemente
se
modifica
la
arquitectura
del
modelo
agregando
una
neurona
extra
a
la
capa
final
para
la
clasificación
.



Sin
embargo
,
este
enfoque
también
presenta
desafíos
.
Primero
,
se
requiere
un
conocimiento
de
la
arquitectura
original
del
modelo
preentrenado
.
Segundo
,
se
necesitan
conjuntos
de
datos
masivos
etiquetados
manualmente
.
Etiquetar
imágenes
de
manera
precisa
y
exhaustiva
puede
ser
complicado
,
especialmente
cuando
una
imagen
puede
tener
múltiples
interpretaciones
.
Esto
significa
que
las
etiquetas
deben
ser
más
detalladas
,
lo
que
aumenta
la
complejidad
del
proceso
de
etiquetado
.



CLIP
es
una
innovadora
propuesta
desarrollada
por
OpenAI
que
aborda
muchos
de
los
desafíos
previamente
discutidos
.
¿
Cómo
lo
logra
?
Combinando
la
potencia
de
los
modelos
de
visión
con
los
modelos
de
lenguaje
.
En
lugar
de
utilizar
el
enfoque
tradicional
de
supervisar
las
imágenes
con
etiquetas
,
CLIP
utiliza
la
supervisión
a
través
del
lenguaje
natural
.
Consta
de
dos
modelos
:
uno
de
visión
,
encargado
de
analizar
las
imágenes
,
y
otro
de
lenguaje
,
que
procesa
el
texto
de
entrada
.
La
tarea
principal
de
CLIP
es
emparejar
imágenes
con
descripciones
,
aprendiendo
a
asociar
correctamente
cada
imagen
con
la
descripción
más
adecuada
.



Esta
idea
aparentemente
simple
tiene
implicaciones
significativas
.
En
primer
lugar
,
CLIP
no
requiere
imágenes
etiquetadas
manualmente
,
sino
que
puede
entrenarse
con
descripciones
en
lenguaje
natural
.
OpenAI
ha
entrenado
este
sistema
con
más
de
250
millones
de
pares
de
imágenes
y
descripciones
obtenidas
de
Internet
,
lo
que
amplía
enormemente
el
alcance
y
la
diversidad
del
conjunto
de
datos
.
Además
,
la
forma
en
que
se
entrena
CLIP
es
crucial
:
no
solo
debe
aprender
a
asociar
correctamente
las
imágenes
con
las
descripciones
adecuadas
,
sino
también
a
discernir
por
qué
ciertas
descripciones
no
son
apropiadas
para
una
imagen
dada
.



¿
Qué
es
CLIP
?



CLIP
está
diseñado
para
predecir
qué
N
×
N
pares
potenciales
(
imagen
,
texto
)
dentro
del
lote
están
realmente
asociados
.
Para
lograr
esto
,
CLIP
establece
un
espacio
de
embeddings
multimodal
mediante
el
entrenamiento
conjunto
de
un
codificador
de
imágenes
y
un
codificador
de
texto
.



La
función
de
pérdida
de
CLIP
tiene
como
objetivo
maximizar
la
similitud
del
coseno
entre
los
embeddings
de
la
imagen
y
del
texto
para
los
N
pares
genuinos
en
el
lote
y
al
mismo
tiempo
minimizar
la
similitud
del
coseno
para
los
N²
-
N
pares
incorrectos
.
El
proceso
de
optimización
implica
el
uso
de
una
función
de
pérdida
de
entropía
cruzada
simétrica
que
opera
con
estas
puntuaciones
de
similitud
.



A
continuación
se
presenta
un
pseudocódigo
(
tomado
del
artículo
original
)
que
describe
la
implementación
principal
de
CLIP
.



image_encoder
-
ResNet
or
Vision
Transformer


text_encoder
-
Text
Transformer


I[n
,
h
,
w
,
c
]
-
minilote
de
imágenes
alineadas


T[n
,
l
]
-
minilote
de
descripciones
alineadas


Wi[di
,
d_e
]
-
proyección
aprendida
del
embedding
de
la
imagen
al
espacio
multimodal


Wt[dt
,
d_e
]
-
proyección
aprendida
del
embedding
de
la
descripción
al
espacio
multimodal


t
-
parámetro
de
temperatura
aprendido


extraer
representaciones
de
características
de
cada
modalidad


If
=
imageencoder(I
)
#
[
n
,
d_i
]


Tf
=
textencoder(T
)
#
[
n
,
d_t
]


obtener
representaciones
multimodales
conjuntas
[
n
,
d_e
]


Ie
=
l2normalize(np.dot(If
,
Wi
)
,
axis=1
)


Te
=
l2normalize(np.dot(Tf
,
Wt
)
,
axis=1
)


cálculo
de
las
similitudes
de
cosenos
por
pares
escaladas
[
n
,
n
]


logits
=
np.dot(Ie
,
Te
.
T
)
*
np.exp(t
)


cálculo
de
la
función
de
pérdida
simétrica


labels
=
np.arange(n
)


lossi
=
crossentropy_loss(logits
,
labels
,
axis=0
)


losst
=
crossentropy_loss(logits
,
labels
,
axis=1
)


loss
=
(
lossi
+
losst)/2



En
las
siguientes
secciones
haremos
una
descripción
paso
por
paso
de
cada
línea
del
pseudocódigo
y
su
implementación
usando
Pytorch
.



Descripción
General
del
Modelo



ClIP
utiliza
dos
arquitecturas
separadas
como
columna
vertebral
para
codificar
conjuntos
de
datos
de
texto
y
visión
:


image_encoder
:
representa
la
arquitectura
de
red
neuronal
(
por
ejemplo
,
ResNet
o
Vision
Transformer
)
responsable
de
codificar
imágenes
.


text_encoder
:
representa
la
arquitectura
de
red
neuronal
(
por
ejemplo
,
CBOW
,
BERT
o
Text
Transformer
)
responsable
de
codificar
información
textual
.



El
modelo
CLIP
original
entrenó
desde
cero
si
tanto
el
encoder
de
imágenes
como
el
encoder
de
texto
con
debido
al
gran
volumen
del
conjunto
de
datos
(
400
millones
de
pares
de
imagen-texto
)
que
utilizaron
.
En
el
ejemplo
de
este
tutorial
,
haremos
las
cosas
de
manera
un
poco
diferente
.
Comenzaremos
con
pesos
previamente
entrenados
de
los
modelos
resnet
(
para
imágenes
)
y
distilbert
(
para
texto
)
para
inicializar
estos
módulos
.





Datos
de
Entrada



El
modelo
toma
como
entrada
un
lote
de
n
pares
de
imágenes
y
textos
donde
:


I[n
,
c
,
h
,
w
]
:
representa
un
minilote
de
imágenes
alineadas
,
donde
n
es
el
tamaño
del
lote
,
h
es
la
altura
de
la
imagen
,
w
es
el
ancho
de
la
imagen
y
c
es
el
número
de
canales
.


T[n
,
l
]
:
representa
un
minilote
de
textos
alineados
,
donde
n
es
el
tamaño
del
lote
y
l
es
la
longitud
de
la
secuencia
textual
.



Extracción
de
Features



If
=
imageencoder(I
):
extrae
embeddings
(
If
)
del
encoder
de
imágenes
.
La
forma
de
If
es
[
n
,
di
]
,
donde
di
es
la
dimensionalidad
de
los
embeddings
de
la
imagen
.


Tf
=
textencoder(T
):
extrae
embeddings
(
Tf
)
del
encoder
de
texto
.
La
forma
de
Tf
es
[
n
,
dt
]
,
donde
dt
es
la
dimensionalidad
de
los
embeddings
de
texto
.



Proyecciones
Aprendidas



Wi[di
,
de
]
:
representa
la
matriz
de
proyección
aprendida
para
mapear
los
embeddings
de
las
imágenes
(
If
)
a
un
espacio
de
embedding
multimodal
(
Ie
)
.
La
forma
de
Wi
es
[
di
,
de
]
,
donde
d_e
es
la
dimensionalidad
deseada
del
espacio
de
embedding
multimodal
.


Wt[dt
,
de
]
:
representa
la
matriz
de
proyección
aprendida
para
mapear
los
embeddings
de
texto
(
Tf
)
a
un
espacio
de
embedding
multimodal(Te
)
.
La
forma
de
Wt
es
[
dt
,
de
]
.



La
operación
de
proyección
se
puede
implementar
 
utilizando
una
red
neuronal
con
dos
capas
densas
,
cuyos
pesos
son
la
matriz
de
proyección
aprendida
.
En
la
mayoría
de
los
casos
,
los
pesos
de
la
matriz
de
proyección
son
los
únicos
pesos
con
gradientes
activos
durante
el
proceso
de
entrenamiento
.
Además
,
la
capa
de
proyección
juega
un
papel
crucial
a
la
hora
de
alinear
las
dimensiones
de
los
embeddings
de
imágenes
y
texto
,
asegurando
que
tengan
el
mismo
tamaño
.



Arquitectura
de
los
Encoders



El
siguiente
código
ilustra
el
procesamiento
secuencial
de
datos
de
imagen
y
texto
.
Inicialmente
,
los
datos
se
procesan
a
través
del
encoder
base
,
seguido
de
la
capa
de
proyección
y
finalmente
,
se
generan
embeddings
normalizadas
para
ambas
modalidades
.



Función
de
Pérdida



logits
=
np.dot(I_e
,
T_e
.
T
)
 
np.exp(t
):
Calcula
similitudes
de
coseno
por
pares
entre
embeddings
de
imagen
y
texto
,
escaladas
por
un
parámetro
de
temperatura
aprendido
t.



En
este
ejemplo
,
usamos
indistintamente
los
términos
similitud
y
logits
de
la
misma
manera
que
se
usó
en
el
artículo
original
.
No
incluiremos
el
parámetro
de
temperatura
t
en
esta
clase
.



CLIP
utiliza
la
pérdida
contrastiva
para
acercar
los
embeddings
de
las
imágenes
y
los
textos
relacionados
mientras
aleja
los
no
relacionados
.


labels
=
np.arange(n
):
Genera
etiquetas
que
representan
los
índices
del
lote
.


lossi
=
crossentropy_loss(logits
,
etiquetas
,
axis=0
):
Calcula
la
pérdida
de
entropía
cruzada
a
lo
largo
del
eje
de
la
imagen
.


losst
=
crossentropy_loss(logits
,
etiquetas
,
axis=1
):
Calcula
la
pérdida
de
entropía
cruzada
a
lo
largo
del
eje
del
texto
.


loss
=
(
lossi
+
losst)/2
:
Calcula
el
promedio
simétrico
de
las
pérdidas
de
imagen
y
texto
.



Modelo
CLIP
personalizado
final



Combinando
todas
las
diferentes
piezas
,
el
modelo
CLIP
personalizado
final
se
parece
al
siguiente
:



Ejemplo
de
Entrenamiento



Este
ejemplo
demuestra
el
proceso
de
creación
de
dataset
de
imágenes
con
descripciones
y
el
entrenamiento
de
un
modelo
CLIP
personalizado
.



El
objetivo
es
entrenar
un
encoder
de
visión
y
un
encoder
de
texto
conjuntamente
para
proyectar
la
representación
de
imágenes
y
sus
descripciones
en
el
mismo
espacio
latente
,
de
modo
que
los
embeddings
de
las
descripciones
estén
ubicadas
cerca
de
los
embeddings
de
las
imágenes
que
describen
.



Dataset
y
Dataloaders



Nuestro
modelo
CLIP
personalizado
se
entrenará
utilizando
el
conjunto
de
datos
flickr30k
.
Este
dataset
comprende
más
de
31.000
imágenes
,
cada
una
con
un
mínimo
de
cinco
descripciones
independientes
generadas
por
humanos
.
Usaremos
dos
descripciones
para
cada
imagen
en
este
ejemplo
para
tener
un
total
de
62
000
pares
de
imágenes
y
texto
para
el
entrenamiento
.



Las
constantes
clave
del
modelo
incluyen
:


embed_dim
para
las
representaciones
aprendidas
,


transformerembeddim
para
los
embeddings
del
transformer
y


max_len
para
la
longitud
máxima
de
entrada
de
texto
.



El
modelo
de
texto
elegido
es
distilbert-base-multilingual-cased
.
El
entrenamiento
abarca
3
épocas
con
un
tamaño
de
lote
de
128
,
que
son
las
constantes
que
se
incorporarán
a
la
construcción
y
el
entrenamiento
del
modelo
.



El
DataLoader
está
configurado
para
una
iteración
eficiente
durante
el
entrenamiento
,
proporcionando
acceso
organizado
a
pares
de
imágenes
y
títulos
.



A
continuación
se
muestra
un
ejemplo
de
un
par
imagen-descripción
en
uno
de
los
lotes
del
dataset
.



Aquí
,
iniciamos
nuestro
CustomModel
y
lo
enviamos
al
dispositivo
(
CPU
o
GPU
)
.
Además
,
especificamos
los
parámetros
a
optimizar
durante
todo
el
proceso
de
entrenamiento
.
Dado
que
hemos
congelado
las
capas
base
de
los
encoders
de
texto
e
imágen
,
solo
los
parámetros
asociados
con
la
capa
de
proyección
se
entrenarán
en
el
nuevo
conjunto
de
datos
.



Entrenamiento
del
Modelo



El
siguiente
bucle
de
entrenamiento
se
realizó
con
una
máquina
GPU
Tesla
T4
(
g4dn-xlarge
)
durante
3
épocas
de
entrenamiento
.
Recuerde
activar
el
uso
de
GPU
en
colab
desde
la
pestaña
Entorno
de
ejecución
.



Uso
del
Modelo
Preentrenado



La
principal
fortaleza
de
CLIP
es
su
capacidad
para
relacionar
texto
e
imágenes
de
manera
robusta
.
Esto
lo
hace
especialmente
útil
en
cualquier
tarea
que
requiera
entender
y
manipular
información
visual
y
textual
de
forma
integrada
.



Algunas
de
estas
aplicaciones
son
:


Búsqueda
y
recuperación
de
imágenes
basadas
en
texto
:


Image
Search
:
Dado
un
texto
descriptivo
,
el
modelo
puede
buscar
y
recuperar
imágenes
que
coincidan
con
la
descripción
.


Text-to-Image
Retrieval
:
Dado
un
conjunto
de
imágenes
y
una
consulta
en
texto
,
el
modelo
puede
identificar
cuál
de
las
imágenes
es
más
relevante
para
la
consulta
.


Búsqueda
y
recuperación
de
texto
basado
en
imágenes
:


Image
Captioning
:
Dada
una
imagen
,
el
modelo
puede
generar
descripciones
en
texto
que
correspondan
a
la
imagen
.


Image-to-Text
Retrieval
:
Dado
un
conjunto
de
descripciones
textuales
y
una
imagen
,
el
modelo
puede
identificar
cuál
de
las
descripciones
es
más
relevante
para
la
imagen
.


Clasificación
de
imágenes
:


Zero-Shot
Classification
:
CLIP
puede
realizar
clasificación
de
imágenes
sin
necesidad
de
entrenar
específicamente
para
una
tarea
.
Solo
se
necesita
proporcionar
las
etiquetas
de
clase
en
forma
de
texto
,
y
el
modelo
determinará
la
clase
más
probable
para
una
imagen
dada
.


Generación
de
contenido
:


Visual
Question
Answering
(
VQA
):
Responder
preguntas
sobre
el
contenido
de
una
imagen
.


Text-based
Image
Generation
:
Utilizando
modelos
como
DALL-E
,
que
se
basan
en
principios
similares
,
se
pueden
generar
imágenes
a
partir
de
descripciones
textuales
.



Clasificación
de
Imágenes
Zero-Shot



CLIP
puede
realizar
clasificación
de
imágenes
sin
necesidad
de
entrenar
específicamente
para
una
tarea
.
Solo
se
necesita
proporcionar
las
etiquetas
de
clase
en
forma
de
texto
,
y
el
modelo
determinará
la
clase
más
probable
para
una
imagen
dada
.



Veamos
como
hacerlo
con
nuestro
CLIP
personalizado
y
con
la
versión
oficial
de
OpenAI
.



Usando
nuestro
modelo



Ya
que
hemos
entrenado
nuestro
propio
modelo
CLIP
,
podemos
generar
una
función
predict
que
preprocese
las
entradas
y
las
pase
por
el
modelo
para
hacer
clasificación
sin
necesidad
de
hacer
fine
tuning
.



Vamos
paso
a
paso
:


Preprocesamiento
de
la
imagen
:

    
python

    
img
=
flickr30kcustomdataset.transform(img).unsqueeze(0).to(device
)


    
Aquí
,
la
imagen
img
se
transforma
utilizando
un
método
de
preprocesamiento
definido
en
el
dataset
flickr30kcustomdataset
.
Luego
,
la
imagen
transformada
se
redimensiona
con
unsqueeze(0
)
para
agregar
una
dimensión
de
lote
(
batch
)
,
convirtiéndola
en
un
tensor
de
4
dimensiones
(
1
,
C
,
H
,
W
)
.
Finalmente
,
se
mueve
a
la
device
especificada
(
por
ejemplo
,
GPU
)
.


Codificación
de
la
imagen
:

    
python

    
imageembed
=
model.visionencoder(img
)


    
La
imagen
preprocesada
se
pasa
a
través
del
codificador
de
visión
(
visionencoder
)
del
modelo
,
obteniendo
un
vector
de
embeddings
de
la
imagen
(
imageembed
)
.


Preparación
de
los
textos
de
las
clases
:

    
python

    
text
=
[
"
a
picture
of
a
"
+
x
for
x
in
classes
]

    
text
=
model.tokenizer(text).to(device
)


    
Se
generan
frases
descriptivas
para
cada
clase
en
la
lista
classes
,
con
el
formato
"
a
picture
of
a
[
clase
]
"
.
Estas
frases
se
tokenizan
usando
el
tokenizer
del
modelo
y
se
convierten
a
tensores
que
se
mueven
al
dispositivo
especificado
(
device
)
.


Codificación
de
los
textos
:

    
python

    
captionembed
=
model.captionencoder(text["input_ids
"
]
)


    
Los
textos
tokenizados
se
pasan
a
través
del
codificador
de
captions
(
captionencoder
)
del
modelo
,
obteniendo
un
vector
de
embeddings
de
los
textos
(
captionembed
)
.


Cálculo
de
la
similitud
:

    
python

    
similarity
=
captionembed
@
imageembed
.
T


    
Se
calcula
la
similitud
entre
los
embeddings
de
las
captions
y
el
embedding
de
la
imagen
utilizando
el
producto
interno
(
@
)
,
que
es
una
operación
de
multiplicación
de
matrices
.
Aquí
se
multiplica
el
embedding
de
los
textos
(
captionembed
)
por
la
transpuesta
del
embedding
de
la
imagen
(
imageembed
.
T
)
.


Aplicación
de
softmax
para
obtener
probabilidades
:

    
python

    
return
similarity.softmax(dim=0
)


    
Finalmente
,
se
aplica
la
función
softmax
a
las
similitudes
calculadas
a
lo
largo
de
la
dimensión
0
(
las
diferentes
clases
)
para
convertirlas
en
probabilidades
.
El
resultado
es
un
tensor
donde
cada
valor
representa
la
probabilidad
de
que
la
imagen
corresponda
a
cada
una
de
las
clases
dadas
.



En
resumen
,
esta
función
toma
una
imagen
y
una
lista
de
clases
,
y
devuelve
un
tensor
de
probabilidades
que
indica
cuán
probable
es
que
la
imagen
pertenezca
a
cada
una
de
las
clases
especificadas
.



Veamos
que
tan
bien
le
va
clasificando
entre
perros
y
gatos
la
siguiente
imagen
.



Podemos
ver
que
nuestro
modelo
no
está
muy
seguro
de
la
respuesta
aunque
está
ligeramente
inclinado
a
contestar
perro
.
Esto
puede
significar
que
nuestro
modelo
requiere
más
epochs
de
entrenamiento
.
Veamos
si
el
modelo
oficial
de
OpenAI
realiza
una
mejor
tarea
.



Usando
la
versión
oficial
de
CLIP



El
siguiente
código
muestra
una
función
predict
equivalente
a
la
anterior
,
pero
que
utiliza
la
versión
de
OpenAI
de
CLIP
que
viene
incluida
en
HuggingFace
.
Para
ello
haremos
uso
directo
de
unas
clases
preparadas
para
la
tarea
:


CLIPModel.from_pretrained("openai
/
clip-vit-base-patch32
"
):
Carga
un
modelo
CLIP
preentrenado
,
que
contiene
tanto
un
codificador
de
imágenes
como
un
codificador
de
texto
.


CLIPProcessor.from_pretrained("openai
/
clip-vit-base-patch32
"
):
Carga
el
procesador
asociado
,
que
se
encarga
de
transformar
imágenes
y
texto
en
el
formato
adecuado
para
el
modelo
CLIP
.



A
continuación
haremos
la
prueba
con
la
misma
imagen
de
los
gatos
para
corroborar
si
hay
un
mejor
desempeño
.



Podemos
ver
que
este
modelo
hace
una
tarea
fenomenal
,
estableciendo
sin
lugar
a
dudas
que
se
trata
de
un
gato
.
Uno
podría
preguntarse
por
qué
el
resultado
es
tan
superior
.
La
respuesta
viene
en
la
diferencia
en
los
tamaños
de
los
datasets
y
la
cantidad
de
épocas
realizadas
.



El
entrenamiento
de
modelos
a
gran
escala
,
como
CLIP
,
generalmente
implica
entrenamientos
durante
varias
semanas
o
incluso
meses
en
grandes
clústeres
de
GPU
.
Durante
este
tiempo
,
los
modelos
pasan
por
miles
o
incluso
decenas
de
miles
de
épocas
para
optimizar
los
parámetros
y
aprender
representaciones
de
datos
de
alta
calidad
.
Mientras
que
el
conjunto
de
datos
contiene
aproximadamente
400
millones
de
pares
de
imágenes
y
textos
.



Conclusión



En
conclusión
,
esta
clase
ha
explorado
el
modelo
CLIP
y
ha
descubierto
su
potencial
para
una
amplia
gama
de
aplicaciones
.
A
medida
que
entendemos
las
aplicaciones
de
CLIP
,
resulta
evidente
que
su
impacto
va
mucho
más
allá
de
las
expectativas
iniciales
,
allanando
el
camino
para
soluciones
innovadoras
en
diversos
campos
.
CLIP
fue
el
primer
modelo
exitoso
que
cerró
la
brecha
entre
diferentes
modalidades
y
abrió
vías
para
innovaciones
interdisciplinarias
.
