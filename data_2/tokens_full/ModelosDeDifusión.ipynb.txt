Modelos
de
Difusi√≥n
(
DDPMs
)



¬ø
Qu√©
es
la
Difusi√≥n
?



La
difusi√≥n
es
un
fen√≥meno
natural
fundamental
observado
en
varios
sistemas
,
incluyendo
la
f√≠sica
,
la
qu√≠mica
y
la
biolog√≠a
.
Esto
se
nota
f√°cilmente
en
la
vida
cotidiana
.
Consider√°
el
ejemplo
de
rociar
perfume
.
Al
principio
,
las
mol√©culas
del
perfume
est√°n
densamente
concentradas
cerca
del
punto
de
rociado
.
Con
el
tiempo
,
las
mol√©culas
se
dispersan
.



La
difusi√≥n
es
el
proceso
por
el
cual
las
part√≠culas
,
la
informaci√≥n
o
la
energ√≠a
se
mueven
desde
un
√°rea
de
alta
concentraci√≥n
a
una
de
menor
concentraci√≥n
.
Esto
sucede
porque
los
sistemas
tienden
a
alcanzar
el
equilibrio
,
donde
las
concentraciones
se
vuelven
uniformes
en
todo
el
sistema
.



En
el
aprendizaje
autom√°tico
y
la
generaci√≥n
de
datos
,
la
difusi√≥n
se
refiere
a
un
enfoque
espec√≠fico
para
generar
datos
usando
un
proceso
estoc√°stico
similar
a
una
cadena
de
Markov
.
En
este
contexto
,
los
modelos
de
difusi√≥n
crean
nuevas
muestras
de
datos
comenzando
con
datos
simples
y
f√°ciles
de
generar
,
y
gradualmente
transform√°ndolos
en
datos
m√°s
complejos
y
realistas
.



En
esta
clase
,
vamos
a
profundizar
en
los
Modelos
Probabil√≠sticos
de
Difusi√≥n
para
Eliminaci√≥n
de
Ruido
(
tambi√©n
conocidos
como
DDPMs
por
las
siglas
en
ingl√©s
de
Denoising
Diffusion
Probabilistic
Models
)
ya
que
los
investigadores
han
logrado
resultados
notables
con
ellos
para
la
generaci√≥n
(
no
condicionada
)
de
im√°genes
/
audio
/
video
.



Primero
,
vamos
a
instalar
e
importar
las
bibliotecas
necesarias
(
asumiendo
que
ya
ten√©s
PyTorch
instalado
)
.



¬ø
Qu√©
es
un
modelo
de
difusi√≥n
?



Un
modelo
de
difusi√≥n
(
de
eliminaci√≥n
de
ruido
)
no
es
tan
complejo
si
lo
compar√°s
con
otros
modelos
generativos
como
GANs
o
VAEs
:
todos
ellos
convierten
ruido
de
alguna
distribuci√≥n
simple
en
una
muestra
de
datos
.
Esto
tambi√©n
ocurre
aqu√≠
,
donde
una
red
neuronal
aprende
a
eliminar
gradualmente
el
ruido
de
los
datos
comenzando
desde
puro
ruido
.



En
un
poco
m√°s
de
detalle
para
im√°genes
,
la
configuraci√≥n
consta
de
2
procesos
:


Un
proceso
de
difusi√≥n
hacia
adelante
fijo
(
o
predefinido
)
de
nuestra
elecci√≥n
,
que
agrega
gradualmente
ruido
gaussiano
a
una
imagen
,
hasta
que
termin√°s
con
puro
ruido
.


Un
proceso
de
difusi√≥n
inverso
de
eliminaci√≥n
de
ruido
aprendido
,
donde
se
entrena
una
red
neuronal
para
eliminar
gradualmente
el
ruido
de
una
imagen
comenzando
desde
puro
ruido
,
hasta
que
termin√°s
con
una
imagen
real
.



Tanto
el
proceso
hacia
adelante
como
el
proceso
inverso
,
indexados
por
,
ocurren
durante
un
cierto
n√∫mero
finito
de
pasos
de
tiempo
 
(
los
autores
de
DDPM
usan
)
.
Comenz√°s
con
 
donde
muestre√°s
una
imagen
real
 
de
tu
distribuci√≥n
de
datos
(
digamos
una
imagen
de
un
gato
de
ImageNet
)
,
y
el
proceso
hacia
adelante
muestrea
algo
de
ruido
de
una
distribuci√≥n
gaussiana
en
cada
paso
de
tiempo
,
que
se
agrega
a
la
imagen
del
paso
de
tiempo
anterior
.
Dado
un
 
suficientemente
grande
y
un
esquema
bien
estructurado
para
agregar
ruido
en
cada
paso
de
tiempo
,
termin√°s
con
lo
que
se
llama
una
distribuci√≥n
gaussiana
isotr√≥pica
en
 
mediante
un
proceso
gradual
.



En
forma
m√°s
matem√°tica



Vamos
a
escribir
esto
de
manera
m√°s
formal
,
ya
que
en
√∫ltima
instancia
necesitamos
una
funci√≥n
de
p√©rdida
tratable
que
nuestra
red
neuronal
pueda
optimizar
.



Sea
 
la
distribuci√≥n
de
datos
reales
,
por
ejemplo
,
de
"
im√°genes
reales
"
.
Podemos
muestrear
de
esta
distribuci√≥n
para
obtener
una
imagen
,
.
Definimos
el
proceso
de
difusi√≥n
hacia
adelante
,
que
agrega
ruido
gaussiano
en
cada
paso
de
tiempo
,
de
acuerdo
con
un
esquema
de
varianza
conocido
 
como



Recordemos
que
una
distribuci√≥n
normal
(
tambi√©n
llamada
distribuci√≥n
gaussiana
)
se
define
por
2
par√°metros
:
una
media
 
y
una
varianza
.
B√°sicamente
,
cada
nueva
imagen
(
ligeramente
m√°s
ruidosa
)
en
el
paso
de
tiempo
 
se
extrae
de
una
distribuci√≥n
gaussiana
condicional
con
 
y
,
lo
cual
podemos
hacer
muestreando
 
y
luego
estableciendo
.



Notemos
que
los
 
no
son
constantes
en
cada
paso
de
tiempo
 
(
de
ah√≠
el
sub√≠ndice
)
‚Äî
de
hecho
,
uno
define
un
llamado
"
esquema
de
varianza
"
,
que
puede
ser
lineal
,
cuadr√°tico
,
coseno
,
etc.
,
como
veremos
m√°s
adelante
(
un
poco
como
un
esquema
de
tasa
de
aprendizaje
)
.



Entonces
,
comenzando
desde
,
terminamos
con
,
donde
 
es
ruido
gaussiano
puro
si
configuramos
el
esquema
adecuadamente
.



Ahora
,
si
conoci√©ramos
la
distribuci√≥n
condicional
,
entonces
podr√≠amos
ejecutar
el
proceso
en
reversa
:
muestreando
algo
de
ruido
gaussiano
aleatorio
,
y
luego
"
eliminando
el
ruido
"
gradualmente
para
que
terminemos
con
una
muestra
de
la
distribuci√≥n
real
.



Sin
embargo
,
no
conocemos
.
Es
intratable
ya
que
requiere
conocer
la
distribuci√≥n
de
todas
las
im√°genes
posibles
para
calcular
esta
probabilidad
condicional
.
Por
lo
tanto
,
vamos
a
aprovechar
una
red
neuronal
para
aproximar
(
aprender
)
esta
distribuci√≥n
de
probabilidad
condicional
,
llam√©mosla
,
siendo
 
los
par√°metros
de
la
red
neuronal
,
actualizados
mediante
descenso
de
gradiente
.



Entonces
,
necesitamos
una
red
neuronal
para
representar
una
distribuci√≥n
de
probabilidad
(
condicional
)
del
proceso
inverso
.
Si
asumimos
que
este
proceso
inverso
tambi√©n
es
gaussiano
,
recordemos
que
cualquier
distribuci√≥n
gaussiana
se
define
por
2
par√°metros
:


una
media
parametrizada
por
;


una
varianza
parametrizada
por
;



por
lo
que
podemos
parametrizar
el
proceso
como



donde
la
media
y
la
varianza
tambi√©n
est√°n
condicionadas
por
el
nivel
de
ruido
.



Por
lo
tanto
,
nuestra
red
neuronal
necesita
aprender
/
representar
la
media
y
la
varianza
.
Sin
embargo
,
los
autores
de
DDPM
decidieron
mantener
la
varianza
fija
y
permitir
que
la
red
neuronal
solo
aprenda
(
represente
)
la
media
 
de
esta
distribuci√≥n
de
probabilidad
condicional
.
Entonces
continuamos
,
asumiendo
que
nuestra
red
neuronal
solo
necesita
aprender
/
representar
la
media
de
esta
distribuci√≥n
de
probabilidad
condicional
.



Definiendo
una
funci√≥n
objetivo
(
reparametrizando
la
media
)



Dado
que
nuestra
red
neuronal
debe
aprender
la
media
de
nuestra
distribuci√≥n
de
probabilidad
condicional
,
nuestra
funci√≥n
de
p√©rdida
se
puede
expresar
como
una
suma
de
p√©rdidas
en
cada
paso
de
tiempo
,
.



Cada
t√©rmino
de
esta
suma
(
excepto
)
de
la
p√©rdida
es
en
realidad
la
divergencia
KL
entre
dos
distribuciones
gaussianas
(
la
real
y
la
predicha
)
.
La
divergencia
KL
mide
cu√°nto
se
desv√≠a
una
distribuci√≥n
de
otra
,
y
en
este
caso
,
se
puede
expresar
como
una
p√©rdida
L2
con
respecto
a
las
medias
de
estas
distribuciones
.



Una
consecuencia
directa
del
proceso
hacia
adelante
,
como
lo
muestra
Sohl-Dickstein
et
al
.
,
es
que
podemos
muestrear
 
en
cualquier
nivel
de
ruido
arbitrario
condicionado
a
 
(
ya
que
la
suma
de
gaussianas
tambi√©n
es
gaussiana
)
.
Esto
es
muy
conveniente
:
no
necesitamos
aplicar
 
repetidamente
para
muestrear
.



Tenemos
que



con
 
y
.



Refiramos
a
esta
ecuaci√≥n
como
la
"
propiedad
agradable
"
.
Esto
significa
que
podemos
muestrear
ruido
gaussiano
y
escalarlo
apropiadamente
y
agregarlo
a
 
para
obtener
 
directamente
.
Notemos
que
los
 
son
funciones
del
esquema
de
varianza
conocido
 
y
,
por
lo
tanto
,
tambi√©n
son
conocidos
y
pueden
precomputarse
.
Esto
nos
permite
,
durante
el
entrenamiento
,
optimizar
t√©rminos
aleatorios
de
la
funci√≥n
de
p√©rdida
 
(
o
en
otras
palabras
,
muestrear
aleatoriamente
 
durante
el
entrenamiento
y
optimizar
)
.



Otra
ventaja
de
esta
propiedad
,
como
se
muestra
en
Ho
et
al
.
,
es
que
uno
puede
(
despu√©s
de
algunos
c√°lculos
,
para
los
cuales
remitimos
al
lector
a
este
excelente
posteo
de
blog
)
reparametrizar
la
media
para
que
la
red
neuronal
aprenda
(
prediga
)
el
ruido
agregado
(
a
trav√©s
de
una
red
)
para
el
nivel
de
ruido
 
en
los
t√©rminos
KL
que
constituyen
las
p√©rdidas
.
Esto
significa
que
nuestra
red
neuronal
se
convierte
en
un
predictor
de
ruido
,
en
lugar
de
un
predictor
de
media
directo
.
La
media
se
puede
calcular
como
sigue
:



La
funci√≥n
objetivo
final
 
entonces
queda
de
la
siguiente
manera
(
para
un
paso
de
tiempo
 
aleatorio
dado
):



Aqu√≠
,
 
es
la
imagen
inicial
(
real
,
no
corrompida
)
,
y
vemos
la
muestra
directa
del
nivel
de
ruido
 
dada
por
el
proceso
hacia
adelante
fijo
.
 
es
el
ruido
puro
muestreado
en
el
paso
de
tiempo
,
y
 
es
nuestra
red
neuronal
.
La
red
neuronal
se
optimiza
utilizando
un
simple
error
cuadr√°tico
medio
(
MSE
)
entre
el
ruido
gaussiano
real
y
el
predicho
.



En
Resumen



Tomamos
una
muestra
aleatoria
 
de
la
distribuci√≥n
de
datos
real
,
desconocida
y
posiblemente
compleja
.


Muestreamos
un
nivel
de
ruido
 
uniformemente
entre
1
y
 
(
es
decir
,
un
paso
de
tiempo
aleatorio
)
.


Muestreamos
algo
de
ruido
de
una
distribuci√≥n
gaussiana
y
corrompemos
la
entrada
con
este
ruido
en
el
nivel
 
(
usando
la
"
propiedad
agradable
"
definida
anteriormente
)
.


La
red
neuronal
se
entrena
para
predecir
este
ruido
basado
en
la
imagen
corrupta
 
(
es
decir
,
el
ruido
aplicado
a
 
basado
en
el
esquema
conocido
)
.



Implementaci√≥n
en
PyTorch



La
Red
Neuronal



La
red
neuronal
necesita
tomar
una
imagen
ruidosa
en
un
paso
de
tiempo
particular
y
devolver
el
ruido
predicho
.
Notemos
que
el
ruido
predicho
es
un
tensor
que
tiene
el
mismo
tama√±o
/
resoluci√≥n
que
la
imagen
de
entrada
.



Entonces
,
t√©cnicamente
,
la
red
toma
y
produce
tensores
de
la
misma
forma
.
¬ø
Qu√©
tipo
de
red
neuronal
podemos
usar
para
esto
?



En
t√©rminos
de
arquitectura
,
los
autores
de
DDPM
optaron
por
un
U-Net
,
introducido
por
Ronneberger
et
al
.
,
2015
(
que
,
en
su
momento
,
logr√≥
resultados
de
√∫ltima
generaci√≥n
para
la
segmentaci√≥n
de
im√°genes
m√©dicas
)
.
Esta
red
,
como
cualquier
autoencoder
,
consta
de
un
cuello
de
botella
en
el
medio
que
asegura
que
la
red
aprenda
solo
la
informaci√≥n
m√°s
importante
.
Importante
destacar
que
introduce
conexiones
residuales
entre
el
codificador
y
el
decodificador
,
mejorando
significativamente
el
flujo
de
gradientes
(
inspirado
por
ResNet
en
He
et
al
.
,
2015
)
.



Como
se
puede
ver
,
un
modelo
U-Net
primero
reduce
la
resoluci√≥n
espacial
de
la
entrada
(
es
decir
,
hace
que
la
entrada
sea
m√°s
peque√±a
en
t√©rminos
de
resoluci√≥n
espacial
)
,
despu√©s
de
lo
cual
se
realiza
un
aumento
de
resoluci√≥n
.



A
continuaci√≥n
,
implementamos
esta
red
paso
a
paso
.



Funciones
Auxiliares
de
la
Red



Primero
,
definimos
algunas
funciones
y
clases
auxiliares
que
se
utilizar√°n
al
implementar
la
red
neuronal
.
Es
importante
destacar
que
definimos
un
m√≥dulo
Residual
,
que
simplemente
suma
la
entrada
a
la
salida
de
una
funci√≥n
particular
(
en
otras
palabras
,
a√±ade
una
conexi√≥n
residual
a
una
funci√≥n
particular
)
.



Tambi√©n
definimos
alias
para
las
operaciones
de
reducci√≥n
y
aumento
de
resoluci√≥n
(
upsampling
y
downsampling
)
.



Embeddings
de
Posici√≥n



Dado
que
los
par√°metros
de
la
red
neuronal
se
comparten
a
lo
largo
del
tiempo
(
nivel
de
ruido
)
,
los
autores
emplean
embeddings
de
posici√≥n
sinusoidales
para
codificar
tt
,
inspirados
en
el
Transformer
(
Vaswani
et
al
.
,
2017
)
.
Esto
permite
que
la
red
neuronal
"
sepa
"
en
qu√©
paso
de
tiempo
particular
(
nivel
de
ruido
)
est√°
operando
,
para
cada
imagen
en
un
lote
.



El
m√≥dulo
SinusoidalPositionEmbeddings
toma
un
tensor
de
forma
(
batchsize,1
)
como
entrada
(
es
decir
,
los
niveles
de
ruido
de
varias
im√°genes
ruidosas
en
un
lote
)
,
y
lo
convierte
en
un
tensor
de
forma
(
batchsize
,
dim
)
,
donde
dim
es
la
dimensionalidad
de
los
embeddings
de
posici√≥n
.
Esto
se
a√±ade
a
cada
bloque
residual
,
como
veremos
m√°s
adelante
.



Bloque
ResNet



A
continuaci√≥n
,
definimos
el
bloque
central
del
modelo
U-Net
.
Los
autores
de
DDPM
emplearon
un
bloque
Wide
ResNet
(
Zagoruyko
et
al
.
,
2016
)
,
pero
Phil
Wang
ha
reemplazado
la
capa
convolucional
est√°ndar
por
una
versi√≥n
"
weight
standardized
"
,
que
funciona
mejor
en
combinaci√≥n
con
la
normalizaci√≥n
por
grupos
(
ver
Kolesnikov
et
al
.
,
2019
para
m√°s
detalles
)
.



M√≥dulo
de
Atenci√≥n



A
continuaci√≥n
,
definimos
el
m√≥dulo
de
atenci√≥n
,
que
los
autores
de
DDPM
a√±adieron
entre
los
bloques
convolucionales
.
La
atenci√≥n
es
el
bloque
de
construcci√≥n
de
la
famosa
arquitectura
Transformer
(
Vaswani
et
al
.
,
2017
)
,
que
ha
mostrado
gran
√©xito
en
varios
dominios
de
la
IA
,
desde
NLP
y
visi√≥n
hasta
plegamiento
de
prote√≠nas
.
Phil
Wang
emplea
2
variantes
de
atenci√≥n
:
una
es
la
atenci√≥n
de
m√∫ltiples
cabezas
(
multi-head
self-attention
)
regular
(
como
se
usa
en
el
Transformer
)
,
y
la
otra
es
una
variante
de
atenci√≥n
lineal
(
Shen
et
al
.
,
2018
)
,
cuyos
requisitos
de
tiempo
y
memoria
escalan
de
manera
lineal
con
la
longitud
de
la
secuencia
,
a
diferencia
de
la
atenci√≥n
regular
que
escala
de
manera
cuadr√°tica
.



Normalizaci√≥n
por
Grupos



Los
autores
de
DDPM
intercalan
las
capas
convolucionales
/
de
atenci√≥n
del
U-Net
con
normalizaci√≥n
por
grupos
(
Wu
et
al
.
,
2018
)
.
A
continuaci√≥n
,
definimos
una
clase
PreNorm
,
que
se
utilizar√°
para
aplicar
la
normalizaci√≥n
por
grupos
antes
de
la
capa
de
atenci√≥n
,
como
veremos
m√°s
adelante
.



U-Net
Condicional



Ahora
que
hemos
definido
todos
los
bloques
de
construcci√≥n
(
embeddings
de
posici√≥n
,
bloques
ResNet
,
atenci√≥n
y
normalizaci√≥n
por
grupos
)
,
es
hora
de
definir
toda
la
red
neuronal
.
Recordemos
que
la
tarea
de
la
red
 
es
tomar
un
lote
de
im√°genes
ruidosas
y
sus
respectivos
niveles
de
ruido
,
y
devolver
el
ruido
a√±adido
a
la
entrada
.
M√°s
formalmente
:


La
red
toma
un
lote
de
im√°genes
ruidosas
de
forma
 
y
un
lote
de
niveles
de
ruido
de
forma
 
como
entrada
,
y
devuelve
un
tensor
de
forma
.



La
red
se
construye
de
la
siguiente
manera
:


Primero
,
se
aplica
una
capa
convolucional
sobre
el
lote
de
im√°genes
ruidosas
y
se
calculan
los
embeddings
de
posici√≥n
para
los
niveles
de
ruido
.


Luego
,
se
aplica
una
secuencia
de
etapas
de
reducci√≥n
de
resoluci√≥n
(
downsampling
)
.
Cada
etapa
de
reducci√≥n
de
resoluci√≥n
consta
de
2
bloques
ResNet
+
normalizaci√≥n
por
grupos
+
atenci√≥n
+
conexi√≥n
residual
+
una
operaci√≥n
de
reducci√≥n
de
resoluci√≥n
.


En
el
medio
de
la
red
,
se
vuelven
a
aplicar
bloques
ResNet
,
intercalados
con
atenci√≥n
.


Luego
,
se
aplica
una
secuencia
de
etapas
de
aumento
de
resoluci√≥n
(
upsampling
)
.
Cada
etapa
de
aumento
de
resoluci√≥n
consta
de
2
bloques
ResNet
+
normalizaci√≥n
por
grupos
+
atenci√≥n
+
conexi√≥n
residual
+
una
operaci√≥n
de
aumento
de
resoluci√≥n
.


Finalmente
,
se
aplica
un
bloque
ResNet
seguido
de
una
capa
convolucional
.



En
√∫ltima
instancia
,
las
redes
neuronales
apilan
capas
como
si
fueran
bloques
de
lego
(
pero
es
importante
entender
c√≥mo
funcionan
)
.



Definiendo
el
proceso
de
difusi√≥n
hacia
adelante



El
proceso
de
difusi√≥n
hacia
adelante
agrega
gradualmente
ruido
a
una
imagen
de
la
distribuci√≥n
real
,
en
una
cantidad
de
pasos
de
tiempo
T.
Esto
ocurre
de
acuerdo
con
un
programa
de
varianza
.
Los
autores
originales
de
DDPM
emplearon
un
programa
lineal
:


    
"
Establecemos
las
varianzas
del
proceso
hacia
adelante
en
constantes
que
aumentan
linealmente
desde
Œ≤1=10‚àí4
hasta
Œ≤T=0.02
.
"



Sin
embargo
,
se
demostr√≥
en
(
Nichol
et
al
.
,
2021
)
que
se
pueden
obtener
mejores
resultados
al
emplear
un
programa
cosenoidal
.



A
continuaci√≥n
,
definimos
varios
programas
para
los
pasos
de
tiempo
TT
(
elegiremos
uno
m√°s
adelante
)
.



Para
comenzar
,
usemos
el
programa
lineal
para
T=300
pasos
de
tiempo
y
definamos
las
diversas
variables
a
partir
de
Œ≤t
que
necesitaremos
,
como
el
producto
acumulativo
de
las
varianzas
Œ±Àât
.
Cada
una
de
las
variables
a
continuaci√≥n
son
solo
tensores
unidimensionales
,
que
almacenan
valores
desde
t
hasta
T.
Es
importante
tambi√©n
definir
una
funci√≥n
de
extracci√≥n
,
que
nos
permitir√°
extraer
el
√≠ndice
t
adecuado
para
un
lote
de
√≠ndices
.



Vamos
a
ilustrar
con
una
imagen
de
gatos
c√≥mo
se
agrega
ruido
en
cada
paso
de
tiempo
del
proceso
de
difusi√≥n
.



El
ruido
se
agrega
a
los
tensores
de
PyTorch
,
en
lugar
de
las
im√°genes
de
Pillow
.
Primero
definiremos
transformaciones
de
imagen
que
nos
permitan
pasar
de
una
imagen
PIL
a
un
tensor
de
PyTorch
(
sobre
el
cual
podemos
agregar
el
ruido
)
,
y
viceversa
.



Estas
transformaciones
son
bastante
simples
:
primero
normalizamos
las
im√°genes
dividi√©ndolas
por
255
(
de
modo
que
est√©n
en
el
rango
[
0,1
]
)
,
y
luego
nos
aseguramos
de
que
est√©n
en
el
rango
[
‚àí1,1
]
.
Del
art√≠culo
de
DDPM
:


    
"
Asumimos
que
los
datos
de
la
imagen
consisten
en
enteros
en
{
0,1,
...
,255
}
escalados
linealmente
a
[
‚àí1,1
]
.
Esto
asegura
que
el
proceso
inverso
de
la
red
neuronal
opere
sobre
entradas
escaladas
consistentemente
comenzando
desde
la
distribuci√≥n
normal
est√°ndar
p(xT
)
.
"



Ahora
podemos
definir
el
proceso
de
difusi√≥n
hacia
adelante
tal
como
en
el
art√≠culo
.



Prob√©moslo
en
un
paso
de
tiempo
particular
.



Visualicemos
esto
para
varios
pasos
de
tiempo
.



Esto
significa
que
ahora
podemos
definir
la
funci√≥n
de
p√©rdida
dado
el
modelo
de
la
siguiente
manera
.



El
denoise_model
ser√°
nuestro
U-Net
definido
anteriormente
.
Utilizaremos
la
p√©rdida
Huber
entre
el
ruido
verdadero
y
el
ruido
predicho
.



Definir
un
Dataset
y
un
DataLoader
en
PyTorch
.



Aqu√≠
definimos
un
Dataset
regular
en
PyTorch
.
El
dataset
simplemente
consiste
en
im√°genes
de
un
dataset
real
,
como
Fashion-MNIST
,
CIFAR-10
o
ImageNet
,
escaladas
linealmente
a
[
‚àí1,1
]
.



Cada
imagen
se
redimensiona
al
mismo
tama√±o
.
Es
interesante
notar
que
las
im√°genes
tambi√©n
se
voltean
horizontalmente
de
manera
aleatoria
.
Del
art√≠culo
:


    
"
Usamos
volteos
horizontales
aleatorios
durante
el
entrenamiento
para
CIFAR10
;
probamos
entrenar
tanto
con
como
sin
volteos
,
y
encontramos
que
los
volteos
mejoran
ligeramente
la
calidad
de
las
muestras
.
"



Aqu√≠
usamos
la
biblioteca
ü§ó
Datasets
para
cargar
f√°cilmente
el
dataset
Fashion
MNIST
desde
el
hub
.
Este
dataset
consiste
en
im√°genes
que
ya
tienen
la
misma
resoluci√≥n
,
es
decir
,
28x28
.



A
continuaci√≥n
,
definimos
una
funci√≥n
que
aplicaremos
sobre
la
marcha
a
todo
el
dataset
.
Usamos
la
funcionalidad
with_transform
para
eso
.
La
funci√≥n
solo
aplica
un
preprocesamiento
b√°sico
de
im√°genes
:
volteos
horizontales
aleatorios
,
reescalado
y
finalmente
,
hacer
que
tengan
valores
en
el
rango
[
‚àí1,1
]
.



Muestreo



Como
muestrearemos
del
modelo
durante
el
entrenamiento
(
para
seguir
el
progreso
)
,
definimos
el
c√≥digo
para
eso
a
continuaci√≥n
.
El
muestreo
se
resume
en
el
art√≠culo
como
el
Algoritmo
2
:



Generar
nuevas
im√°genes
a
partir
de
un
modelo
de
difusi√≥n
ocurre
al
invertir
el
proceso
de
difusi√≥n
:
comenzamos
desde
T
,
donde
muestreamos
ruido
puro
de
una
distribuci√≥n
Gaussiana
,
y
luego
usamos
nuestra
red
neuronal
para
deshacer
gradualmente
el
ruido
(
usando
la
probabilidad
condicional
que
ha
aprendido
)
,
hasta
que
terminamos
en
el
paso
de
tiempo
t=0
.
Como
se
muestra
arriba
,
podemos
derivar
una
imagen
ligeramente
menos
ruidosa
 
al
enchufar
la
reparametrizaci√≥n
de
la
media
,
usando
nuestro
predictor
de
ruido
.
Recuerda
que
la
varianza
se
conoce
de
antemano
.



Idealmente
,
terminamos
con
una
imagen
que
parece
provenir
de
la
distribuci√≥n
de
datos
reales
.



El
c√≥digo
a
continuaci√≥n
implementa
esto
.



Este
c√≥digo
implementa
el
proceso
de
muestreo
en
un
modelo
de
difusi√≥n
.
Su
objetivo
es
generar
una
nueva
imagen
a
partir
de
ruido
puro
,
siguiendo
una
serie
de
pasos
de
denoising
(
reducci√≥n
progresiva
de
ruido
)
.
Vamos
a
analizar
cada
funci√≥n
para
entender
c√≥mo
se
lleva
a
cabo
este
proceso
.


p_sample



La
funci√≥n
p_sample
genera
una
muestra
para
un
paso
temporal
espec√≠fico
t
(
un
nivel
de
ruido
dado
en
el
proceso
de
denoising
)
.
La
estructura
de
esta
funci√≥n
sigue
el
modelo
de
difusi√≥n
inversa
,
que
elimina
el
ruido
paso
a
paso
,
comenzando
desde
una
imagen
completamente
ruidosa
.


Par√°metros
de
entrada
:


model
:
El
modelo
de
difusi√≥n
(
normalmente
una
U-Net
)
,
que
predice
el
ruido
presente
en
la
imagen
en
el
paso
t.


x
:
La
imagen
(
o
tensor
)
actual
en
el
paso
t.


t
:
El
√≠ndice
temporal
del
paso
actual
,
que
indica
el
nivel
de
ruido
.


t_index
:
√çndice
del
paso
actual
en
el
bucle
(
usado
para
verificar
si
estamos
en
el
paso
inicial
o
no
)
.


Proceso
dentro
de
p_sample
:


Extraer
Par√°metros
Temporales
:


betast
,
sqrtoneminusalphascumprodt
,
y
sqrtrecipalphas_t
se
extraen
del
registro
predefinido
de
par√°metros
del
modelo
,
en
funci√≥n
de
t.
Estos
valores
est√°n
precomputados
y
permiten
calcular
la
cantidad
de
ruido
o
informaci√≥n
preservada
en
cada
paso
del
muestreo
.


Predecir
la
Media
(
Ecuaci√≥n
de
Denoising
):


La
expresi√≥n
para
model_mean
implementa
la
Ecuaci√≥n
11
en
el
paper
de
DDPM
,
que
define
c√≥mo
usar
la
salida
del
modelo
para
predecir
una
versi√≥n
menos
ruidosa
de
la
imagen
.


model(x
,
t
)
predice
el
ruido
en
la
imagen
x
en
el
paso
t.
Este
ruido
se
elimina
de
x
para
obtener
una
imagen
denoised
usando
la
expresi√≥n
:

     
[

     
\text{model\mean
}
=
\text{sqrt\recip\alphas\t
}
\times
\left
(
x
-
\frac{\text{betas\t
}
\times
\text{model}(x
,
t)}{\text{sqrt\one\minus\alphas\cumprod\t
}
}
\right
)

     
]


A√±adir
Ruido
seg√∫n
la
Varianza
Posterior
:


Si
tindex
es
0
(
√∫ltimo
paso
)
,
se
retorna
directamente
modelmean
como
la
imagen
final
.


Para
pasos
intermedios
,
se
a√±ade
un
componente
de
ruido
(
noise
)
ponderado
por
posteriorvariancet
.
Esto
simula
la
incertidumbre
en
el
proceso
de
denoising
.


psampleloop



Esta
funci√≥n
aplica
p_sample
en
un
bucle
para
generar
una
imagen
desde
el
ruido
puro
,
retrocediendo
en
los
pasos
temporales
desde
T
(
tiempo
m√°ximo
)
hasta
0
(
imagen
final
)
.


Par√°metros
de
entrada
:


model
:
El
modelo
de
difusi√≥n
.


shape
:
La
forma
de
la
imagen
a
generar
,
incluyendo
el
tama√±o
de
lote
.


Proceso
dentro
de
psampleloop
:


Inicializaci√≥n
con
Ruido
Puro
:


Comienza
con
una
imagen
img
que
es
simplemente
ruido
aleatorio
.
La
forma
de
img
depende
de
shape
(
en
este
caso
,
(
batchsize
,
channels
,
imagesize
,
image_size
)
)
.


Muestreo
Paso
a
Paso
:


Se
itera
desde
timesteps
hasta
0
,
generando
una
versi√≥n
menos
ruidosa
de
la
imagen
en
cada
paso
i.


En
cada
iteraci√≥n
,
p_sample
se
llama
con
la
imagen
actual
img
,
el
paso
de
tiempo
i
,
y
se
guarda
la
salida
en
imgs
(
lista
de
im√°genes
en
cada
paso
de
denoising
)
.


Resultado
:


imgs
contiene
el
historial
de
im√°genes
generadas
en
cada
paso
de
denoising
,
desde
el
ruido
inicial
hasta
la
imagen
generada
final
.


sample



La
funci√≥n
sample
es
una
envoltura
conveniente
para
configurar
y
llamar
a
psampleloop
con
los
par√°metros
de
tama√±o
de
imagen
y
tama√±o
de
lote
.


Par√°metros
de
entrada
:


model
:
El
modelo
de
difusi√≥n
.


image_size
:
Tama√±o
de
la
imagen
generada
.


batch_size
:
Tama√±o
del
lote
de
im√°genes
generadas
.


channels
:
N√∫mero
de
canales
de
la
imagen
(
normalmente
3
para
im√°genes
RGB
)
.


Proceso
dentro
de
sample
:


Define
la
forma
de
la
imagen
(
shape
)
seg√∫n
batchsize
,
channels
,
imagesize
.


Llama
a
psampleloop
con
el
modelo
y
la
forma
de
la
imagen
,
generando
el
conjunto
de
im√°genes
finales
en
el
lote
.


Resumen
General
del
Flujo


Empieza
con
Ruido
:
sample
llama
a
psampleloop
,
donde
img
se
inicializa
como
ruido
.


Proceso
de
Denoising
en
Bucle
:
psampleloop
realiza
una
secuencia
de
pasos
de
denoising
desde
el
m√°ximo
nivel
de
ruido
hasta
la
imagen
final
.


Retorno
de
Im√°genes
:
En
cada
paso
de
p_sample
,
se
aplica
una
correcci√≥n
de
ruido
basada
en
la
predicci√≥n
del
modelo
y
se
agrega
ruido
para
los
pasos
intermedios
.
En
el
√∫ltimo
paso
,
se
obtiene
una
imagen
limpia
,
que
es
el
resultado
final
del
modelo
de
difusi√≥n
.



Entrenar
el
modelo



A
continuaci√≥n
,
entrenamos
el
modelo
de
la
manera
habitual
en
PyTorch
.
Tambi√©n
definimos
alguna
l√≥gica
para
guardar
peri√≥dicamente
las
im√°genes
generadas
,
utilizando
el
m√©todo
de
muestreo
definido
anteriormente
.



A
continuaci√≥n
,
definimos
el
modelo
y
lo
movemos
a
la
GPU
.
Tambi√©n
definimos
un
optimizador
est√°ndar
(
Adam
)
.



¬°
Empecemos
a
entrenar
!



Muestreo
(
inferencia
)



Para
muestrear
del
modelo
,
simplemente
podemos
usar
nuestra
funci√≥n
de
muestreo
definida
anteriormente
:
