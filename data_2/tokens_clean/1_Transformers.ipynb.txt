href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
NLP/6Transformers/1Transformers.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Transformers
Necesitamos
Prestar
Atención
clase
analizamos
mecanismos
atención
método
omnipresente
modelos
modernos
Deep
Learning
atención
concepto
ayudó
mejorar
rendimiento
aplicaciones
traducción
automática
neuronal
clase
veremos
The
Transformer
modelo
utiliza
mecanismos
atención
aumentar
velocidad
entrenar
modelos
Transformer
propuesto
artículo
Attention
is
All
you
Need
publicación
intentaremos
simplificar
cosas
introducir
conceptos
fácil
entender
personas
conocimiento
profundo
tema
Estructura
General
Comencemos
mirando
modelo
caja
negra
aplicación
traducción
automática
tomaría
oración
idioma
generaría
traducción
abrir
Optimus
Prime
vemos
encoder
decoder
conexiones
encoder
realidad
pila
encoders
paper
apila
mágico
número
definitivamente
experimentar
arreglos
decoder
pila
número
decoders
encoders
idénticos
estructura
comparten
pesos
divide
subcapas
entradas
encoder
fluyen
capa
Auto-Atención
Self-Attention
capa
ayuda
codificador
palabras
oración
entrada
codifica
palabra
específica
Veremos
cerca
Auto-Atención
notebook
salidas
capa
Auto-Atención
envían
red
feed-forward
nombre
reciben
MLP
red
feed-forward
aplica
forma
independiente
posición
decoder
ambas
capas
capa
atención
común
ayuda
decoder
enfocarse
partes
relevantes
oración
entrada
Flujo
Tensores
visto
componentes
principales
modelo
comencemos
diversos
tensores
fluyen
componentes
convertir
entrada
modelo
entrenado
salida
caso
aplicaciones
NLP
general
comenzamos
convirtiendo
palabra
entrada
vector
utilizando
algoritmo
embedding
transformación
embeddings
ocurre
encoder
abajo
abstracción
común
encoders
reciben
lista
vectores
tamaño
512
encoder
inferior
serían
embeddings
palabras
salida
encoder
directamente
tamaño
lista
hiperparámetro
configurar
básicamente
longitud
oración
larga
conjunto
datos
entrenamiento
generar
embeddings
palabras
secuencia
entrada
fluye
capas
encoder
comenzamos
propiedad
clave
Transformer
palabra
posición
fluye
camino
encoder
dependencias
rutas
capa
autoatención
capa
feed-forward
dependencias
diversas
rutas
ejecutar
paralelo
fluye
capa
Auto-Atención
clase
habíamos
visto
mejoraba
performance
modelos
secuencia
secuencia
decoder
prestaba
atención
frase
entera
procesada
encoder
predecir
token
Recordemos
proceso
decoder
miraba
conjunto
ocultos
recibió
encoder
oculto
asociado
determinada
palabra
oración
entrada
asignaba
puntuación
oculto
calculada
función
atención
aditiva
producto
escalar
producto
escalar
generalizado
pasaba
valores
softmax
multiplicaba
oculto
puntaje
softmaxeado
amplificando
ocultos
puntajes
altos
diluyendo
ocultos
puntajes
bajos
sumaba
obteníamos
vector
contexto
proceso
hora
calcular
puntajes
tomábamos
oculto
actual
decoder
query
ocultos
tiempo
encoder
keys
values
hablar
auto-atención
queries
keys
values
provienen
lugar
tokens
entrada
medida
modelo
procesa
palabra
posición
secuencia
entrada
auto-atención
permite
observar
posiciones
secuencia
entrada
busca
pistas
puedan
ayudar
codificar
palabra
veamos
calcular
autoatención
usando
vectores
procedamos
implementa
realmente
usando
matrices
paso
calcular
autoatención
crear
vectores
vectores
entrada
encoder
caso
embedding
palabra
palabra
creamos
vector
query
vector
key
vector
value
vectores
crean
multiplicando
embedding
matrices
entrenamos
proceso
entrenamiento
vectores
pequeños
dimensión
vector
embedding
dimensionalidad
64
embeddings
vectores
entrada
salida
encoders
dimensionalidad
512
obligatorio
pequeños
elección
arquitectura
explicaremos
paso
calcular
autoatención
calcular
puntuación
Digamos
calculando
autoatención
palabra
ejemplo
Thinking
Necesitamos
puntuar
palabra
oración
entrada
palabra
puntuación
determina
atención
prestar
partes
oración
entrada
medida
codificamos
palabra
posición
determinada
puntuación
calcula
tomando
producto
escalar
vector
query
vector
key
palabra
respectiva
puntuando
procesando
autoatención
palabra
posición
n.
puntaje
producto
escalar
q1
k1
puntuación
producto
escalar
q1
k2
pasos
cuarto
consisten
dividir
puntuaciones
raíz
cuadrada
dimensión
vectores
key
utilizados
paper
64
gradientes
estables
valores
posibles
predeterminado
pase
resultado
operación
softmax
Softmax
normaliza
puntuaciones
comporten
distribución
probabilidad
positivas
sumen
puntuación
softmax
determina
enfocará
palabra
posición
Claramente
palabra
posición
puntuación
softmax
alta
útil
prestar
atención
palabra
relevante
palabra
actual
quinto
paso
multiplicar
vector
valor
puntuación
softmax
preparación
sumarlos
intuición
mantener
intactos
valores
palabras
centrarnos
diluir
palabras
irrelevantes
multiplicándolas
números
pequeños
0,001
ejemplo
sexto
paso
sumar
vectores
valores
ponderados
produce
salida
capa
autoatención
posición
palabra
Múltiples
Cabezales
paper
refina
capa
autoatención
agregar
mecanismo
llamado
atención
múltiples
cabezales
mejora
rendimiento
capa
atención
maneras
Expande
capacidad
modelo
enfocarse
posiciones
ejemplo
z1
contiene
posiciones
dominada
palabra
traduciendo
oración
animal
cruzó
calle
cansado
útil
refiere
animal
calle
capa
atención
múltiples
subespacios
representación
veremos
continuación
atención
cabezales
conjuntos
matrices
pesos
Query
Key
Value
Transformer
utiliza
cabezales
atención
terminamos
conjuntos
encoder
decoder
conjuntos
inicializa
aleatoriamente
entrenamiento
conjunto
proyectar
embeddings
entrada
vectores
salida
encoders
decoders
inferiores
subespacio
representación
cálculo
autoatención
describimos
anteriormente
matrices
peso
terminamos
matrices
deja
pequeño
desafío
capa
feed-forward
espera
matrices
espera
matriz
vector
palabra
necesitamos
forma
condensar
matriz
Concatenamos
matrices
multiplicamos
matriz
pesos
adicional
proceso
Auto-Atención
Múltiples
Cabezales
Pongamos
gráfico
pipeline
completo
Codificación
Posicional
cosa
falta
modelo
descrito
forma
codificar
orden
palabras
secuencia
entrada
RNNs
procesen
problema
secuencial
llegar
problema
abordar
Transformer
agrega
vector
embedding
entrada
vectores
siguen
patrón
específico
aprende
modelo
ayuda
determinar
posición
palabra
distancia
palabras
secuencia
intuición
agregar
valores
embeddings
proporciona
distancias
significativas
vectores
embeddings
proyectan
vectores
atención
producto
escalar
opciones
codificaciones
posicionales
aprendidas
fijas
trabajo
utilizaron
funciones
seno
coseno
frecuencias
pos
posición
dimensión
dimensión
codificación
posicional
corresponde
sinusoide
longitudes
onda
forman
progresión
geométrica
2π
10000⋅2π
autores
eligieron
función
intuían
permitiría
modelo
aprendiera
fácilmente
atender
posiciones
relativas
desplazamiento
fijo
representarse
función
lineal
imagen
muestra
ve
patron
Conexiones
Residuales
detalle
arquitectura
encoder
debemos
mencionar
continuar
subcapa
autoatención
feed-forward
encoder
conexión
residual
seguida
paso
normalización
capa
normalización
modelos
Reduce
tiempo
entrenamiento
evita
modelo
sesgue
características
valor
permite
pesos
exploten
lados
restringiéndolos
rango
deseable
entrenar
modelo
descenso
gradiente
características
normalizadas
normalización
capa
tomamos
media
desviación
estandar
características
oración
aplicamos
fórmula
aplica
subcapas
decoder
tuviéramos
pensar
transformer
encoders
decoders
apilados
vería
Etapa
Decodificación
cubierto
mayoría
conceptos
encoder
básicamente
funcionan
componentes
decoders
echemos
vistazo
funcionan
juntos
encoder
comienza
procesando
secuencia
entrada
salida
encoder
superior
transforma
conjunto
vectores
atención
V.
utilizados
decoder
capa
atención
encoder-decoder
ayuda
decoder
enfocarse
lugares
apropiados
secuencia
entrada
siguientes
pasos
repiten
proceso
alcanza
símbolo
especial
indica
decoder
transformer
completado
salida
salida
paso
alimenta
decoder
inferior
paso
tiempo
decoders
pasando
resultados
decodificación
encoders
hicimos
entradas
encoder
generamos
embedding
agregamos
codificación
posicional
entradas
decoder
indicar
posición
palabra
capas
autoatención
decoder
funcionan
ligeramente
encoder
decoder
capa
autoatención
atender
posiciones
anteriores
secuencia
salida
enmascarando
posiciones
futuras
configurándolas
-inf
paso
softmax
cálculo
autoatención
capa
Atención
encoder-decoder
funciona
autoatención
cabezales
explicamos
crea
matriz
queries
capa
toma
matriz
keys
values
salida
pila
encoder
Capa
Densa
Predicción
pila
decoder
genera
vector
punto
flotante
convertimos
palabra
trabajo
capa
densa
seguida
capa
Softmax
capa
transforma
vector
producido
pila
decoders
vector
llamado
vector
logits
Supongamos
modelo
conoce
10
000
palabras
únicas
inglés
vocabulario
salida
modelo
aprendió
conjunto
datos
entrenamiento
haría
vector
logits
tuviera
10
000
celdas
ancho
celda
correspondiente
puntuación
palabra
única
capa
softmax
convierte
puntajes
probabilidades
positivos
suman
1.0
elige
celda
probabilidad
alta
palabra
asociada
produce
salida
paso
tiempo
