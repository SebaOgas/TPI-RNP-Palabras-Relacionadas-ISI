Modelos
Generativos
Variational
AutoEncoders
VAEs
modelos
generativos
sencillos
intentan
maximizar
similitud
imágenes
generadas
imágenes
alimentadas
aprender
representar
valores
entrada
forma
compacta
muestreo
distribución
aprendida
capaz
generar
muestras
datos
Reducción
Dimensionalidad
aprendizaje
automático
reducción
dimensionalidad
proceso
reducir
cantidad
características
describen
datos
reducción
llevarse
cabo
selección
conservan
características
existentes
extracción
crea
número
reducido
características
basadas
características
viejas
útil
situaciones
requieren
datos
baja
dimensión
visualización
datos
almacenamiento
cómputos
pesados
etc.
métodos
reducción
dimensionalidad
establecer
marco
general
presente
mayoría
métodos
llamemos
encoder
proceso
produce
representación
características
representación
características
viejas
selección
extracción
decoder
proceso
inverso
reducción
dimensionalidad
interpretar
compresión
datos
encoder
comprime
datos
espacio
inicial
espacio
codificado
llamado
espacio
latente
decoder
descomprime
dependiendo
distribución
datos
inicial
dimensión
espacio
latente
definición
codificador
compresión
pérdida
significa
información
pierde
proceso
codificación
recuperar
decodificar
propósito
principal
método
reducción
dimensionalidad
encontrar
par
encoder
decoder
palabras
conjunto
posibles
encoders
decoders
buscando
par
mantiene
máxima
información
codificar
mínimo
error
reconstrucción
decodificar
denotamos
respectivamente
familias
encoders
decoders
considerando
escribir
problema
reducción
dimensionalidad
define
medida
error
reconstrucción
datos
entrada
datos
codificados-decodificados
AutoEncoders
Analicemos
AutoEncoders
veamos
redes
neuronales
reducir
dimensionalidad
idea
general
AutoEncoders
simple
consiste
establecer
Encoder
Decoder
redes
neuronales
aprender
esquema
codificación-decodificación
proceso
optimización
iterativo
iteración
alimentamos
Arquitectura
AutoEncoder
encoder
seguido
decoder
datos
comparamos
salida
datos
iniciales
función
pérdida
propagamos
error
arquitectura
actualizar
pesos
redes
backpropagation
intuitivamente
Arquitectura
AutoEncoder
crea
cuello
botella
datos
garantiza
importante
información
pasar
reconstruirse
Teniendo
marco
general
familia
encoders
considerados
definida
arquitectura
red
encoder
familia
decoders
considerados
definida
arquitectura
red
decoder
búsqueda
encoder
decoder
minimiza
error
reconstrucción
realiza
descenso
gradiente
parámetros
redes
Implementación
PyTorch
Creación
Encoder
Creación
Decoder
Creación
AutoEncoder
Entrenamiento
Utilidad
Práctica
AutoEncoders
usos
directos
práctica
Compresión
Imágenes
autoencoder
generar
representaciones
espacios
dimensionalidad
menor
imagen
original
método
eficiente
comprimir
imágenes
Eliminar
ruido
decoder
aprende
regenerar
imagen
entrada
representación
latente
aplicar
ruido
generado
automáticamente
imagen
calcular
representación
latente
decoder
aprenderá
reconstruir
imagen
original
ruido
función
pérdida
compara
imagen
generada
imagen
entrada
ruido
Reconstrucción
sectores
imágenes
capas
llamadas
Dropout
desactivan
neuronas
aleatorias
red
generar
efecto
pérdida
píxeles
imágenes
decoder
recuperar
función
pérdida
imagen
completa
original
Visualización
Generación
datos
punto
pregunta
natural
viene
mente
vínculo
Autoencoders
generación
contenido
Autoencoder
entrenado
Encoder
Decoder
forma
real
producir
contenido
vista
podríamos
sentir
tentación
pensar
espacio
latente
suficientemente
regular
organizado
Encoder
proceso
entrenamiento
podríamos
tomar
punto
azar
espacio
latente
decodificarlo
obtener
contenido
imágenes
espacio
latente
creado
autoencoder
irregular
generar
contenido
tomando
punto
espacio
regularidad
espera
espacio
latente
proceso
generativo
expresar
propiedades
principales
continuidad
puntos
cercanos
espacio
latente
deberían
contenidos
completamente
decodificados
completitud
distribución
elegida
punto
muestreado
espacio
latente
debería
contenido
significativo
decodificado
compleja
arquitectura
dimensiones
reducir
manteniendo
error
reconstrucción
Intuitivamente
Encoder
Decoder
suficientes
grados
libertad
reducir
dimensionalidad
inicial
encoder
potencia
infinita
tomar
teóricamente
dimensiones
iniciales
reducirlas
dimensiones
decoder
asociado
transformación
inversa
pérdida
proceso
Acá
debemos
cosas
reducción
importante
dimensionalidad
pérdida
reconstrucción
precio
falta
estructuras
interpretables
explotables
espacio
latente
falta
regularidad
lugar
mayoría
propósito
reducción
dimensionalidad
reducir
número
dimensiones
datos
reducir
número
dimensiones
mantiene
información
estructura
datos
representaciones
reducidas
razones
dimensión
espacio
latente
profundidad
autoencoders
definen
grado
calidad
compresión
controlarse
ajustarse
cuidadosamente
propósito
reducción
dimensionalidad
discutimos
sección
regularidad
espacio
latente
autoencoders
punto
difícil
depende
distribución
datos
espacio
inicial
dimensión
espacio
latente
arquitectura
encoder
difícil
imposible
asegurar
priori
codificador
organizará
espacio
latente
inteligente
compatible
proceso
generativo
acabamos
describir
ilustrar
punto
consideremos
ejemplo
dimos
anteriormente
describimos
encoder
decoder
suficientemente
potentes
conjunto
datos
entrenamiento
inicial
dimensiones
eje
real
punto
datos
codifica
valor
real
decodificarlos
pérdida
reconstrucción
caso
alto
grado
libertad
autoencoder
permite
codificar
decodificar
pérdida
información
baja
dimensionalidad
espacio
latente
conduce
sobreajuste
severo
implica
puntos
espacio
latente
darán
contenido
sentido
decodificado
ejemplo
unidimensional
elegido
voluntariamente
extremo
notar
problema
regularidad
espacio
latente
autoencoders
general
merece
atención
especial
pensarlo
minuto
falta
regularidad
datos
codificados
espacio
latente
normal
tarea
autoencoder
entrenado
fuerza
generar
regularidad
autoencoder
entrenado
únicamente
codificar
decodificar
menor
pérdida
importar
esté
organizado
espacio
latente
cuidado
definición
arquitectura
natural
entrenamiento
red
aproveche
posibilidades
sobreajuste
lograr
tarea
regularicemos
explícitamente
Definición
AutoEncoder
Variacional
decoder
autoencoder
fines
generativos
debemos
asegurarnos
espacio
latente
suficientemente
regular
solución
obtener
dicha
regularidad
introducir
regularización
explícita
proceso
entrenamiento
autoencoder
variacional
definir
autoencoder
cuyo
entrenamiento
regulariza
evitar
sobreajuste
garantizar
espacio
latente
propiedades
permitan
proceso
generativo
autoencoder
común
autoencoder
variacional
arquitectura
compuesta
encoder
decoder
entrenado
minimizar
error
reconstrucción
datos
codificados-decodificados
datos
iniciales
introducir
regularización
espacio
latente
procedemos
ligera
modificación
proceso
codificación-decodificación
lugar
codificar
entrada
punto
codificamos
distribución
espacio
latente
modelo
entrena
entrada
codifica
distribución
espacio
latente
toma
muestra
punto
espacio
latente
distribución
decodifica
punto
muestreado
calcular
error
reconstrucción
error
reconstrucción
retropropaga
red
simple
VAE
codifiquen
entradas
distribuciones
lugar
puntos
suficiente
garantizar
continuidad
completitud
término
regularización
definido
modelo
aprender
minimizar
error
reconstrucción
ignorar
devolver
distribuciones
comportan
autoencoders
clásicos
sobreajuste
encoder
devolver
distribuciones
pequeñas
variaciones
tenderían
distribuciones
puntuales
devolver
distribuciones
medias
estarían
separadas
espacio
latente
casos
distribuciones
incorrecta
cancelando
beneficio
esperado
satisface
continuidad
completitud
evitar
efectos
regularizar
matriz
covarianza
media
distribuciones
devueltas
encoder
práctica
regularización
realiza
cumplir
distribuciones
estén
cerca
distribución
normal
estándar
centrada
reducida
requerimos
matrices
covarianza
estén
cerca
identidad
evitando
distribuciones
puntuales
media
esté
cerca
evitando
distribuciones
codificadas
estén
separadas
función
pérdida
minimiza
entrenar
VAE
compone
término
reconstrucción
capa
tiende
esquema
codificación-decodificación
eficaz
término
regularización
capa
latente
tiende
regularizar
organización
espacio
latente
distribuciones
devueltas
codificador
aproximen
distribución
normal
estándar
término
regularización
expresa
divergencia
Kulback-Leibler
distribución
devuelta
Gaussiana
estándar
divergencia
Kulback-Leibler
distribuciones
cualquieras
define
distribuciones
continuas
reemplazamos
distribuciones
gaussianas
univariadas
divergencia
Kullback-Leibler
forma
cerrada
expresarse
directamente
términos
medias
matrices
covarianza
distribuciones
necesitaremos
datos
calcular
término
regularizador
término
regularización
evitamos
modelo
codifique
datos
separados
espacio
latente
fomentamos
medida
distribuciones
tiendan
superponerse
satisfaciendo
condiciones
continuidad
integridad
esperadas
Naturalmente
término
regularización
precio
error
reconstrucción
datos
entrenamiento
compensación
error
reconstrucción
divergencia
KL
ajustar
veremos
sección
expresión
equilibrio
emerge
naturalmente
derivación
formal
concluir
subsección
observar
continuidad
integridad
obtenidas
regularización
tienden
crear
gradiente
información
codificada
espacio
latente
ejemplo
punto
espacio
latente
estaría
camino
medios
distribuciones
codificadas
provienen
datos
entrenamiento
decodificarse
esté
lugar
datos
distribución
datos
distribución
Creación
Variational
AutoEncoder
Función
Pérdida
personalizada
regularización
explícita
Entrenamiento
Entrenar
modelo
Evaluación
modelo
continuación
utilizaremos
modelo
entrenado
verificar
conceptos
teóricos
explicados
anteriormente
Reconstrucción
entrada
evaluar
reconstrucción
autoencoder
evaluar
fiel
salida
entrada
original
Regularización
espacio
latente
graficaremos
espacio
latente
propiedades
deseadas
continuidad
completitud
alcanzadas
graficar
espacio
latente
establecimos
cantidad
dimensiones
reducir
dimensionalidad
afectar
calidad
salidas
alta
pérdida
información
graficar
espacio
latente
perder
calidad
modelo
utilizaremos
TSNE
algoritmo
aprende
forma
representar
2D
espacio
dimensionalidad
gráficas
TSNE
parecen
mostrar
grupos
grupos
visuales
verse
fuertemente
influenciados
parametrización
elegida
necesaria
comprensión
funcionamiento
interno
algoritmo
arrojar
conclusiones
gráficos
obtenidos
algoritmo
demostrar
tales
grupos
aparecen
datos
agrupados
hallazgos
falsos
imagen
clusteres
tienden
superponerse
dejar
huecos
declarar
logró
alcanzar
continuidad
completitud
Generación
Contenido
Dígitos
similares
ejemplo
dataset
utilizar
modelo
entrenado
generar
10
imágenes
decodificadas
muestras
latentes
Dígito
Individual
clase
deseada
distribución
clases
gracias
encoder
generar
dígito
artificial
clase
queramos
Interpolación
imágenes
tratar
entender
regularización
habíamos
llegado
conclusión
continuidad
completitud
causaba
efecto
gradiente
imágenes
generadas
causaba
efecto
puntos
localizados
distancia
medias
gaussianas
devolvían
imágenes
mezclaban
ambas
clases
observar
efecto
gradiente
utilizar
vectores
latentes
distribuciones
distintas
calcular
serie
vectores
latentes
intermedios
decodificarlos
mutación
