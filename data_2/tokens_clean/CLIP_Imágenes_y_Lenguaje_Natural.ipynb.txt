Modelo
CLIP
importancia
embeddings
multimodales
CLIP
significa
Contrastive
Language-Image
Pretraining
modelo
aprendizaje
profundo
desarrollado
OpenAI
2021
embeddings
CLIP
imágenes
texto
comparten
espacio
permite
comparaciones
directas
modalidades
logra
entrenando
modelo
acercar
imágenes
textos
relacionados
separa
relacionados
artículo
explicará
funciona
CLIP
guiará
ejemplo
entrenar
modelo
CLIP
utilizando
dataset
flicker30k
Problemas
enfoques
anteriores
Computer
Vision
empezar
entender
importancia
modelo
CLIP
centrarnos
entender
puntos
débiles
tambalear
modelos
computer
vision
preexistentes
modelos
poderosos
versátiles
limitaciones
notables
ejemplo
red
neuronal
convolucional
entrenada
clasificar
perros
gatos
enfrentar
dificultades
clasificar
imágenes
caballos
entrenada
red
fijada
tarea
inicial
debemos
modificarla
abordar
tareas
falta
flexibilidad
llevado
desarrollo
tendencia
importante
trabajar
modelos
preentrenados
conjuntos
datos
imágenes
variadas
finetuning
utilizar
modelo
preentrenado
aprovechan
patrones
aprendidos
previamente
tareas
diversas
ejemplo
necesita
clasificador
perros
gatos
caballos
simplemente
modifica
arquitectura
modelo
agregando
neurona
extra
capa
clasificación
enfoque
presenta
desafíos
requiere
conocimiento
arquitectura
original
modelo
preentrenado
necesitan
conjuntos
datos
masivos
etiquetados
manualmente
Etiquetar
imágenes
precisa
exhaustiva
complicado
especialmente
imagen
múltiples
interpretaciones
significa
etiquetas
detalladas
aumenta
complejidad
proceso
etiquetado
CLIP
innovadora
propuesta
desarrollada
OpenAI
aborda
desafíos
previamente
discutidos
logra
Combinando
potencia
modelos
visión
modelos
lenguaje
lugar
utilizar
enfoque
tradicional
supervisar
imágenes
etiquetas
CLIP
utiliza
supervisión
lenguaje
natural
Consta
modelos
visión
encargado
analizar
imágenes
lenguaje
procesa
texto
entrada
tarea
principal
CLIP
emparejar
imágenes
descripciones
aprendiendo
asociar
correctamente
imagen
descripción
adecuada
idea
aparentemente
simple
implicaciones
significativas
lugar
CLIP
requiere
imágenes
etiquetadas
manualmente
entrenarse
descripciones
lenguaje
natural
OpenAI
entrenado
sistema
250
millones
pares
imágenes
descripciones
obtenidas
Internet
amplía
enormemente
alcance
diversidad
conjunto
datos
forma
entrena
CLIP
crucial
aprender
asociar
correctamente
imágenes
descripciones
adecuadas
discernir
descripciones
apropiadas
imagen
dada
CLIP
CLIP
diseñado
predecir
pares
potenciales
imagen
texto
lote
realmente
asociados
lograr
CLIP
establece
espacio
embeddings
multimodal
entrenamiento
conjunto
codificador
imágenes
codificador
texto
función
pérdida
CLIP
objetivo
maximizar
similitud
coseno
embeddings
imagen
texto
pares
genuinos
lote
tiempo
minimizar
similitud
coseno
N²
pares
incorrectos
proceso
optimización
implica
función
pérdida
entropía
cruzada
simétrica
opera
puntuaciones
similitud
continuación
presenta
pseudocódigo
tomado
artículo
original
describe
implementación
principal
CLIP
image_encoder
ResNet
or
Vision
Transformer
text_encoder
Text
Transformer
I[n
minilote
imágenes
alineadas
T[n
minilote
descripciones
alineadas
Wi[di
d_e
proyección
aprendida
embedding
imagen
espacio
multimodal
Wt[dt
d_e
proyección
aprendida
embedding
descripción
espacio
multimodal
parámetro
temperatura
aprendido
extraer
representaciones
características
modalidad
If
imageencoder(I
d_i
Tf
textencoder(T
d_t
obtener
representaciones
multimodales
conjuntas
d_e
Ie
l2normalize(np.dot(If
Wi
axis=1
l2normalize(np.dot(Tf
Wt
axis=1
cálculo
similitudes
cosenos
pares
escaladas
logits
np.dot(Ie
np.exp(t
cálculo
función
pérdida
simétrica
labels
np.arange(n
lossi
crossentropy_loss(logits
labels
axis=0
losst
crossentropy_loss(logits
labels
axis=1
loss
lossi
losst)/2
siguientes
secciones
haremos
descripción
paso
paso
línea
pseudocódigo
implementación
usando
Pytorch
Descripción
General
Modelo
ClIP
utiliza
arquitecturas
separadas
columna
vertebral
codificar
conjuntos
datos
texto
visión
image_encoder
representa
arquitectura
red
neuronal
ejemplo
ResNet
Vision
Transformer
responsable
codificar
imágenes
text_encoder
representa
arquitectura
red
neuronal
ejemplo
CBOW
BERT
Text
Transformer
responsable
codificar
información
textual
modelo
CLIP
original
entrenó
cero
encoder
imágenes
encoder
texto
volumen
conjunto
datos
400
millones
pares
imagen-texto
utilizaron
ejemplo
tutorial
haremos
cosas
Comenzaremos
pesos
previamente
entrenados
modelos
resnet
imágenes
distilbert
texto
inicializar
módulos
Datos
Entrada
modelo
toma
entrada
lote
pares
imágenes
textos
I[n
representa
minilote
imágenes
alineadas
tamaño
lote
altura
imagen
ancho
imagen
número
canales
T[n
representa
minilote
textos
alineados
tamaño
lote
longitud
secuencia
textual
Extracción
Features
If
imageencoder(I
extrae
embeddings
If
encoder
imágenes
forma
If
di
di
dimensionalidad
embeddings
imagen
Tf
textencoder(T
extrae
embeddings
Tf
encoder
texto
forma
Tf
dt
dt
dimensionalidad
embeddings
texto
Proyecciones
Aprendidas
Wi[di
representa
matriz
proyección
aprendida
mapear
embeddings
imágenes
If
espacio
embedding
multimodal
Ie
forma
Wi
di
d_e
dimensionalidad
deseada
espacio
embedding
multimodal
Wt[dt
representa
matriz
proyección
aprendida
mapear
embeddings
texto
Tf
espacio
embedding
multimodal(Te
forma
Wt
dt
operación
proyección
implementar
utilizando
red
neuronal
capas
densas
cuyos
pesos
matriz
proyección
aprendida
mayoría
casos
pesos
matriz
proyección
únicos
pesos
gradientes
activos
proceso
entrenamiento
capa
proyección
juega
papel
crucial
hora
alinear
dimensiones
embeddings
imágenes
texto
asegurando
tengan
tamaño
Arquitectura
Encoders
código
ilustra
procesamiento
secuencial
datos
imagen
texto
Inicialmente
datos
procesan
encoder
base
seguido
capa
proyección
finalmente
generan
embeddings
normalizadas
ambas
modalidades
Función
Pérdida
logits
np.dot(I_e
T_e
np.exp(t
Calcula
similitudes
coseno
pares
embeddings
imagen
texto
escaladas
parámetro
temperatura
aprendido
t.
ejemplo
indistintamente
términos
similitud
logits
usó
artículo
original
incluiremos
parámetro
temperatura
clase
CLIP
utiliza
pérdida
contrastiva
acercar
embeddings
imágenes
textos
relacionados
aleja
relacionados
labels
np.arange(n
Genera
etiquetas
representan
índices
lote
lossi
crossentropy_loss(logits
etiquetas
axis=0
Calcula
pérdida
entropía
cruzada
eje
imagen
losst
crossentropy_loss(logits
etiquetas
axis=1
Calcula
pérdida
entropía
cruzada
eje
texto
loss
lossi
losst)/2
Calcula
promedio
simétrico
pérdidas
imagen
texto
Modelo
CLIP
personalizado
Combinando
piezas
modelo
CLIP
personalizado
Ejemplo
Entrenamiento
ejemplo
demuestra
proceso
creación
dataset
imágenes
descripciones
entrenamiento
modelo
CLIP
personalizado
objetivo
entrenar
encoder
visión
encoder
texto
conjuntamente
proyectar
representación
imágenes
descripciones
espacio
latente
embeddings
descripciones
estén
ubicadas
cerca
embeddings
imágenes
describen
Dataset
Dataloaders
modelo
CLIP
personalizado
entrenará
utilizando
conjunto
datos
flickr30k
dataset
comprende
31.000
imágenes
mínimo
descripciones
independientes
generadas
humanos
Usaremos
descripciones
imagen
ejemplo
62
000
pares
imágenes
texto
entrenamiento
constantes
clave
modelo
incluyen
embed_dim
representaciones
aprendidas
transformerembeddim
embeddings
transformer
max_len
longitud
máxima
entrada
texto
modelo
texto
elegido
distilbert-base-multilingual-cased
entrenamiento
abarca
épocas
tamaño
lote
128
constantes
incorporarán
construcción
entrenamiento
modelo
DataLoader
configurado
iteración
eficiente
entrenamiento
proporcionando
acceso
organizado
pares
imágenes
títulos
continuación
muestra
ejemplo
par
imagen-descripción
lotes
dataset
iniciamos
CustomModel
enviamos
dispositivo
CPU
GPU
especificamos
parámetros
optimizar
proceso
entrenamiento
congelado
capas
base
encoders
texto
imágen
parámetros
asociados
capa
proyección
entrenarán
conjunto
datos
Entrenamiento
Modelo
bucle
entrenamiento
máquina
GPU
Tesla
T4
g4dn-xlarge
épocas
entrenamiento
Recuerde
activar
GPU
colab
pestaña
Entorno
ejecución
Modelo
Preentrenado
principal
fortaleza
CLIP
capacidad
relacionar
texto
imágenes
robusta
especialmente
útil
tarea
requiera
entender
manipular
información
visual
textual
forma
integrada
aplicaciones
Búsqueda
recuperación
imágenes
basadas
texto
Image
Search
texto
descriptivo
modelo
buscar
recuperar
imágenes
coincidan
descripción
Text-to-Image
Retrieval
conjunto
imágenes
consulta
texto
modelo
identificar
imágenes
relevante
consulta
Búsqueda
recuperación
texto
basado
imágenes
Image
Captioning
Dada
imagen
modelo
generar
descripciones
texto
correspondan
imagen
Image-to-Text
Retrieval
conjunto
descripciones
textuales
imagen
modelo
identificar
descripciones
relevante
imagen
Clasificación
imágenes
Zero-Shot
Classification
CLIP
clasificación
imágenes
necesidad
entrenar
específicamente
tarea
necesita
proporcionar
etiquetas
clase
forma
texto
modelo
determinará
clase
probable
imagen
dada
Generación
contenido
Visual
Question
Answering
VQA
Responder
preguntas
contenido
imagen
Text-based
Image
Generation
Utilizando
modelos
DALL-E
basan
principios
similares
generar
imágenes
descripciones
textuales
Clasificación
Imágenes
Zero-Shot
CLIP
clasificación
imágenes
necesidad
entrenar
específicamente
tarea
necesita
proporcionar
etiquetas
clase
forma
texto
modelo
determinará
clase
probable
imagen
dada
Veamos
CLIP
personalizado
versión
oficial
OpenAI
Usando
modelo
entrenado
modelo
CLIP
generar
función
predict
preprocese
entradas
pase
modelo
clasificación
necesidad
fine
tuning
paso
paso
Preprocesamiento
imagen
python
img
flickr30kcustomdataset.transform(img).unsqueeze(0).to(device
imagen
img
transforma
utilizando
método
preprocesamiento
definido
dataset
flickr30kcustomdataset
imagen
transformada
redimensiona
unsqueeze(0
agregar
dimensión
lote
batch
convirtiéndola
tensor
dimensiones
Finalmente
mueve
device
especificada
ejemplo
GPU
Codificación
imagen
python
imageembed
model.visionencoder(img
imagen
preprocesada
pasa
codificador
visión
visionencoder
modelo
obteniendo
vector
embeddings
imagen
imageembed
Preparación
textos
clases
python
text
picture
of
for
in
classes
text
model.tokenizer(text).to(device
generan
frases
descriptivas
clase
lista
classes
formato
picture
of
clase
frases
tokenizan
usando
tokenizer
modelo
convierten
tensores
mueven
dispositivo
especificado
device
Codificación
textos
python
captionembed
model.captionencoder(text["input_ids
textos
tokenizados
pasan
codificador
captions
captionencoder
modelo
obteniendo
vector
embeddings
textos
captionembed
Cálculo
similitud
python
similarity
captionembed
imageembed
calcula
similitud
embeddings
captions
embedding
imagen
utilizando
producto
interno
operación
multiplicación
matrices
multiplica
embedding
textos
captionembed
transpuesta
embedding
imagen
imageembed
Aplicación
softmax
obtener
probabilidades
python
return
similarity.softmax(dim=0
Finalmente
aplica
función
softmax
similitudes
calculadas
dimensión
clases
convertirlas
probabilidades
resultado
tensor
valor
representa
probabilidad
imagen
corresponda
clases
dadas
resumen
función
toma
imagen
lista
clases
devuelve
tensor
probabilidades
indica
cuán
probable
imagen
pertenezca
clases
especificadas
Veamos
clasificando
perros
gatos
imagen
modelo
seguro
respuesta
ligeramente
inclinado
contestar
perro
significar
modelo
requiere
epochs
entrenamiento
Veamos
modelo
oficial
OpenAI
realiza
tarea
Usando
versión
oficial
CLIP
código
muestra
función
predict
equivalente
utiliza
versión
OpenAI
CLIP
viene
incluida
HuggingFace
haremos
directo
clases
preparadas
tarea
CLIPModel.from_pretrained("openai
clip-vit-base-patch32
Carga
modelo
CLIP
preentrenado
contiene
codificador
imágenes
codificador
texto
CLIPProcessor.from_pretrained("openai
clip-vit-base-patch32
Carga
procesador
asociado
encarga
transformar
imágenes
texto
formato
adecuado
modelo
CLIP
continuación
haremos
prueba
imagen
gatos
corroborar
desempeño
modelo
tarea
fenomenal
estableciendo
lugar
dudas
gato
preguntarse
resultado
superior
respuesta
viene
diferencia
tamaños
datasets
cantidad
épocas
realizadas
entrenamiento
modelos
escala
CLIP
generalmente
implica
entrenamientos
semanas
meses
clústeres
GPU
tiempo
modelos
pasan
miles
decenas
miles
épocas
optimizar
parámetros
aprender
representaciones
datos
alta
calidad
conjunto
datos
contiene
400
millones
pares
imágenes
textos
Conclusión
conclusión
clase
explorado
modelo
CLIP
descubierto
potencial
amplia
gama
aplicaciones
medida
entendemos
aplicaciones
CLIP
resulta
evidente
impacto
allá
expectativas
iniciales
allanando
camino
soluciones
innovadoras
diversos
campos
CLIP
modelo
exitoso
cerró
brecha
modalidades
abrió
vías
innovaciones
interdisciplinarias
