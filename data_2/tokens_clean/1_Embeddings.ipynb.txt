href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
NLP/3Embeddings/1Embeddings.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Embedding
palabras
word2vec
lenguaje
natural
sistema
complejo
utilizado
expresar
significados
sistema
palabras
unidad
básica
significado
nombre
indica
vectores
palabras
vectores
utilizan
representar
palabras
considerar
vectores
características
representaciones
palabras
técnica
mapear
palabras
vectores
reales
llama
Embedding
palabras
años
embedding
palabras
convertido
gradualmente
conocimiento
básico
procesamiento
lenguaje
natural
vectores
One-Hot
mala
elección
usado
vectores
one-hot
representar
palabras
Suponga
número
palabras
diccionario
tamaño
diccionario
palabra
corresponde
número
entero
índice
obtener
representación
vectorial
one-hot
palabra
índice
creamos
vector
longitud
establecemos
elemento
posición
palabra
representa
vector
longitud
utilizado
directamente
redes
neuronales
vectores
one-hot
fáciles
construir
general
opción
principales
razones
Ineficiencia
tamaño
vector
depende
tamaño
vocabulario
vocabularios
contienen
cientos
miles
palabras
vectores
ocupan
espacio
mayormente
compuestos
ceros
Ortogonalidad
par
vectores
one-hot
ortogonal
noción
natural
similaridad
Ejemplo
inspirador
Embedding
personalidad
escala
100
introvertido
extrovertido
introvertido
100
extrovertido
has
prueba
personalidad
prueba
Rasgos
Personalidad
has
pruebas
lista
preguntas
califican
serie
ejes
introversión
extraversión
Imaginemos
Jay
obtuvo
puntuación
38/100
rasgo
introversión
extraversión
graficar
Supongamos
contemplamos
rasgos
escalas
100
reemplazadas
escala
-1
obtendríamos
vector
represente
Jay
vector
representa
parcialmente
personalidad
Jay
utilidad
representación
viene
compararlo
personas
Digamos
atropella
autobús
necesita
reemplazado
alguien
personalidad
similar
figura
personas
Jay
persona
n°
parecida
Jay
gráficamente
vector
cerca
Jay
supongamos
vector
rasgos
análisis
gráfico
opción
calcular
similaridad
ángulo
vectores
calcula
utilizando
coseno
vimos
obtener
métrica
facilite
comparación
Seguimos
viendo
persona
parecida
n°
métrica
cuán
reemplazo
persona
ejercicios
asociados
clase
jugar
dataset
contiene
vectores
asociados
personas
reales
utilidad
práctica
concepto
Embedding
palabras
codificamos
significado
palabras
procesadas
modelos
aprendizaje
automático
necesitan
forma
representación
numérica
modelos
puedan
cálculos
entendemos
representación
vectores
proceder
observar
ejemplos
vectores
palabras
entrenados
llamados
embeddings
palabras
comenzar
observar
propiedades
interesantes
Word2Vec
algoritmo
demostrar
vector
representar
correctamente
palabras
capturara
relaciones
semánticas
significado
ejemplo
analizar
vectores
palabras
similares
opuestas
analizando
dirección
sentido
vector
par
palabras
Estocolmo
Suecia
relación
Cairo
Egipto
analizando
distancias
vectores
relaciones
sintácticas
basadas
gramática
ejemplo
relación
embedding
palabra
rey
vector
GloVe
entrenado
Wikipedia
lista
50
números
mirando
valores
visualicémoslo
compararlo
vectores
palabras
Pongamos
números
fila
codifiquemos
colores
celdas
valores
rojo
cerca
blanco
cerca
azul
cerca
-2
Procederemos
ignorando
números
observando
colores
indicar
valores
celdas
comparemos
Rey
palabras
cosas
señalar
columna
roja
recta
palabras
similares
dimensión
codifica
dimensión
Puedes
mujer
niña
similares
lugares
hombre
niño
niño
niña
lugares
similares
mujer
hombre
codificando
vaga
concepción
juventud
palabra
palabras
representan
personas
ejemplo
columna
azul
abajo
detiene
embedding
agua
lugares
claros
rey
reina
similares
distintos
codificando
concepto
vago
realeza
ejemplo
prueba
poderosos
embeddings
concepto
analogías
sumar
restar
embeddings
palabras
llegar
resultados
interesantes
ejemplo
famoso
fórmula
rey
hombre
mujer
vector
resultante
rey-hombre+mujer
exactamente
reina
reina
palabra
cercana
400
000
embeddings
palabras
colección
importante
quedar
contraste
embeddings
personalidad
analizamos
anteriormente
codifica
dimensión
vector
conjeturas
experimentos
permitan
validar
vectores
modelando
forma
semántica
codificación
datos
red
pareció
eficiente
entrenamiento
Semántica
Distribucional
semántica
distribucional
área
investigación
desarrolla
estudia
teorías
métodos
cuantificar
categorizar
similitudes
semánticas
elementos
lingüísticos
propiedades
distribucionales
muestras
datos
lingüísticos
idea
básica
semántica
distribucional
resumir
llamada
hipótesis
distribucional
elementos
lingüísticos
distribuciones
similares
significados
similares
hipótesis
distribucional
origina
teoría
semántica
lingüístico
palabras
aparecen
contextos
tienden
transmitir
significados
parecidos.​
idea
subyacente
dime
andas
diré
significas
Word2Vec
Entrenamiento
Auto-supervisado
semántica
distribucional
entrenar
modelos
lenguaje
ventaja
mayoría
modelos
aprendizaje
automático
ventaja
entrenarlos
texto
corriente
abundancia
información
necesitamos
freecuencia
palabras
suelen
aparecer
juntas
Piensen
libros
artículos
contenido
Wikipedia
formas
datos
texto
Comparen
modelos
aprendizaje
automático
necesitan
funciones
hechas
mano
datos
recopilados
especialmente
herramienta
word2vec
contiene
modelos
skip-gram
continuous
bag
of
words
CBOW
representaciones
semánticamente
significativas
entrenamiento
basa
probabilidades
condicionales
verse
predicción
palabras
usando
palabras
rodean
corpus
supervisión
proviene
datos
etiquetas
skip-gram
CBOW
modelos
autosupervisados
continuación
presentaremos
modelos
métodos
entrenamiento
Skipgram
modelo
skip-gram
asume
palabra
usarse
generar
palabras
rodean
secuencia
texto
probabilidad
condicional
generar
palabras
contexto
palabras
distancia
palabra
central
Suponga
palabras
contexto
generan
forma
independiente
dada
palabra
central
independencia
condicional
caso
probabilidad
condicional
reescribir
modelo
skip-gram
palabra
vectores
-dimensionales
asociados
calcular
probabilidades
condicionales
concretamente
palabra
índice
diccionario
denota
vectores
palabra
central
palabra
contexto
respectivamente
probabilidad
condicional
generar
palabra
contexto
índice
diccionario
dada
palabra
central
índice
diccionario
modelar
operación
softmax
producto
punto
vectores
define
conjunto
índices
vocabulario
Dada
secuencia
texto
longitud
palabra
paso
tiempo
denota
Suponga
palabras
contexto
generan
forma
independiente
dada
palabra
central
tamaño
ventana
contexto
función
probabilidad
modelo
skip-gram
probabilidad
generar
palabras
contexto
dada
palabra
central
omitir
paso
tiempo
inferior
superior
parámetros
modelo
skipgram
vectores
representan
palabras
centrales
contexto
objetivo
encontrar
representaciones
vectoriales
tratando
maximizar
función
verosimilitud
inglés
likelihood
transformar
función
función
pérdida
minimizar
aplicarle
logaritmo
cambiarle
signo
calcular
valor
promedio
transformándola
verosimilitud
logarítmica
negativa
media
calcular
función
objetivo
necesitamos
calcular
vectores
palabra
palabra
central
palabra
contexto
probabilidad
aparezca
palabra
contexto
dada
palabra
central
Continuous
Bag
of
Words
CBOW
modelo
continuous
bag
of
words
CBOW
similar
modelo
skip-gram
principal
diferencia
CBOW
supone
genera
palabra
central
función
palabras
contexto
rodean
secuencia
texto
múltiples
palabras
contexto
CBOW
vectores
palabras
contexto
promedian
cálculo
probabilidad
condicional
Específicamente
palabra
índice
diccionario
denota
vectores
palabra
contexto
palabra
central
respectivamente
significados
intercambian
modelo
skip-gram
probabilidad
condicional
generar
palabra
central
índice
diccionario
dadas
palabras
contexto
rodean
índice
diccionario
modelado
abreviar
conjunto
palabras
contexto
ventana
actual
vector
promedio
obtenido
palabras
ecuación
simplificar
Dada
secuencia
texto
longitud
palabra
paso
tiempo
denota
tamaño
ventana
contexto
función
verosimilitud
CBOW
probabilidad
generar
palabras
centrales
dadas
palabras
contexto
Comparación
Modelos
Muestreo
Negativo
operación
softmax
considerado
palabra
contexto
palabra
diccionario
función
pérdida
mencionada
anteriormente
incluye
realidad
suma
diccionario
diccionarios
cientos
miles
millones
palabras
sobrecarga
calcular
gradiente
alta
reducir
complejidad
computacional
presentaremos
método
entrenamiento
aproximados
continuación
muestreo
negativo
muestreo
negativo
consiste
transformar
problema
clasificación
plantea
skipgram
palabra
contexto
dada
central
problema
regresión
devuelva
probabilidad
palabras
vecinas
simple
cambio
transforma
regresión
softmax
usada
clasificación
multiclase
regresión
logística
usada
clasificación
binaria
vuelve
simple
rápido
calcular
cambio
requiere
cambiemos
estructura
conjunto
datos
etiqueta
columna
valores
palabras
agregamos
vecinas
calcular
velocidad
vertiginosa
procesando
millones
ejemplos
minutos
cabo
suelto
ejemplos
positivos
etiqueta
abrimos
posibilidad
modelo
sabelotodo
devuelve
logrando
100%
accuracy
aprender
generando
embeddings
basura
impedir
debemos
introducir
muestras
negativas
conjunto
datos
muestras
palabras
vecinas
modelo
necesita
devolver
muestras
desafío
modelo
trabajar
duro
resolver
velocidad
increíblemente
rápida
Llamemos
evento
palabra
contexto
provenga
ventana
contexto
palabra
central
evento
involucre
palabras
provengan
distribución
predefinida
muestreemos
palabras
negativas
ventana
contexto
Llamemos
evento
palabra
negativa
provenga
ventana
contexto
Suponga
eventos
involucran
ejemplo
positivo
ejemplos
negativos
mutuamente
independientes
muestreo
negativo
reescribe
probabilidad
conjunta
involucrando
ejemplos
positivos
probabilidad
condicional
aproxima
eventos
llamamos
índices
palabra
paso
tiempo
secuencia
texto
palabra
negativa
respectivamente
pérdida
logarítmica
probabilidades
condicionales
costo
computacional
gradientes
paso
entrenamiento
tamaño
diccionario
depende
linealmente
establecer
hiperparámetro
valor
menor
costo
computacional
gradientes
paso
entrenamiento
muestreo
negativo
menor
Entrenamiento
comienzo
fase
entrenamiento
creamos
matrices
matriz
embedding
matriz
contexto
matrices
embedding
palabra
central
contexto
respectivamente
palabra
vocabulario
matrices
funcionan
capas
transfroman
índices
token
vectores
ejmplo
entrenamiento
pasar
índices
palbras
centrales
contexto
respectivas
capas
obtenemos
embeddings
calcula
producto
punto
vector
palabra
central
vectores
contexto
negativos
obtener
medida
similares
producto
punto
proporcional
coseno
calcula
función
pérdida
habíamos
definido
sigmoidea
utiliza
modificar
parámetros
particularidad
entrenamiento
embeddings
embeddings
obtener
salida
parámetros
entrenando
caso
word2vec
embeddings
finales
obtendrán
matriz
palabras
centrales
matriz
contexto
descartada
GloVe
Embeddings
palabras
Vectores
Globales
co-ocurrencias
palabra-palabra
ventanas
contexto
información
semántica
rica
ejemplo
corpus
probable
palabra
sólido
coincida
hielo
vapor
palabra
gas
probablemente
coincida
vapor
frecuencia
hielo
estadísticas
corpus
global
tales
co-ocurrencias
precalcularse
conducir
entrenamiento
eficiente
aprovechar
información
estadística
corpus
embeddings
palabras
revisemos
modelo
skip-gram
interpretándolo
usando
estadísticas
globales
corpus
recuentos
co-ocurrencia
Razón
Probabilidades
Co-ocurrencia
probabilidad
condicional
generar
palabra
contexto
dada
palabra
central
corpus
tabla
enumera
probabilidades
co-ocurrencia
dadas
palabras
hielo
vapor
proporciones
basadas
estadísticas
corpus
|=|sólido|gas|agua|moda|
|:--|:-|:-|:-|:-|
||0.00019|0.000066|0.003|0.000017|
||0.000022|0.00078|0.0022|0.000018|
||8.9|0.085|1.36|0.96|
observar
palabra
relacionada
hielo
vapor
esperamos
proporción
probabilidades
co-ocurrencia
8.9
palabra
relacionada
vapor
hielo
esperamos
proporción
pequeña
probabilidades
co-ocurrencia
0.085
palabra
relacionada
hielo
vapor
esperamos
relación
probabilidades
co-ocurrencia
cercana
1.36
palabra
relacionada
hielo
vapor
esperamos
relación
probabilidades
co-ocurrencia
cercana
0.96
Función
Pérdida
relación
probabilidades
co-ocurrencia
expresar
intuitivamente
relación
palabras
diseñar
función
vectores
palabras
ajustar
razón
razón
probabilidades
co-ocurrencia
palabra
central
palabras
contexto
ajustar
proporción
usando
función
paper
GloVe
decide
exponencial
elijamos
constante
sacar
logaritmo
lados
obtenemos
términos
sesgo
adicionales
ajustar
palabra
central
bias
palabra
contexto
bias
gráfico
explica
desarrollo
matemático
fórmula
mínimos
cuadrados
ecuación
queda
función
pérdida
hiperparámetro
indica
coocurrencia
calculada
resulltado
función
pérdida
Aclaraciones
GloVe
particulares
entrenamiento
GloVe
sutiles
diferencias
word2vec
usemos
SGD
minilotes
xij
calculan
antemano
basados
dataset
completo
llaman
vectores
globales
Global
Vectors
Observe
palabra
aparece
ventana
contexto
palabra
palabra
aparecerá
ventana
contexto
palabra
embedding
palabra
central
embedding
palabra
contexto
palabra
deberían
equivalentes
GloVe
embeddings
aprendidos
palabra
valores
inicialización
aprender
embeddings
GloVe
utilizará
suma
vector
palabra
central
vector
palabra
contexto
embedding
palabra
fastText
Añadiendo
morfología
morfología
rama
lingüística
estudia
estructura
interna
palabras
definir
clasificar
unidades
variantes
palabras
formación
palabras
word2vec
GloVe
utilizan
morfología
generar
embeddings
ejemplo
perro
perros
representados
vectores
relación
vectores
representa
directamente
modelo
fastText
Bojanowski
et
2017
propone
método
embedding
subpalabras
intentando
introducir
información
morfológica
modelo
skip-gram
word2vec
fastText
palabra
central
representa
colección
ngramas
fastText
palabra
registramos
unión
subpalabras
longitud
subpalabras
especiales
diccionario
unión
conjunto
subpalabras
palabras
resto
fastText
modelo
skip-gram
comparación
modelo
skip-gram
vocabulario
fastText
resultado
parámetros
modelo
calcular
representación
palabra
vectores
subpalabras
sumarse
genera
complejidad
computacional
gracias
parámetros
compartidos
subpalabras
palabras
estructuras
similares
palabras
raras
palabras
vocabulario
obtener
mejores
representaciones
vectoriales
fastText
Codificación
Pares
Bytes
fastText
propuesta
utilizar
subpalabras
embeddings
permitió
obtener
mejores
vectores
palabras
complejas
comunes
palabras
diccionario
mirando
palabras
estructuras
similares
diseño
inocente
desventajas
diccionario
resulta
parámetros
modelos
complejos
vector
palabra
requiere
suma
vectores
subpalabras
resultado
complejidad
cálculo
n-gramas
longitud
fija
definirse
antemano
método
FastText
asume
implícitamente
n-grama
igualmente
importante
independientemente
contexto
realidad
caso
n-gramas
morfema
permitir
subpalabras
longitud
variable
vocabulario
tamaño
fijo
aplicar
algoritmo
compresión
llamado
codificación
pares
bytes
BPE
extraer
subpalabras
codificación
pares
bytes
realiza
análisis
estadístico
conjunto
datos
entrenamiento
descubrir
símbolos
comunes
palabra
caracteres
consecutivos
longitud
arbitraria
símbolos
longitud
codificación
pares
bytes
fusiona
iterativamente
par
frecuente
símbolos
consecutivos
producir
símbolos
largos
motivos
eficacia
pares
cruzan
límites
palabras
tales
símbolos
subpalabras
segmentar
palabras
codificación
pares
bytes
variantes
utilizado
representaciones
entrada
modelos
populares
preentrenamiento
procesamiento
lenguaje
natural
GPT-2
Radford
et
2019
RoBERTa
Liu
et
2019
continuación
ilustraremos
funciona
codificación
pares
bytes
Supongamos
dataset
compone
siguientes
frases
extraeremos
ngramas
longitud
calcularemos
frecuencia
detecta
ngrama
longitud
frecuente
fusionan
ngramas
longitud
componen
cantidad
letras
aparecen
juntas
resta
cantidad
aparecen
seguimos
siguientes
ngramas
longitud
frecuentes
fusionamos
formar
ngramas
longitud
sucesivamente
piensa
rato
seguramente
llegará
problema
método
establecemos
sistema
frenado
algoritmo
seguirá
ngrama
palabra
completa
Generalmente
utiliza
cantidad
iteraciones
algoritmo
evitar
problema
cantidad
hiperparámetro
depende
tamaño
vocabulario
