href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
NLP/3Embeddings/2Entrenamiento_Word2Vec.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Implementación
Entrenamiento
Word2Vec
conocemos
detalles
técnicos
modelos
word2vec
métodos
entrenamiento
aproximados
veamos
implementaciones
Específicamente
tomaremos
ejemplo
modelo
skip-gram
muestreo
negativo
Cargando
Dataset
extraer
información
Semántica
Distribucional
necesaria
entrenar
embeddings
Español
usaremos
libro
ingenioso
hidalgo
don
Quijote
Mancha
Cabe
aclarar
libro
usado
entiendan
salidas
pasos
proceso
práctica
necesitamos
Dataset
MUCHÍSIMO
generar
embeddings
coherentes
Wikipedia
entera
llevaría
tiempo
objetivo
clase
siguientes
celdas
código
descargan
libro
leen
archivo
listas
oraciones
generan
vocabulario
torchtext
hechos
clases
anteriores
Subsampling
datos
texto
suelen
palabras
alta
frecuencia
aparecer
miles
millones
corpus
palabras
coexisten
palabras
ventanas
contexto
proporcionando
señales
útiles
ejemplo
considere
palabra
chip
ventana
contexto
intuitivamente
coexistencia
palabra
baja
frecuencia
intel
útil
entrenamiento
coexistencia
palabra
alta
frecuencia
entrenamiento
cantidades
palabras
alta
frecuencia
lento
entrenar
modelos
embedding
palabras
alta
frecuencia
submuestrear
Específicamente
palabra
indexada
conjunto
datos
descartará
probabilidad
relación
cantidad
palabras
cantidad
palabras
dataset
constante
hiperparámetro
experimento
frecuencia
relativa
descartar
palabra
alta
frecuencia
frecuencia
relativa
palabra
probabilidad
descarte
celda
muestra
10
palabras
frecuentes
quijote
verán
palabras
relleno
encajar
oración
independientemente
significado
provee
información
semántia
distribucional
función
elimina
palabras
frecuentes
dataset
siguiendo
fórmula
propuesta
fragmento
código
traza
histograma
número
tokens
oración
submuestreo
esperar
submuestreo
acorta
significativamente
oraciones
eliminar
palabras
alta
frecuencia
acelerará
entrenamiento
fichas
individuales
frecuencia
muestreo
palabra
alta
frecuencia
inferior
1/20
contrario
palabras
baja
frecuencia
mesa
mantienen
completo
submuestreo
asignamos
índices
tokens
generamos
corpus
Extracción
palabras
centrales
palabras
contexto
función
getcentersandcontexts
extrae
palabras
centrales
palabras
contexto
corpus
Muestrea
uniformemente
número
entero
maxwindow_size
azar
tamaño
ventana
contexto
palabra
central
palabras
cuya
distancia
exceda
tamaño
ventana
contexto
muestreada
palabras
contexto
continuación
ilustrar
funcionamiento
creamos
conjunto
datos
artificial
contiene
oraciones
palabras
respectivamente
Haremos
tamaño
máximo
ventana
contexto
imprimiremos
palabras
centrales
palabras
contexto
ejercicios
estableceremos
tamaño
máximo
ventana
contexto
celda
extrae
palabras
centrales
palabras
contexto
Muestreo
negativo
muestreo
negativo
entrenamiento
aproximado
muestrear
palabras
negativas
distribución
predefinida
definimos
clase
RandomGenerator
distribución
muestreo
posiblemente
normalizada
pasa
argumento
sampling_weights
par
palabra
central
palabra
contexto
muestreamos
aleatoriamente
experimento
palabras
negativas
sugerencias
paper
word2vec
probabilidad
muestreo
palabra
negativa
establece
frecuencia
relativa
diccionario
elevada
potencia
0,75
Cargando
ejemplos
entrenamiento
minilotes
extraídas
palabras
centrales
palabras
contexto
palabras
negativas
muestreadas
transformaremos
datos
minilotes
ejemplos
cargar
forma
iterativa
entrenamiento
minilote
ejemplo
incluye
palabra
central
palabras
contexto
palabras
negativas
tamaños
ventana
contexto
varía
ejemplo
concatenamos
palabras
contexto
palabras
negativas
variable
contextsnegatives
rellenamos
ceros
longitud
concatenación
alcance
max_len
excluir
rellenos
cálculo
pérdida
definimos
variable
máscara
mask
correspondencia
elementos
mask
elementos
contextnegative
ceros
contrario
mask
corresponden
rellenos
contextnegative
distinguir
ejemplos
positivos
negativos
separamos
palabras
contexto
palabras
negativas
contextsnegatives
variable
labels
forma
similar
máscaras
correspondencia
elementos
etiquetas
elementos
contextosnegativos
contrario
ceros
etiquetas
corresponden
palabras
contexto
contexts_negatives
idea
implementa
función
collate_batch
datos
entrada
lista
longitud
tamaño
lote
elemento
ejemplo
consta
palabra
central
center
palabras
contexto
context
palabras
negativas
negative
función
devuelve
minilote
cargar
cálculos
entrenamiento
incluir
variable
máscara
Probemos
función
usando
mini
lote
ejemplos
Juntar
definimos
función
loaddataquijote
lee
quijote
devuelve
iterador
datos
vocabulario
Imprimamos
minilote
iterador
datos
Proceso
Entrenamiento
Implementamos
modelo
skip-gram
capas
Embedding
multiplicaciones
matrices
lotes
revisemos
funcionan
capas
Embedding
Embedding
Layer
capa
embedding
asigna
índice
token
vector
características
pesos
capas
conforman
matriz
cuyo
número
filas
tamaño
diccionario
input_dim
número
columnas
dimensión
vector
token
output_dim
entrenar
modelo
embedding
palabras
peso
convertiran
embedding
utilizaremos
representar
palabra
entrada
capa
Embedding
índice
token
palabra
índice
token
representación
vectorial
obtener
i-ésima
fila
matriz
pesos
capa
Embedding
dimensión
vector
output_dim
estableció
capa
Embedding
devuelve
vectores
forma
minilote
índices
token
forma
Parámetros
Modelo
comienzo
fase
entrenamiento
creamos
matrices
matriz
embedding
matriz
contexto
matrices
embedding
palabra
central
contexto
respectivamente
palabra
vocabulario
modelo
matrices
estarán
definidas
capas
Embedding
dimensión
vectores
100
Definición
función
forward
función
forward
entrada
modelo
skip-gram
incluye
center
índices
palabra
central
forma
tamaño
lote
contextsandnegatives
indices
concatenados
palabras
contexto
negativas
forma
tamaño
lote
max_len
variables
transforman
índices
token
vectores
capa
Embedding
calcula
producto
punto
vector
palabra
central
vectores
contexto
negativos
usando
multiplicación
matrices
lotes
elemento
salida
producto
escalar
vector
palabra
central
vector
palabra
contexto
negativa
Imprimamos
forma
salida
SkipGram
entradas
ejemplo
Entrenamiento
función
forward
proceso
entrenamiento
similar
modelos
vistos
anterioridad
calcula
función
pérdida
utiliza
modificar
parámetros
particularidad
entrenamiento
embeddings
embeddings
obtener
salida
parámetros
entrenando
Función
pérdida
definición
función
pérdida
muestreo
negativo
presentamos
anteriormente
usaremos
entropía
cruzada
binaria
conocida
sigmoidea
probamos
datos
inventados
Vemos
resultados
generar
usando
función
sigmoidea
eficiente
considerar
salidas
pérdidas
normalizadas
promedian
predicciones
enmascaradas
Definición
ciclo
entrenamiento
ciclo
entrenamiento
define
continuación
existencia
relleno
cálculo
función
pérdida
ligeramente
comparación
funciones
entrenamiento
anteriores
entrenar
modelo
skip-gram
usando
muestreo
negativo
Aplicación
Embeddings
Palabras
entrenar
modelo
word2vec
similitud
coseno
vectores
palabras
modelo
entrenado
encontrar
palabras
diccionario
similares
semánticamente
palabra
entrada
Recordemos
entrenado
dataset
pequeño
resultados
deberían
