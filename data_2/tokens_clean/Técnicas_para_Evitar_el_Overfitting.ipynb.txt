href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/5EvaluacionModelos/3TecnicasOverfitting.ipynb
target="_parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
Técnicas
Evitar
Overfitting
Regularización
pesos
caracterizado
problema
sobreajuste
introducir
técnicas
estándar
regularizar
modelos
Recuerde
mitigar
sobreajuste
saliendo
recopilando
datos
entrenamiento
costoso
tiempo
completamente
control
haciéndolo
imposible
corto
plazo
asumir
tantos
datos
alta
calidad
recursos
permitan
centrarnos
técnicas
regularización
Recuerde
ejemplo
regresión
polinomial
podríamos
limitar
capacidad
modelo
simplemente
modificando
grado
polinomio
ajustado
limitar
número
características
técnica
popular
mitigar
sobreajuste
simplemente
dejar
características
medida
drástica
Siguiendo
ejemplo
regresión
polinomial
considere
suceder
entradas
alta
dimensión
extensiones
naturales
polinomios
datos
multivariados
denominan
monomios
simplemente
productos
potencias
variables
grado
monomio
suma
potencias
ejemplo
monomios
grado
número
términos
grado
aumenta
rápidamente
medida
crece
Dadas
variables
número
monomios
grado
multiselección
pequeños
cambios
grado
digamos
aumentan
drásticamente
complejidad
modelo
necesitamos
herramienta
fina
ajustar
complejidad
función
Normas
operadores
útiles
álgebra
lineal
normas
Informalmente
norma
vector
cuán
vector
noción
tamaño
refiere
dimensionalidad
magnitud
componentes
notar
normas
parecen
medidas
distancia
distancia
euclidiana
norma
concreto
norma
Supongamos
elementos
vector
-dimensional
norma
raíz
cuadrada
suma
cuadrados
elementos
vector
subíndice
omite
normas
equivalente
codigo
calcular
norma
vector
aprendizaje
profundo
trabajamos
norma
cuadrado
encontrará
frecuencia
norma
expresa
suma
valores
absolutos
elementos
vector
comparación
norma
influenciada
valores
atípicos
calcular
norma
componemos
función
valor
absoluto
suma
elementos
norma
norma
casos
especiales
norma
general
análoga
norma
vectores
norma
Frobenius
matriz
raíz
cuadrada
suma
cuadrados
elementos
matriz
norma
Frobenius
satisface
propiedades
normas
vectoriales
comporta
norma
vector
forma
matriz
Invocar
función
calculará
norma
Frobenius
matriz
Regularizacion
Weight
Decay
regularización
técnica
utilizada
regularizar
modelos
aprendizaje
automático
paramétrico
técnica
motivada
intuición
básica
funciones
función
asigna
valor
entradas
sentido
simple
medir
complejidad
función
distancia
cero
precisión
debemos
medir
distancia
función
cero
respuesta
correcta
ramas
enteras
matemáticas
incluidas
partes
análisis
funcional
teoría
espacios
Banach
dedicadas
responder
problema
interpretación
simple
medir
complejidad
función
lineal
norma
vector
peso
ejemplo
método
común
asegurar
vector
peso
pequeño
agregar
norma
término
penalización
problema
minimizar
pérdida
reemplazamos
objetivo
original
minimizar
pérdida
predicción
etiquetas
entrenamiento
objetivo
minimizar
suma
pérdida
predicción
término
penalización
vector
peso
crece
algoritmo
aprendizaje
enfocarse
minimizar
norma
peso
vs
minimizar
error
entrenamiento
exactamente
ilustrar
cosas
código
revivamos
ejemplo
clase
regresión
lineal
pérdida
dada
Recuerde
características
etiquetas
ejemplos
datos
parámetros
peso
sesgo
respectivamente
penalizar
tamaño
vector
peso
debemos
agregar
función
pérdida
debería
modelo
compensar
pérdida
estándar
penalización
aditiva
práctica
caracterizamos
compensación
constante
regularización
hiperparámetro
negativo
ajustamos
usando
datos
validación
recuperamos
función
pérdida
original
restringimos
tamaño
Dividimos
convención
tomamos
derivada
función
cuadrática
cancelan
asegurando
expresión
actualización
vea
simple
lector
astuto
preguntarse
trabajamos
norma
cuadrado
norma
estándar
distancia
euclidiana
conveniencia
computacional
elevar
cuadrado
norma
eliminamos
raíz
cuadrada
dejando
suma
cuadrados
componente
vector
peso
derivada
penalización
fácil
calcular
suma
derivadas
derivada
suma
efecto
logrado
ve
gráfico
ejemplo
pesos
w1
w2
ejes
función
pérdida
graficada
curvas
nivel
círculo
celeste
representa
restricción
establecida
pesos
regularización
l2
circulito
amarillo
combinación
pesos
elegida
modelo
observar
modelo
acerca
mínimo
función
respetando
restricción
tamaño
preguntarse
trabajamos
norma
lugar
digamos
norma
razón
trabajar
norma
impone
penalización
descomunal
pesos
vector
sesga
algoritmo
aprendizaje
modelos
distribuyen
peso
uniforme
cantidad
features
práctica
hacerlos
robustos
error
medición
variable
contrario
penalizaciones
conducen
modelos
concentran
pesos
pequeño
conjunto
características
disminuyendo
pesos
cero
llama
selección
características
deseable
razones
Regresión
lineal
alta
dimensión
ilustrar
beneficios
pérdida
peso
ejemplo
sintético
simple
generamos
datos
Elegimos
etiqueta
función
lineal
entradas
alterada
ruido
gaussiano
media
cero
desviación
estándar
0,01
efectos
sobreajuste
pronunciados
aumentar
dimensionalidad
problema
trabajar
pequeño
conjunto
entrenamiento
contiene
20
ejemplos
Reutilizaremos
funciones
clase
regresión
lineal
Implementación
cero
continuación
implementaremos
regularización
pesos
cero
simplemente
agregando
penalización
cuadrado
función
objetivo
original
Definiendo
modelo
definiremos
modelo
regresión
lineal
inicializaremos
aleatoriamente
parámetros
Nótese
agregamos
método
loss
calcula
error
cuadrático
Definiendo
Norma
Penalización
forma
conveniente
implementar
penalización
elevar
cuadrado
términos
sumarlos
Modelo
Regularizacion
implementar
modelo
calcule
pérdida
regularizando
pesos
l2_penalty
definiremos
modelo
subclase
definimos
redefina
función
pérdida
Definición
ciclo
entrenamiento
código
ajusta
modelo
conjunto
entrenamiento
evalúa
conjunto
prueba
Entrenando
Regularización
correr
función
train
definida
constante
regularización
osea
regularización
llevará
cabo
Nótese
produciendo
sobreajuste
error
entrenamiento
decreciendo
error
testeo
mantiene
Entrenando
Regularización
constante
regularización
distinta
Nótese
error
testeo
disminuyendo
entrenamiento
descienda
bruscamente
precisamente
efecto
esperamos
regularización
Implementación
concisa
regularización
pesos
omnipresente
optimización
redes
neuronales
Pytorch
especialmente
conveniente
integra
algoritmo
optimización
facilitar
combinación
función
pérdida
código
especificamos
hiperparámetro
regularización
pesos
directamente
weightdecay
instanciar
optimizador
forma
predeterminada
PyTorch
regulariza
pesos
bias
simultáneamente
establecemos
weightdecay
peso
parámetro
bias
regularizará
gráficas
ven
idénticas
implementamos
regularización
pesos
cero
funcionan
considerablemente
rápido
fáciles
implementar
beneficio
pronunciado
problemas
Robustez
perturbaciones
noción
útil
simplicidad
suavidad
función
sensible
pequeños
cambios
entradas
ejemplo
clasificamos
imágenes
esperaríamos
agregar
ruido
aleatorio
píxeles
mayoría
inofensivo
entrena
red
profunda
capas
inyectar
ruido
entradas
impone
suavidad
mapeo
entrada
salida
llevamos
idea
allá
inyectamos
ruido
capa
red
calcular
capa
subsiguiente
entrenamiento
idea
llamada
dropout
consiste
inyectar
ruido
calcula
capa
interna
forward
convertido
técnica
estándar
entrenar
redes
neuronales
método
llama
dropout
abandono
literalmente
abandona
neuronas
entrenamiento
entrenamiento
iteración
dropout
estándar
consiste
cero
fracción
neuronas
capa
calcular
capa
desafío
clave
inyectar
ruido
idea
inyectar
ruido
imparcial
valor
esperado
capa
dejan
fijas
valor
habría
tomado
ruido
regularización
dropout
estándar
elimina
sesgo
capa
normalización
fracción
nodos
retuvieron
abandonaron
palabras
probabilidad
abandono
activación
intermedia
reemplaza
variable
aleatoria
diseño
esperanza
permanece
cambios
Dropout
práctica
MLP
capa
oculta
unidades
ocultas
ve
figura
izquierdo
transforma
derecho
aplicamos
dropout
dicha
capa
poniendo
cero
unidad
oculta
probabilidad
resultado
verse
red
contiene
subconjunto
neuronas
originales
figura
eliminan
consecuencia
cálculo
salidas
depende
gradiente
respectivo
desvanece
backpropagation
cálculo
capa
salida
depender
ningúnun
elemento
general
deshabilitamos
abandono
momento
prueba
modelo
entrenado
ejemplo
eliminamos
nodo
necesitamos
normalizar
Implementación
cero
implementar
función
dropout
capa
debemos
extraer
tantas
muestras
variable
aleatoria
Bernoulli
binaria
dimensiones
capa
variable
aleatoria
toma
valor
mantener
probabilidad
abandonar
probabilidad
forma
sencilla
implementar
extraer
muestras
distribución
uniforme
mantener
nodos
muestra
correspondiente
descartando
resto
código
implementamos
función
dropout_layer
descarta
elementos
entrada
tensor
probabilidad
dropout
reescalando
resto
describe
dividiendo
sobrevivientes
1.0-dropout
probar
función
dropout_layer
ejemplos
siguientes
líneas
código
pasamos
entrada
capa
dropout
probabilidades
0,5
respectivamente
Definición
parámetros
modelo
Nuevamente
trabajamos
conjunto
datos
Fashion-MNIST
Definimos
MLP
capas
ocultas
contienen
1024
unidades
Definición
modelo
modelo
aplica
dropout
salida
capa
oculta
siguiendo
función
activación
establecer
probabilidades
dropout
capa
separado
tendencia
común
establecer
probabilidad
dropout
baja
cerca
capa
entrada
continuación
establecemos
0.2
0.5
capa
oculta
respectivamente
aseguramos
dropout
esté
activo
entrenamiento
Entrenamiento
Pruebas
similar
entrenamiento
prueba
MLP
descritos
anteriormente
Implementación
concisa
API
alto
nivel
agregar
capa
Dropout
capa
completamente
conectada
pasando
probabilidad
dropout
único
argumento
constructor
entrenamiento
capa
Dropout
eliminará
aleatoriamente
salidas
capa
equivalente
entradas
capa
posterior
probabilidad
dropout
especificada
entrenamiento
capa
Dropout
simplemente
pasa
datos
prueba
continuación
entrenamos
probamos
modelo
