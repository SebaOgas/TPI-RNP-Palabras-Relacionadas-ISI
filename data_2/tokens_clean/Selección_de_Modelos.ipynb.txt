href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/5EvaluacionModelos/2SeleccionModelos.ipynb
target="_parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
Selección
modelos
aprendizaje
automático
generalmente
seleccionamos
modelo
evaluar
modelos
candidatos
proceso
llama
selección
modelo
modelos
sujetos
comparación
naturaleza
fundamentalmente
ejemplo
árboles
decisión
frente
modelos
lineales
ocasiones
comparando
miembros
clase
modelos
entrenados
configuraciones
hiperparámetros
MLP
ejemplo
deseemos
comparar
modelos
números
capas
ocultas
números
unidades
ocultas
opciones
funciones
activación
aplicadas
capa
oculta
determinar
modelos
candidatos
generalmente
emplearemos
conjunto
datos
validación
Conjunto
datos
validación
principio
deberíamos
tocar
conjunto
prueba
hayamos
elegido
hiperparámetros
utilizáramos
datos
prueba
proceso
selección
modelo
riesgo
podamos
sobreajustar
datos
prueba
estaríamos
serios
problemas
sobreajustamos
datos
entrenamiento
evaluación
datos
prueba
mantenernos
honestos
sobreajustamos
datos
prueba
sabríamos
debemos
confiar
datos
prueba
selección
modelo
confiar
únicamente
datos
entrenamiento
selección
modelo
estimar
error
generalización
datos
entrenar
modelo
aplicaciones
prácticas
imagen
vuelve
turbia
idealmente
tocaríamos
datos
prueba
evaluar
modelo
comparar
pequeña
cantidad
modelos
datos
prueba
mundo
real
rara
descartan
Rara
permitirnos
conjunto
prueba
ronda
experimentos
práctica
común
abordar
problema
dividir
datos
maneras
incorporando
conjunto
datos
validación
conjunto
validación
conjuntos
datos
entrenamiento
prueba
ejemplo
distinguir
conjunto
prueba
validación
plataforma
Kaggle
competencias
aprendizaje
automático
inicios
Kaggle
plataforma
concursos
empresas
publican
problemas
participantes
compiten
construir
algoritmo
generalmente
premios
efectivo
organización
elos
concursos
consiste
organizador
separar
dataset
conjunto
entrenamiento
publicado
conjunto
prueba
cuyas
features
publicadas
etiquetas
permanecerán
ocultas
participantes
descargar
datos
entrenamiento
deberán
elegir
modelo
presentar
competencia
deberán
selección
modelos
generando
conjunto
validación
datos
entrenamiento
seleccionado
modelo
funcione
datos
validación
alimenta
modelo
features
conjunto
prueba
obtener
etiquetas
prueba
predichas
modelo
entregan
etiquetas
prueba
predichas
organizador
compara
reales
ganador
modelo
erroes
cometido
conjuntos
prueba
validación
diferenciados
elegir
modelo
evaluar
modelo
elegido
datos
vio
entrenamiento
indique
explícitamente
contrario
experimentos
curso
realidad
trabajando
correctamente
debería
llamarse
datos
entrenamiento
datos
validación
verdaderos
conjuntos
prueba
reportado
experimento
realmente
accuracy
validación
accuracy
conjunto
pruebas
-fold
cross-validation
datos
entrenamiento
escasos
siquiera
podamos
permitirnos
mantener
suficientes
datos
constituir
conjunto
validación
adecuado
solución
popular
problema
emplear
-fold
cross-validation
datos
entrenamiento
originales
dividen
subconjuntos
superponen
entrenamiento
validación
modelo
ejecutan
entrenando
subconjuntos
validando
subconjunto
usó
entrenar
ronda
Finalmente
errores
entrenamiento
validación
estiman
promediando
resultados
experimentos
Model
Definamos
red
neuronal
simple
conjunto
datos
MNIST
Función
reiniciar
pesos
Necesitamos
restablecer
pesos
modelo
fold
cross
validation
comience
inicial
aleatorio
aprenda
folds
anteriores
llamar
reset_weights
módulos
hijos
Modificamos
ligeramente
pipelines
entrenamiento
ordenado
lineas
calcular
pérdida
mejorar
parámetros
ponemos
función
train
encargan
calcular
accuracy
función
test
Dataset
Necesitamos
concatenar
partes
entrenamiento
prueba
dataset
MNIST
usaremos
entrenar
modelo
K-fold
implica
generemos
divisiones
PyTorch
haga
Clase
KFold
KFold
clase
librería
sklearn
ayudar
cross
validation
debemos
instanciar
objeto
kfold
indicando
cantidad
folds
atributo
n_splits
constructor
clase
KFold
método
llamado
split
iterator
recibe
dataset
separar
devuelve
tupla
listas
índices
lista
índices
entrenamiento
lista
índices
testeo
fold
generar
folds
entrenar
modelo
definiendo
loop
itere
folds
especificando
lista
identificadores
ejemplos
entrenamiento
validación
fold
particular
loop
print
id
fold
entrenamos
muestreando
elementos
train
test
SubsetRandomSampler
clase
pasar
lista
índices
elementos
muestrear
dataset
