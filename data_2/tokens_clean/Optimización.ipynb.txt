href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/6Optimizadores/1Optimizadores.ipynb
target="_parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
Algoritmos
optimización
figuras
anteriores
recorrido
parametros
modelo
aplicar
optimizador
casos
mínimo
figuras
anteriores
muestran
limitaciones
SGD
optimizador
trabajar
tasa
aprendizaje
fija
encontremos
situación
convergencia
dirección
rápida
izquierda
caso
dirección
parametros
acercan
minimo
dirección
parametro
diverge
derecha
problema
convergencia
rápida
convergencia
lenta
desconocemos
características
funciones
pérdida
debemos
algun
optimizador
robusto
permita
evitar
tipo
problemas
tolerar
bajas
tasas
convergencia
Solución
Tasa
aprendizaje
dependiente
tiempo
corresponde
llaman
schedulers
planificadores
época
modifica
valor
tasa
aprendizaje
empenzando
valores
altos
cayendo
valores
bajos
página
muestran
planificadores
existentes
muestra
general
planificadores
acplicarse
ciclo
entrenaminto
model
Parameter(torch.randn(2
requires_grad
True
optimizer
SGD(model
0.1
scheduler
ExponentialLR(optimizer
gamma=0.9
for
epoch
in
range(20
for
input
target
in
dataset
optimizer.zero_grad
output
model(input
loss
loss_fn(output
target
loss.backward
optimizer.step
scheduler.step
entrenamiento
debemos
actualizar
existentes
planificadores
decaimiento
exponencial
disminución
lineal
función
coseno
posibilidad
agregar
función
arbitraria
Solución
Metodos
acelerados
Momentum
alternativa
consiste
datos
función
pérdida
constantemente
calculando
gradientes
valores
parámetros
información
gradientes
servirnos
mejorar
convergencia
optimizador
trucos
optimizador
llamado
momentum
nombre
inspirado
momentos
conjugados
mecánica
clásica
perdida
generalidad
considerar
velocidad
movil
momento
conjugado
optimizador
pretende
imitar
pequeña
pelota
cae
recipiente
concavo
punto
optimizador
gradiente
calculado
parametro
optimizar
tasa
aprendizaje
agregamos
variables
hiperparámetro
paso
actualizamos
resultado
gradiente
termina
actuando
fuerza
ficticia
partícula
variable
velocidad
ficticia
viscosidad
fluido
punto
vista
estadístico
media
movil
exponencial
convergencia
acelere
limitación
observado
terminar
volviendo
problema
original
problema
divergencia
rápido
Solución
función
minimizar
problemas
curvatura
función
varía
distinta
distintos
ejes
figura
izqueirda
genera
problema
teníamos
dirección
adelante-atras
función
curvada
dirección
izquierda-derecha
curvatura
menor
generaba
divergencia
tasa
aprendizaje
Adagrad
función
izquierda
curvatura
dirección
dirección
diverge
caso
estuvieramos
caso
podríamos
salvar
problema
solución
natural
calcular
hessiano
función
general
costoso
computacionalmente
alternativa
notar
hessiano
curvatura
relacionados
variabilidad
gradiente
varianza
desviación
estandar
gradiente
aproximación
hessiano
valor
ventaja
calcularla
costosa
hessiano
cantidad
gráfico
sugiere
algun
tipo
expansión
contracción
permita
pasar
situación
curvaturas
distintas
curvaturas
homogeneas
conoce
precondicionar
matriz
técnica
resolución
ecuaciones
lineales
acelerar
convergencia
problema
algoritmo
ida
conoce
Adagrad
gradiente
calculado
parametro
optimizar
tasa
aprendizaje
agregamos
variables
paso
actualizamos
hiperparámetro
típicamente
evitar
dividir
observa
especie
varianza
gradiente
varianza
curvatura
tendra
función
pérdida
menor
varianza
menor
curvatura
dividimos
raiz
cuadrada
precondicionar
matriz
problemas
Adagrad
tasa
aprendizaje
queda
acomplada
gradiente
intentos
Adagrad
planificador
traigan
problemas
medida
épocas
pasan
planificador
disminuye
tasa
aprendizaje
dismunuye
factor
terminar
compensando
disminución
superarla
optimizadores
intentan
arreglar
problema
discutirlos
señalaremos
Torch
implementado
Adagrad
llamado
usando
RMSProp
soluciones
propone
optimizador
RMSProp
utilizar
media
movil
calcular
desviación
estandar
lugar
desvisión
estandar
convencional
crecer
ilimitada
gradiente
calculado
parametro
optimizar
tasa
aprendizaje
agregamos
variables
hiperparámetro
paso
actualización
acualizamos
factor
clave
asegura
crezca
ilimitada
par
desacopla
gradiente
tasa
aprendizaje
Torch
implementado
RMSProp
llamado
usando
Adadelta
solución
propuesta
eliminar
tasa
aprendizaje
actualizarla
usando
información
gradiente
gradiente
calculado
parametro
optimizar
agregamos
variables
hiperparámetro
paso
actualización
acualizamos
tasa
aprendizaje
información
varian
parametros
Decimos
especie
varianza
modificamos
parametros
Torch
implementado
Adadelta
llamado
usando
Adam
Adam
algortimos
optimización
intenta
mayoría
ideas
anteriores
sentido
propuesta
Adam
gradiente
calculado
parametro
optimizar
tasa
aprendizaje
agregamos
variables
hiperparámetros
general
vemos
Adam
media
movil
exponencial
acelerar
convergencia
varianza
movil
exponencial
precondicionador
Finalmente
variables
usadas
evitar
sesgos
estadísticos
cálculo
media
varianza
Yogi
visto
casos
Adam
fallar
variante
consiste
reemplazar
actualización
forma
calcular
llamada
Yogi
Torch
implementado
Adam
llamado
usando
