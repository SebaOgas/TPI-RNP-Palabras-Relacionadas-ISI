href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
CV/2Convoluciones
CapasConvolucionales.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Ejemplo
Motivacional
Supongamos
generar
sistema
reconozca
números
escritos
mano
forma
contar
cantidad
formas
simples
número
verificar
número
corresponde
conteo
tabla
número
simplemente
deberemos
contar
cantidad
formas
aparecen
predecir
valor
problema
método
determinar
forma
presente
imagen
pensamos
imágenes
matrices
celda
matriz
representa
intensidad
píxel
representa
negro
255
representa
píxel
blanco
puro
descubrir
patrón
muestra
imagen
caso
escrito
mano
usaremos
especie
scanner
aprendizaje
automático
scanners
llaman
filtros
filtro
utiliza
cálculo
convolución
matrices
clásico
operación
dará
resultado
número
alto
patrón
definido
filtro
sector
imagen
escaneado
dará
número
figura
imaginemos
matriz
verde
imagen
cuadrado
negro
centro
ceros
pixeles
negros
rodeado
borde
blanco
pixeles
blancos)y
borde
negro
matriz
amarilla
representa
filtro
cuadrado
blanco
3x3
escanear
imagen
filtro
tirará
pixel
valor
dirá
sector
3x3
cuadrado
blanco
escaneado
imagen
completa
obtendremos
resultado
pasa
esquinas
necesitamos
Padding
relleno
padding
prácticamente
extiende
matriz
atender
valores
borde
describe
imagen
continuación
capa
rosa
forma
matriz
originals
ayuda
convolución
ejemplo
relleno
toma
resultado
interpretar
salida
filtro
imagen
valores
blancos
indican
sectores
parecidos
patrón
filtro
hicimos
filtro
detecte
cuadrado9s
blancos
diseñar
filtros
detecten
formas
necesitamos
reconocer
números
forma
podríamos
entrenar
red
aprenda
filtros
necesita
detectar
reconocer
números
lugar
diseñándolos
Redes
neuronales
convolucionales
sección
vimos
convoluciones
herramienta
útil
procesamiento
imágenes
utilidad
herramientas
eventualmente
lugar
crear
eventualmente
redes
neuronales
convolucionales
diferencia
redes
MLP
habíamos
visto
anteriormente
parametro
aprendidos
kernels
realizan
convoluciones
Veamos
ejemplo
empezando
ejemplo
cuadricula
aplicamos
kernel
Analicemos
llegado
tamaño
eje
tamaño
salida
ligeramente
pequeño
tamaño
entrada
kernel
ancho
alto
calcular
correctamente
convolución
ubicaciones
kernel
encaja
completamente
imagen
tamaño
salida
viene
tamaño
entrada
tamaño
kernel
convolución
caso
necesitamos
suficiente
espacio
mover
kernel
convolución
imagen
veremos
mantener
tamaño
cambios
rellenando
imagen
ceros
borde
suficiente
espacio
kernel
continuación
implementamos
proceso
función
corr2d
acepta
tensor
entrada
tensor
kernel
devuelve
tensor
salida
Y.
Capas
convolucionales
capa
convolucional
aplica
función
sesgo
escalar
producir
salida
parámetros
capa
convolucional
núcleo
sesgo
escalar
entrenar
modelos
basados
capas
convolucionales
normalmente
inicializamos
núcleos
azar
haríamos
capa
completamente
conectada
listos
implementar
capa
convolucional
bidimensional
basado
función
corr2d
definida
anteriormente
método
constructor
init
declaramos
weight
bias
parámetros
modelo
función
propagación
directa
llama
función
corr2d
agrega
sesgo
ejemplo
Detector
bordes
verticales
Tomemos
momento
analizar
aplicación
simple
capa
convolucional
detectar
borde
objeto
imagen
buscamos
ubicación
cambio
píxel
construimos
imagen
píxeles
columnas
negras
resto
blancas
continuación
construimos
núcleo
altura
ancho
realizamos
aplicar
convolución
elementos
adyacentes
horizontalmente
iguales
salida
contrario
salida
distinta
cero
kernel
caso
especial
operador
diferencias
finitas
ubicación
calcula
calcula
diferencia
valores
píxeles
adyacentes
horizontalmente
aproximación
discreta
derivada
dirección
horizontal
listos
convolución
argumentos
entrada
kernel
detectamos
borde
blanco
negro
-1
borde
negro
blanco
salidas
toman
valor
aplicar
kernel
transpuesto
obtenemos
ceros
versión
transpuesta
kernel
detecta
bordes
verticales
imagen
Aprendiendo
kernel
Diseñar
detector
bordes
diferencias
finitas
-1
útil
precisamente
buscando
medida
observamos
núcleos
considerar
capas
sucesivas
convoluciones
imposible
especificar
precisamente
filtro
manualmente
veamos
aprender
núcleo
generó
mirando
pares
entrada-salida
construimos
capa
convolucional
inicializa
kernel
tensor
aleatorio
continuación
iteración
usaremos
error
cuadrado
comparar
salida
capa
convolucional
calcular
gradiente
actualizar
kernel
simplicidad
clase
incorporada
capas
convolucionales
bidimensionales
ignorar
sesgo
10
iteraciones
aproximación
kernel
esperado
Correlación
cruzada
convoluciones
hablado
convoluciones
realidad
diferencia
operación
convolución
propiamente
dicha
operación
realizamos
realidad
correlación
cruzada
pasaría
capas
realizaran
operaciones
convolución
estrictas
lugar
correlaciones
cruzadas
obtener
resultado
operación
convolución
estricta
necesitamos
voltear
tensor
kernel
bidimensional
horizontal
verticalmente
operación
correlación
cruzada
tensor
entrada
Cabe
señalar
núcleos
aprenden
datos
aprendizaje
salidas
capas
convolucionales
ven
afectadas
importa
llevamos
cabo
convoluciones
estrictas
operaciones
correlación
cruzada
terminología
estándar
literatura
aprendizaje
profundo
seguiremos
refiriéndonos
operación
correlación
cruzada
convolución
estrictamente
hablando
ligeramente
término
elemento
referirnos
entrada
componente
tensor
represente
representación
capa
núcleo
convolución
Mapa
características
Campo
receptivo
salida
capa
convolucional
llama
mapa
características
representaciones
aprendidas
características
dimensiones
espaciales
ejemplo
ancho
alto
alimentan
capa
subsiguiente
CNN
elemento
capa
campo
receptivo
refiere
elementos
capas
anteriores
afectar
cálculo
propagación
directa
campo
receptivo
tamaño
real
entrada
Sigamos
usando
figura
explicar
campo
receptivo
kernel
núcleo
convolución
campo
receptivo
elemento
salida
sombreado
valor
elementos
sombreada
entrada
denotar
salida
forma
considere
CNN
profunda
capa
convolucional
adicional
toma
entrada
salida
elemento
caso
campo
receptivo
incluye
elementos
campo
receptivo
entrada
original
incluye
elementos
entrada
elemento
mapa
características
necesita
campo
receptivo
construir
red
profunda
campos
receptivos
derivan
nombre
neurofisiología
serie
experimentos
variedad
animales
estímulos
Hubel
Wiesel
exploraron
respuesta
corteza
visual
dichos
estímulos
general
encontraron
niveles
inferiores
responden
bordes
formas
relacionadas
Posteriormente
ilustró
efecto
naturaleza
imágenes
llamar
núcleos
convolucionales
Resulta
relación
válida
características
calculadas
capas
profundas
redes
entrenadas
tareas
clasificación
imágenes
Baste
circunvoluciones
demostrado
herramienta
increíblemente
poderosa
visión
computadora
biología
código
sorprende
retrospectiva
anunciaran
éxito
reciente
aprendizaje
profundo
Padding
Stride
Volvamos
figura
entrada
altura
ancho
núcleo
convolución
altura
ancho
produciendo
representación
salida
dimensión
Dijimos
entrada
forma
kernel
forma
forma
salida
mover
kernel
agote
píxeles
aplicar
convolución
continuación
exploraremos
serie
técnicas
incluyendo
relleno
convoluciones
strides
trancos
ofrecen
control
tamaño
salida
motivación
núcleos
generalmente
ancho
alto
aplicar
convoluciones
sucesivas
tendemos
terminar
resultados
considerablemente
pequeño
entrada
comenzamos
imagen
píxeles
capas
convoluciones
imágen
recude
píxeles
cortando
imagen
borrando
información
interesante
límites
imagen
original
Padding
agregar
relleno
herramienta
popular
manejar
problema
casos
querer
reducir
dimensionalidad
drásticamente
ejemplo
encontramos
resolución
entrada
original
difícil
manejar
convoluciones
strides
trancos
técnica
popular
ayudar
casos
Padding
describió
anteriormente
problema
aplicar
capas
convolucionales
tendemos
perder
píxeles
perímetro
imagen
Considere
próxima
imagen
representa
utilización
píxeles
función
tamaño
kernel
convolución
posición
imagen
píxeles
esquinas
utilizan
normalmente
núcleos
pequeños
convolución
dada
perdamos
píxeles
número
píxeles
crece
medida
aplicamos
capas
convolucionales
sucesivas
solución
directa
problema
agregar
píxeles
adicionales
relleno
límite
imagen
entrada
aumentando
tamaño
efectivo
imagen
general
establecemos
valores
píxeles
adicionales
cero
imágen
rellenamos
entrada
aumentando
tamaño
salida
correspondiente
aumenta
matriz
partes
sombreadas
elemento
salida
elementos
entrada
kernel
utilizados
cálculo
salida
general
agregamos
filas
relleno
mitad
mitad
abajo
columnas
relleno
mitad
izqueirda
mitad
derecha
salida
significa
altura
ancho
salida
aumentará
respectivamente
casos
querremos
configurar
entrada
salida
altura
anchura
facilitará
predicción
forma
salida
capa
construir
red
Asumiendo
impar
rellenaremos
filas
lados
altura
par
posibilidad
pad
filas
superior
entrada
filas
inferior
Rellenaremos
lados
ancho
CNN
suelen
utilizar
núcleos
convolución
valores
impares
alto
ancho
Elegir
tamaños
kernel
impares
ventaja
preservar
dimensionalidad
rellena
número
filas
superior
inferior
número
columnas
izquierda
derecha
práctica
núcleos
impares
padding
preservar
precisión
dimensionalidad
ofrece
beneficio
adicional
tensor
bidimensional
tamaño
núcleo
impar
número
filas
columnas
relleno
bordes
iguales
salida
forma
entrada
campo
receptivo
salida
Y[i
calcula
ventana
centrada
X[i
kernel
ejemplo
creamos
capa
convolucional
bidimensional
kernel
altura
ancho
agregamos
píxel
relleno
bordes
Dada
entrada
altura
ancho
encontramos
altura
ancho
salida
truco
kernel
tamaño
impar
Stride
trancos
calcular
convoluciones
empezamos
ventana
convolución
esquina
superior
izquierda
tensor
entrada
deslizamos
ubicaciones
abajo
derecha
ejemplos
anteriores
defecto
deslizamos
elemento
eficiencia
computacional
deseamos
reducir
muestra
movemos
ventana
elemento
saltándose
ubicaciones
intermedias
particularmente
útil
kernel
convolución
captura
área
imagen
subyacente
referimos
número
filas
columnas
atravesadas
calculo
stride
tranco
usado
trancos
altura
ancho
querer
paso
figura
muestra
ejemplo
tranco
vertical
horizontal
partes
sombreadas
elementos
salida
elementos
kernel
entrada
utilizados
cálculo
salida
genera
elemento
columna
ventana
convolución
desliza
abajo
filas
ventana
convolución
desliza
columnas
derecha
genera
elemento
fila
ventana
convolución
continúa
deslizando
columnas
derecha
entrada
salida
elemento
entrada
llenar
ventana
agreguemos
columna
relleno
general
tranco
alto
ancho
salida
forma
Fijando
salida
reduce
ancho
alto
entrada
divisible
ancho
alto
tranco
encontes
expresión
reduce
ejemplo
fijar
ancho
alto
tranco
cosas
complejas
Entradas
salidas
multicanal
ignorado
completamente
naturaleza
tridimensional
imágenes
imágenes
color
tradicionales
canales
RGB
indicar
intensidad
rojo
verde
azul
indices
ancho
alto
canal
simplificado
ejemplos
numéricos
trabajando
canal
entrada
canal
salida
permitió
pensar
entradas
núcleos
convolución
generar
tensores
bidimensionales
agregamos
canales
mezcla
entradas
representaciones
ocultas
convierten
tensores
tridimensionales
ejemplo
imagen
entrada
RGB
forma
referimos
eje
tamaño
dimensión
canal
noción
canales
antiguo
CNN
sección
echar
vistazo
profundo
convoluciones
múltiples
canales
entrada
salida
Entrada
multicanal
datos
entrada
contienen
múltiples
canales
necesitamos
construir
convolución
número
canales
datos
entrada
convolución
datos
entrada
Suponiendo
número
canales
datos
entrada
número
canales
entrada
kernel
convolución
forma
ventana
kernel
convolución
pensar
kernel
convolución
tensor
bidimensional
forma
necesitamos
kernel
contenga
tensor
forma
canal
entrada
Concatenando
tensores
juntos
produce
kernel
convolución
forma
tensor
entrada
convolución
canales
convolución
tensor
bidimensional
entrada
tensor
bidimensional
kernel
convolución
canal
sumando
resultados
juntos
suma
canales
producir
tensor
bidimensional
resultado
convolución
bidimensional
entrada
multicanal
núcleo
convolución
múltiples
canales
entrada
figura
ejemplo
convolucion
bidimensional
canales
entrada
partes
sombreadas
elemento
salida
elementos
tensor
kernel
entrada
utilizados
cálculo
salida
Tratemos
implementar
ejemplo
Recordemos
convolución
canal
sumamos
canal
Tratemos
reproducir
resultados
figura
validar
función
actúe
correctamente
Salidas
múltiples
canales
Independientemente
número
canales
entrada
terminamos
canal
salida
resulta
esencial
múltiples
canales
capa
arquitecturas
redes
neuronales
populares
realidad
aumentamos
dimensión
canal
medida
profundizamos
red
neuronal
Típicamente
reducimos
tamaño
imagen
compensar
perdida
resolución
espacial
aumentamos
profundidad
canal
Intuitivamente
pensar
canal
respondiendo
conjunto
características
realidad
complicada
interpretación
ingenua
sugeriría
representaciones
aprenden
forma
independiente
píxel
canal
cambio
canales
optimizados
útiles
conjunto
significa
lugar
asignar
canal
detector
borde
significar
simplemente
dirección
espacio
canal
corresponde
detección
bordes
Denotamos
número
canales
entrada
salida
respectivamente
altura
ancho
kernel
obtener
salida
múltiples
canales
crear
tensor
kernel
forma
canal
salida
concatenamos
dimensión
canal
salida
forma
núcleo
convolución
operaciones
convolución
calcula
resultado
canal
salida
kernel
convolución
correspondiente
canal
salida
toma
entrada
canales
tensor
entrada
Implementamos
función
convolución
calcular
salida
múltiples
canales
muestra
continuación
armar
ejemplo
kernel
canales
salida
ejemplo
mostramos
código
aplicamos
función
recien
armamos
tensor
recien
creado
salida
canales
caso
canal
salida
coincide
calculado
anteriormente
Convoluciones
tamaño
principio
convolución
sentido
convolución
analiza
píxeles
adyacentes
convolución
obviamente
obstante
operaciones
populares
incluyen
diseños
redes
profundas
complejas
Veamos
detalle
realmente
utiliza
ventana
mínima
convolución
pierde
capacidad
capas
convolucionales
reconocer
patrones
consisten
interacciones
elementos
adyacentes
dimensiones
alto
ancho
único
cálculo
convolución
ocurre
dimensión
canal
figura
muestra
cálculo
convolución
usando
núcleo
convolución
canales
entrada
canales
salida
entradas
salidas
altura
anchura
elemento
salida
deriva
combinación
lineal
elementos
posición
imagen
entrada
capa
convolucional
pensarse
capa
densa
aplicada
ubicación
píxel
transformar
valores
entrada
correspondientes
valores
salida
capa
convolucional
pesos
vinculados
ubicación
píxel
capa
convolucional
requiere
pesos
sesgo
capas
convolucionales
capa
continuada
función
activación
asegura
convoluciones
imitadas
convoluciones
Veamos
implementa
práctica
Consideremos
capa
densa
aplicada
convolución
Slo
necesitamos
adaptar
entrada
salida
multiplicacion
matrices
resultado
indistinguible
descripto
compración
Pooling
casos
tarea
plantea
pregunta
global
imagen
ejemplo
contiene
gato
consecuencia
unidades
capa
sensible
entrada
Agregando
información
gradualmente
produciendo
campos
receptivos
logramos
objetivo
finalmente
aprender
representación
global
manteniendo
ventajas
capas
convolucionales
capas
intermedias
procesamiento
adentramos
red
campo
receptivo
relación
entrada
nodo
oculto
sensible
Reducir
resolución
espacial
acelera
proceso
núcleos
convolución
cubren
área
efectiva
detectar
características
nivel
inferior
bordes
representaciones
invariables
translación
ejemplo
tomamos
imagen
delimitación
nítida
blanco
negro
desplazamos
imagen
píxel
derecha
Z[i
X[i
salida
imagen
borde
desplazado
píxel
realidad
objetos
encuentran
exactamente
lugar
trípode
objeto
estacionario
vibración
cámara
movimiento
obturador
cambiar
píxel
cámaras
gama
alta
cargadas
características
especiales
abordar
problema
Hablaremos
capas
pooling
sirven
doble
propósito
mitigar
sensibilidad
capas
convolucionales
ubicación
representaciones
reducción
muestreo
espacial
Pooling
máximos
promedios
capas
convolucionales
operadores
pooling
consisten
ventana
forma
fija
desliza
regiones
entrada
stride
calcula
salida
ubicación
atravesada
ventana
forma
fija
conocida
ventana
pooling
diferencia
cálculo
convoluciones
capa
pooling
contiene
parámetros
kernel
cambio
operadores
pooling
deterministas
típicamente
calculando
valor
máximo
promedio
elementos
ventana
pooling
operaciones
denominan
pooling
máximo
max-pooling
abreviar
pooling
promedio
respectivamente
pooling
promedio
esencialmente
antigua
CNN
idea
similar
reducir
resolución
imagen
lugar
simplemente
tomar
valor
tercio
píxel
imagen
menor
resolución
promediar
píxeles
adyacentes
obtener
imagen
relación
señal
ruido
combinando
información
múltiples
píxeles
adyacentes
Max-pooling
introdujo
contexto
neurociencia
cognitiva
describir
agregación
información
agregarse
jerárquicamente
propósito
reconocimiento
objetos
versión
reconocimiento
voz
casos
max-pooling
preferible
casos
convolución
pensar
ventana
pooling
partiendo
superior
izquierda
tensor
entrada
deslizándose
tensor
entrada
izquierda
derecha
abajo
ubicación
llega
ventana
pooling
calcula
máximo
promedio
valor
subtensor
entrada
ventana
dependiendo
emplea
pooling
máximo
promedio
tensor
salida
figura
tamaño
elementos
calulados
términos
generales
definir
capa
pooling
región
tamaño
Volviendo
problema
detección
bordes
salida
capa
convolucional
entrada
max-pooling
Denotaremos
entrada
capa
convolucional
salida
capa
pooling
Independientemente
valores
X[i
X[i
X[i+1
X[i+1
capa
pooling
genera
Y[i
usando
capa
pooling
máximo
detectar
patrón
reconocido
capa
convolucional
mueve
elemento
altura
anchura
código
implementamos
propagación
directa
capa
pooling
función
pool2d
función
similar
función
corr2d
necesita
kernel
calculando
salida
máximo
promedio
región
entrada
Veamos
ejemplos
validar
función
Padding
Stride
capas
convolucionales
capas
pooling
cambian
forma
salida
ajustar
operación
lograr
forma
salida
deseada
rellenando
entrada
ajustando
stride
demostrar
padding
stride
capas
pooling
capa
max
pooling
pytorch
construimos
tensor
entrada
cuya
forma
dimensiones
número
ejemplos
tamaño
lote
número
canales
capa
pooling
agrupa
ingomación
area
mayoría
frameworks
supone
tamaño
ventana
pooling
coincide
strides
ejemplo
ventana
pytorch
supone
stride
defecto
modificar
querramos
Trabajando
múltiples
canales
procesar
datos
entrada
multicanal
capa
pooling
agrupa
canal
entrada
separado
lugar
sumar
entradas
canales
capa
convolucional
significa
número
canales
salida
capa
pooling
número
canales
entrada
continuación
concatenaremos
tensores
dimensión
canal
construir
entrada
canales
dijimos
número
canales
entrada
salido
LeNet
ingredientes
necesarios
armar
CNN
totalmente
funcional
trabajamos
anteriormente
imágenes
habíamos
usado
modelo
lineal
regresión
softmax
MLP
imágenes
ropa
dataset
Fashion-MNIST
datos
manejables
aplanamos
imagen
matriz
vector
dimensional
longitud
fija
procesó
capas
completamente
conectadas
control
capas
convolucionales
retener
estructura
espacial
imágenes
beneficio
adicional
reemplazar
capas
completamente
conectadas
capas
convolucionales
modelos
requieren
parámetros
sección
presentaremos
LeNet
primeras
CNN
publicadas
capturar
amplia
atención
desempeño
tareas
visión
computadora
modelo
presentado
nombrado
Yann
LeCun
investigador
AT&T
Bell
Labs
propósito
reconocer
dígitos
escritos
mano
imágenes
dataset
trabajó
LeCun
equipo
conoce
MNIST
modelo
conocido
probado
especialista
argumenta
carece
sentido
usarlo
Fashion
MNIST
propuesto
alternativa
momento
LeNet
logró
resultados
sobresalientes
equiparar
rendimiento
máquinas
vectores
soporte
enfoque
dominante
aprendizaje
supervisado
logrando
tasa
error
1%
dígito
LeNet
rasgos
LeNet
partes
consistente
encoder
convolucional
capas
MLP
capas
Decimos
encoder
convolucional
toma
imágenes
transforma
vectores
120
componentes
representa
información
contenida
imágen
unidades
básicas
bloque
convolucional
capa
convolucional
función
activación
sigmoidea
posterior
operación
pooling
promedios
ReLU
MaxPooling
funcionan
descubrimientos
momento
publicación
LeNet
capa
convolucional
kernel
función
activación
sigmoidea
Observé
salida
capa
connvolucional
aumenta
número
canales
salida
capa
convolucional
canales
salida
16
operación
pooling
stride
reduce
dimensionalidad
factor
reducción
resolución
espacial
bloque
convolucional
emite
salida
forma
dada
tamaño
lote
número
canal
alto
ancho
pasar
salida
bloque
convolucional
MLP
debemos
aplanar
ejemplo
minilote
palabras
tomamos
entrada
dimensiones
transformamos
entrada
bidimensional
esperada
MLP
recordatorio
tensor
forma
tamaño
minilote
dim
plana
número
dim
plana
número
dimensiones
vector
utilizado
representación
vectorial
plana
ejemplo
bloque
denso
LeNet
capas
completamente
conectadas
120
84
10
salidas
respectivamente
realizando
clasificación
capa
salida
10
dimensiones
corresponde
número
posibles
clases
salida
tortuoso
parecer
descripción
implementación
usando
torch
sencilla
necesitaremos
método
Sequential
quieran
analizar
paper
LeCunn
pequeña
modificación
LeCunn
equipo
usaron
clasificación
gaussiano
usaremos
SoftMax
continuación
mostramos
cambia
forma
tensor
entrante
saliente
capas
usadas
Cargando
datos
dijimos
trabajar
Fashion
MNIST
cargaremos
dataset
biblioteca
torch
calcularemos
accuracy
modelo
Definiremos
función
entrenamiento
evaluación
habíamos
usado
función
calcular
tiempo
cálculo
Entrenamiento
