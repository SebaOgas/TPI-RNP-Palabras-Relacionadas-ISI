Modelos
Difusión
DDPMs
Difusión
difusión
fenómeno
natural
fundamental
observado
sistemas
incluyendo
física
química
biología
nota
fácilmente
vida
cotidiana
Considerá
ejemplo
rociar
perfume
principio
moléculas
perfume
densamente
concentradas
cerca
punto
rociado
tiempo
moléculas
dispersan
difusión
proceso
partículas
información
energía
mueven
área
alta
concentración
menor
concentración
sucede
sistemas
tienden
alcanzar
equilibrio
concentraciones
vuelven
uniformes
sistema
aprendizaje
automático
generación
datos
difusión
refiere
enfoque
específico
generar
datos
usando
proceso
estocástico
similar
cadena
Markov
contexto
modelos
difusión
crean
muestras
datos
comenzando
datos
simples
fáciles
generar
gradualmente
transformándolos
datos
complejos
realistas
clase
profundizar
Modelos
Probabilísticos
Difusión
Eliminación
Ruido
conocidos
DDPMs
siglas
inglés
Denoising
Diffusion
Probabilistic
Models
investigadores
logrado
resultados
notables
generación
condicionada
imágenes
audio
video
instalar
importar
bibliotecas
necesarias
asumiendo
tenés
PyTorch
instalado
modelo
difusión
modelo
difusión
eliminación
ruido
complejo
comparás
modelos
generativos
GANs
VAEs
convierten
ruido
distribución
simple
muestra
datos
ocurre
red
neuronal
aprende
eliminar
gradualmente
ruido
datos
comenzando
puro
ruido
detalle
imágenes
configuración
consta
procesos
proceso
difusión
fijo
predefinido
elección
agrega
gradualmente
ruido
gaussiano
imagen
terminás
puro
ruido
proceso
difusión
inverso
eliminación
ruido
aprendido
entrena
red
neuronal
eliminar
gradualmente
ruido
imagen
comenzando
puro
ruido
terminás
imagen
real
proceso
proceso
inverso
indexados
ocurren
número
finito
pasos
tiempo
autores
DDPM
Comenzás
muestreás
imagen
real
distribución
datos
digamos
imagen
gato
ImageNet
proceso
muestrea
ruido
distribución
gaussiana
paso
tiempo
agrega
imagen
paso
tiempo
suficientemente
esquema
estructurado
agregar
ruido
paso
tiempo
terminás
llama
distribución
gaussiana
isotrópica
proceso
gradual
forma
matemática
escribir
formal
instancia
necesitamos
función
pérdida
tratable
red
neuronal
optimizar
distribución
datos
reales
ejemplo
imágenes
reales
muestrear
distribución
obtener
imagen
Definimos
proceso
difusión
agrega
ruido
gaussiano
paso
tiempo
esquema
varianza
conocido
Recordemos
distribución
normal
llamada
distribución
gaussiana
define
parámetros
media
varianza
Básicamente
imagen
ligeramente
ruidosa
paso
tiempo
extrae
distribución
gaussiana
condicional
muestreando
estableciendo
Notemos
constantes
paso
tiempo
subíndice
define
llamado
esquema
varianza
lineal
cuadrático
coseno
etc.
veremos
esquema
tasa
aprendizaje
comenzando
terminamos
ruido
gaussiano
puro
configuramos
esquema
adecuadamente
conociéramos
distribución
condicional
podríamos
ejecutar
proceso
reversa
muestreando
ruido
gaussiano
aleatorio
eliminando
ruido
gradualmente
terminemos
muestra
distribución
real
conocemos
intratable
requiere
distribución
imágenes
posibles
calcular
probabilidad
condicional
aprovechar
red
neuronal
aproximar
aprender
distribución
probabilidad
condicional
llamémosla
parámetros
red
neuronal
actualizados
descenso
gradiente
necesitamos
red
neuronal
representar
distribución
probabilidad
condicional
proceso
inverso
asumimos
proceso
inverso
gaussiano
recordemos
distribución
gaussiana
define
parámetros
media
parametrizada
varianza
parametrizada
parametrizar
proceso
media
varianza
condicionadas
nivel
ruido
red
neuronal
necesita
aprender
representar
media
varianza
autores
DDPM
decidieron
mantener
varianza
fija
permitir
red
neuronal
aprenda
represente
media
distribución
probabilidad
condicional
continuamos
asumiendo
red
neuronal
necesita
aprender
representar
media
distribución
probabilidad
condicional
Definiendo
función
objetivo
reparametrizando
media
red
neuronal
aprender
media
distribución
probabilidad
condicional
función
pérdida
expresar
suma
pérdidas
paso
tiempo
término
suma
pérdida
realidad
divergencia
KL
distribuciones
gaussianas
real
predicha
divergencia
KL
mide
desvía
distribución
caso
expresar
pérdida
L2
medias
distribuciones
consecuencia
directa
proceso
muestra
Sohl-Dickstein
et
muestrear
nivel
ruido
arbitrario
condicionado
suma
gaussianas
gaussiana
conveniente
necesitamos
aplicar
repetidamente
muestrear
Refiramos
ecuación
propiedad
agradable
significa
muestrear
ruido
gaussiano
escalarlo
apropiadamente
agregarlo
obtener
directamente
Notemos
funciones
esquema
varianza
conocido
conocidos
precomputarse
permite
entrenamiento
optimizar
términos
aleatorios
función
pérdida
palabras
muestrear
aleatoriamente
entrenamiento
optimizar
ventaja
propiedad
muestra
Ho
et
cálculos
remitimos
lector
excelente
posteo
blog
reparametrizar
media
red
neuronal
aprenda
prediga
ruido
agregado
red
nivel
ruido
términos
KL
constituyen
pérdidas
significa
red
neuronal
convierte
predictor
ruido
lugar
predictor
media
directo
media
calcular
función
objetivo
queda
paso
tiempo
aleatorio
imagen
inicial
real
corrompida
vemos
muestra
directa
nivel
ruido
dada
proceso
fijo
ruido
puro
muestreado
paso
tiempo
red
neuronal
red
neuronal
optimiza
utilizando
simple
error
cuadrático
MSE
ruido
gaussiano
real
predicho
Resumen
Tomamos
muestra
aleatoria
distribución
datos
real
desconocida
posiblemente
compleja
Muestreamos
nivel
ruido
uniformemente
paso
tiempo
aleatorio
Muestreamos
ruido
distribución
gaussiana
corrompemos
entrada
ruido
nivel
usando
propiedad
agradable
definida
anteriormente
red
neuronal
entrena
predecir
ruido
basado
imagen
corrupta
ruido
aplicado
basado
esquema
conocido
Implementación
PyTorch
Red
Neuronal
red
neuronal
necesita
tomar
imagen
ruidosa
paso
tiempo
particular
devolver
ruido
predicho
Notemos
ruido
predicho
tensor
tamaño
resolución
imagen
entrada
técnicamente
red
toma
produce
tensores
forma
tipo
red
neuronal
términos
arquitectura
autores
DDPM
optaron
U-Net
introducido
Ronneberger
et
2015
momento
logró
resultados
generación
segmentación
imágenes
médicas
red
autoencoder
consta
cuello
botella
asegura
red
aprenda
información
importante
Importante
destacar
introduce
conexiones
residuales
codificador
decodificador
mejorando
significativamente
flujo
gradientes
inspirado
ResNet
et
2015
modelo
U-Net
reduce
resolución
espacial
entrada
entrada
pequeña
términos
resolución
espacial
realiza
aumento
resolución
continuación
implementamos
red
paso
paso
Funciones
Auxiliares
Red
definimos
funciones
clases
auxiliares
utilizarán
implementar
red
neuronal
importante
destacar
definimos
módulo
Residual
simplemente
suma
entrada
salida
función
particular
palabras
añade
conexión
residual
función
particular
definimos
alias
operaciones
reducción
aumento
resolución
upsampling
downsampling
Embeddings
Posición
parámetros
red
neuronal
comparten
tiempo
nivel
ruido
autores
emplean
embeddings
posición
sinusoidales
codificar
tt
inspirados
Transformer
Vaswani
et
2017
permite
red
neuronal
sepa
paso
tiempo
particular
nivel
ruido
operando
imagen
lote
módulo
SinusoidalPositionEmbeddings
toma
tensor
forma
batchsize,1
entrada
niveles
ruido
imágenes
ruidosas
lote
convierte
tensor
forma
batchsize
dim
dim
dimensionalidad
embeddings
posición
añade
bloque
residual
veremos
Bloque
ResNet
continuación
definimos
bloque
central
modelo
U-Net
autores
DDPM
emplearon
bloque
Wide
ResNet
Zagoruyko
et
2016
Phil
Wang
reemplazado
capa
convolucional
estándar
versión
weight
standardized
funciona
combinación
normalización
grupos
Kolesnikov
et
2019
detalles
Módulo
Atención
continuación
definimos
módulo
atención
autores
DDPM
añadieron
bloques
convolucionales
atención
bloque
construcción
famosa
arquitectura
Transformer
Vaswani
et
2017
mostrado
éxito
dominios
IA
NLP
visión
plegamiento
proteínas
Phil
Wang
emplea
variantes
atención
atención
múltiples
cabezas
multi-head
self-attention
regular
Transformer
variante
atención
lineal
Shen
et
2018
cuyos
requisitos
tiempo
memoria
escalan
lineal
longitud
secuencia
diferencia
atención
regular
escala
cuadrática
Normalización
Grupos
autores
DDPM
intercalan
capas
convolucionales
atención
U-Net
normalización
grupos
Wu
et
2018
continuación
definimos
clase
PreNorm
utilizará
aplicar
normalización
grupos
capa
atención
veremos
U-Net
Condicional
definido
bloques
construcción
embeddings
posición
bloques
ResNet
atención
normalización
grupos
hora
definir
red
neuronal
Recordemos
tarea
red
tomar
lote
imágenes
ruidosas
respectivos
niveles
ruido
devolver
ruido
añadido
entrada
formalmente
red
toma
lote
imágenes
ruidosas
forma
lote
niveles
ruido
forma
entrada
devuelve
tensor
forma
red
construye
aplica
capa
convolucional
lote
imágenes
ruidosas
calculan
embeddings
posición
niveles
ruido
aplica
secuencia
etapas
reducción
resolución
downsampling
etapa
reducción
resolución
consta
bloques
ResNet
normalización
grupos
atención
conexión
residual
operación
reducción
resolución
red
vuelven
aplicar
bloques
ResNet
intercalados
atención
aplica
secuencia
etapas
aumento
resolución
upsampling
etapa
aumento
resolución
consta
bloques
ResNet
normalización
grupos
atención
conexión
residual
operación
aumento
resolución
Finalmente
aplica
bloque
ResNet
seguido
capa
convolucional
instancia
redes
neuronales
apilan
capas
fueran
bloques
lego
importante
entender
funcionan
Definiendo
proceso
difusión
proceso
difusión
agrega
gradualmente
ruido
imagen
distribución
real
cantidad
pasos
tiempo
T.
ocurre
programa
varianza
autores
originales
DDPM
emplearon
programa
lineal
Establecemos
varianzas
proceso
constantes
aumentan
linealmente
β1=10−4
βT=0.02
demostró
Nichol
et
2021
obtener
mejores
resultados
emplear
programa
cosenoidal
continuación
definimos
programas
pasos
tiempo
TT
elegiremos
comenzar
usemos
programa
lineal
T=300
pasos
tiempo
definamos
diversas
variables
βt
necesitaremos
producto
acumulativo
varianzas
αˉt
variables
continuación
tensores
unidimensionales
almacenan
valores
T.
importante
definir
función
extracción
permitirá
extraer
índice
adecuado
lote
índices
ilustrar
imagen
gatos
agrega
ruido
paso
tiempo
proceso
difusión
ruido
agrega
tensores
PyTorch
lugar
imágenes
Pillow
definiremos
transformaciones
imagen
permitan
pasar
imagen
PIL
tensor
PyTorch
agregar
ruido
viceversa
transformaciones
simples
normalizamos
imágenes
dividiéndolas
255
estén
rango
0,1
aseguramos
estén
rango
−1,1
artículo
DDPM
Asumimos
datos
imagen
consisten
enteros
0,1,
,255
escalados
linealmente
−1,1
asegura
proceso
inverso
red
neuronal
opere
entradas
escaladas
consistentemente
comenzando
distribución
normal
estándar
p(xT
definir
proceso
difusión
artículo
Probémoslo
paso
tiempo
particular
Visualicemos
pasos
tiempo
significa
definir
función
pérdida
modelo
denoise_model
U-Net
definido
anteriormente
Utilizaremos
pérdida
Huber
ruido
ruido
predicho
Definir
Dataset
DataLoader
PyTorch
definimos
Dataset
regular
PyTorch
dataset
simplemente
consiste
imágenes
dataset
real
Fashion-MNIST
CIFAR-10
ImageNet
escaladas
linealmente
−1,1
imagen
redimensiona
tamaño
interesante
notar
imágenes
voltean
horizontalmente
aleatoria
artículo
volteos
horizontales
aleatorios
entrenamiento
CIFAR10
probamos
entrenar
volteos
encontramos
volteos
mejoran
ligeramente
calidad
muestras
biblioteca
Datasets
cargar
fácilmente
dataset
Fashion
MNIST
hub
dataset
consiste
imágenes
resolución
28x28
continuación
definimos
función
aplicaremos
marcha
dataset
funcionalidad
with_transform
función
aplica
preprocesamiento
básico
imágenes
volteos
horizontales
aleatorios
reescalado
finalmente
tengan
valores
rango
−1,1
Muestreo
muestrearemos
modelo
entrenamiento
seguir
progreso
definimos
código
continuación
muestreo
resume
artículo
Algoritmo
Generar
imágenes
modelo
difusión
ocurre
invertir
proceso
difusión
comenzamos
muestreamos
ruido
puro
distribución
Gaussiana
red
neuronal
deshacer
gradualmente
ruido
usando
probabilidad
condicional
aprendido
terminamos
paso
tiempo
t=0
muestra
derivar
imagen
ligeramente
ruidosa
enchufar
reparametrización
media
usando
predictor
ruido
Recuerda
varianza
conoce
antemano
Idealmente
terminamos
imagen
provenir
distribución
datos
reales
código
continuación
implementa
código
implementa
proceso
muestreo
modelo
difusión
objetivo
generar
imagen
ruido
puro
siguiendo
serie
pasos
denoising
reducción
progresiva
ruido
analizar
función
entender
cabo
proceso
p_sample
función
p_sample
genera
muestra
paso
temporal
específico
nivel
ruido
proceso
denoising
estructura
función
modelo
difusión
inversa
elimina
ruido
paso
paso
comenzando
imagen
completamente
ruidosa
Parámetros
entrada
model
modelo
difusión
normalmente
U-Net
predice
ruido
presente
imagen
paso
t.
imagen
tensor
actual
paso
t.
índice
temporal
paso
actual
indica
nivel
ruido
t_index
Índice
paso
actual
bucle
usado
verificar
paso
inicial
Proceso
p_sample
Extraer
Parámetros
Temporales
betast
sqrtoneminusalphascumprodt
sqrtrecipalphas_t
extraen
registro
predefinido
parámetros
modelo
función
t.
valores
precomputados
permiten
calcular
cantidad
ruido
información
preservada
paso
muestreo
Predecir
Media
Ecuación
Denoising
expresión
model_mean
implementa
Ecuación
11
paper
DDPM
define
salida
modelo
predecir
versión
ruidosa
imagen
model(x
predice
ruido
imagen
paso
t.
ruido
elimina
obtener
imagen
denoised
usando
expresión
\text{model\mean
\text{sqrt\recip\alphas\t
\times
\left
\frac{\text{betas\t
\times
\text{model}(x
t)}{\text{sqrt\one\minus\alphas\cumprod\t
\right
Añadir
Ruido
Varianza
Posterior
tindex
paso
retorna
directamente
modelmean
imagen
pasos
intermedios
añade
componente
ruido
noise
ponderado
posteriorvariancet
simula
incertidumbre
proceso
denoising
psampleloop
función
aplica
p_sample
bucle
generar
imagen
ruido
puro
retrocediendo
pasos
temporales
tiempo
máximo
imagen
Parámetros
entrada
model
modelo
difusión
shape
forma
imagen
generar
incluyendo
tamaño
lote
Proceso
psampleloop
Inicialización
Ruido
Puro
Comienza
imagen
img
simplemente
ruido
aleatorio
forma
img
depende
shape
caso
batchsize
channels
imagesize
image_size
Muestreo
Paso
Paso
itera
timesteps
generando
versión
ruidosa
imagen
paso
i.
iteración
p_sample
llama
imagen
actual
img
paso
tiempo
guarda
salida
imgs
lista
imágenes
paso
denoising
Resultado
imgs
contiene
historial
imágenes
generadas
paso
denoising
ruido
inicial
imagen
generada
sample
función
sample
envoltura
conveniente
configurar
llamar
psampleloop
parámetros
tamaño
imagen
tamaño
lote
Parámetros
entrada
model
modelo
difusión
image_size
Tamaño
imagen
generada
batch_size
Tamaño
lote
imágenes
generadas
channels
Número
canales
imagen
normalmente
imágenes
RGB
Proceso
sample
Define
forma
imagen
shape
batchsize
channels
imagesize
Llama
psampleloop
modelo
forma
imagen
generando
conjunto
imágenes
finales
lote
Resumen
General
Flujo
Empieza
Ruido
sample
llama
psampleloop
img
inicializa
ruido
Proceso
Denoising
Bucle
psampleloop
realiza
secuencia
pasos
denoising
máximo
nivel
ruido
imagen
Retorno
Imágenes
paso
p_sample
aplica
corrección
ruido
basada
predicción
modelo
agrega
ruido
pasos
intermedios
paso
obtiene
imagen
limpia
resultado
modelo
difusión
Entrenar
modelo
continuación
entrenamos
modelo
habitual
PyTorch
definimos
lógica
guardar
periódicamente
imágenes
generadas
utilizando
método
muestreo
definido
anteriormente
continuación
definimos
modelo
movemos
GPU
definimos
optimizador
estándar
Adam
Empecemos
entrenar
Muestreo
inferencia
muestrear
modelo
simplemente
función
muestreo
definida
anteriormente
