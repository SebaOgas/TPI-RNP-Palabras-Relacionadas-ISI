href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
NLP/6Transformers/2Implementacion.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Implementación
Transformer
PyTorch
clase
implementaremos
versión
ligeramente
modificada
modelo
Transformer
paper
Attention
is
All
You
Need
imágenes
notebook
tomaron
paper
Transformer
obtener
información
Transformer
mirá
artículos
Introducción
Transformers
utilizan
tipo
recurrencia
cambio
modelo
compuesto
capas
lineales
mecanismos
atención
normalización
momento
creación
práctico
Septiembre
2022
Transformers
arquitectura
dominante
NLP
utilizan
lograr
resultados
arte
tareas
seguirán
utilizando
futuro
variante
Transformer
popular
BERT
Bidirectional
Encoder
Representations
from
Transformers
versiones
pre-entrenadas
utilizan
comúnmente
reemplazar
capas
embedding
modelos
NLP
diferencias
implementación
notebook
paper
utilizamos
codificación
posicional
aprendida
lugar
estática
utilizamos
optimizador
Adam
estándar
tasa
aprendizaje
estático
lugar
pasos
calentamiento
enfriamiento
suavizado
etiquetas
Realizamos
cambios
siguen
cerca
configuración
BERT
mayoría
variantes
Transformer
utilizan
configuración
similar
Cargando
Datos
entrenar
modelo
traduzca
español
frases
escritas
inglés
necesitamos
dataset
frases
escritas
idiomas
datasets
origen
destino
Usaremos
dataset
publicado
organización
Tatoeba
descargar
celda
readtokenizetatoeba
función
readtokenizetatoeba
responsable
leer
archivo
texto
fuente
contiene
frases
inglés
español
tokenización
frase
utilizar
modelo
traducción
función
cumple
propósitos
clave
cargar
datos
limpiar
texto
eliminando
información
adicional
innecesaria
tokenizar
frase
secuencias
tokens
manejables
lugar
función
lee
línea
archivo
texto
utiliza
serie
expresiones
regulares
limpiar
contenido
eliminan
marcas
innecesarias
licencia
caracteres
deseados
interferir
entrenamiento
Posteriormente
utilizan
modelos
spaCy
tokenizar
texto
inglés
español
crucial
preparar
modelo
traducción
salida
función
incluye
secuencias
tokenizadas
frases
inglés
engrows
español
sparows
listas
tokens
elementos
especiales
bos
beginning
of
sentence
eos
end
of
sentence
necesarios
modelo
sepa
comienzan
terminan
frases
forma
estructurada
representar
datos
facilita
entrenamiento
modelo
mejora
calidad
predicciones
asegurar
frase
esté
completamente
comprendida
función
readtokenizetatoeba
observa
oraciones
origen
src
incluyen
token
bos
beginning
of
sentence
inicio
secuencias
decisión
token
bos
utilizado
principalmente
indicar
modelo
traducción
comenzar
generar
secuencia
salida
target
cambio
oraciones
origen
proporcionan
modelo
propósito
simplemente
procesadas
red
requerir
marcador
explícito
inicio
oración
Tatoeba_Vocab
clase
Tatoeba_Vocab
responsable
construir
vocabulario
datos
cargados
encarga
contar
frecuencia
tokens
definir
índice
permite
control
preciso
tokens
frecuentes
incluirán
vocabulario
tokens
reservados
utilizados
entrenamiento
estructura
garantiza
capturen
elementos
relevantes
corpus
texto
eviten
palabras
frecuentes
agregar
ruido
modelo
TatoebaDataset
clase
TatoebaDataset
utiliza
cargar
datos
archivo
fuente
dividirlos
conjuntos
entrenamiento
validación
prueba
preparar
secuencias
modelo
clase
facilita
lectura
tokenización
texto
dividiéndolo
frases
inglés
español
utilizarán
pares
origen
destino
entrenamiento
dataset
divide
subconjuntos
específicos
asegurar
correcta
evaluación
prueba
rendimiento
modelo
BucketSampler
implementación
BucketSampler
permite
agrupar
ejemplos
longitud
similar
mejora
eficiencia
entrenamiento
minimizar
cantidad
padding
necesario
enfoque
asegura
modelo
procese
ejemplos
uniformes
términos
longitud
reduce
desperdicio
capacidad
computacional
mejora
convergencia
modelo
collate_batch
preparación
datos
pasados
modelo
implementado
función
collatebatch
función
toma
ejemplos
individuales
dataset
convierte
minilotes
procesados
paralelo
modelo
Utiliza
padsequence
asegurar
secuencias
minilote
tengan
longitud
añadiendo
tokens
padding
necesario
permite
modelo
procese
datos
eficiente
evita
errores
surgir
secuencias
longitud
desigual
DataLoaders
crear
DataLoaders
imprimir
contenido
Capas
Necesarias
Modelo
armar
modelo
explicar
capas
introducidas
paper
forman
esencial
encoder
decoder
Capa
atención
múltiples
cabezales
conceptos
clave
novedosos
introducidos
documento
Transformer
capa
atención
múltiples
cabezales
atención
considerar
consultas
queries
claves
keys
valores
values
consulta
clave
obtener
vector
atención
generalmente
resultado
operación
softmax
valores
suman
obtener
suma
ponderada
valores
Transformer
utiliza
atención
producto
punto
escalada
consulta
clave
combinan
tomando
producto
punto
aplicando
softmax
escalando
finalmente
multiplicar
valor
constante
dimensión
cabeza
head_dim
explicaremos
detalle
similar
atención
producto
punto
estándar
escalada
documento
evitar
resultados
productos
punto
hagan
gradientes
vuelvan
pequeños
atención
producto
punto
escalada
aplica
simplemente
consultas
claves
valores
lugar
aplicación
atención
única
consultas
claves
valores
hiddim
dividido
cabezas
atención
producto
punto
escalada
calcula
cabezas
paralelo
significa
lugar
prestar
atención
concepto
aplicación
atención
prestamos
atención
conceptos
volvemos
combinar
cabezas
forma
hiddim
capa
densa
aplicada
capa
atención
múltiples
cabezales
llamaremos
código
fco
capas
densas
código
llamaremos
fcq
fck
fcv
Recorriendo
código
módulo
calculamos
capas
lineales
fcq
fck
fcv
darnos
V.
continuación
dividimos
hiddim
consulta
clave
valor
nheads
usando
.view
permutamos
correctamente
puedan
multiplicar
juntos
calculamos
energía
atención
normalizada
multiplicando
escalando
raíz
cuadrada
headdim
calcula
hiddim
nheads
enmascaramos
energía
prestemos
atención
elemento
secuencia
deberíamos
aplicamos
softmax
dropout
continuación
aplicamos
atención
valores
cabezales
combinar
resultados
nheads
Finalmente
multiplicamos
representado
fco
implementación
longitudes
claves
valores
matriz
multiplica
salida
softmax
attention
tendremos
tamaños
dimensión
válidos
multiplicación
matrices
multiplicación
cabo
usando
torch.matmul
tensores
2-dimensionales
multiplicación
matricial
lotes
dimensiones
tensor
multiplicación
matriz
lotes
formas
query
len
key
len
value
len
head
dim
tamaño
lote
cabezal
proporciona
resultado
forma
batch
size
heads
query
len
head
dim
extraño
principio
dropout
aplica
directamente
atención
significa
probable
vector
atención
sume
prestemos
atención
token
establecerse
dropout
problemas
explican
siquiera
mencionan
paper
implementación
oficial
implementaciones
Transformer
incluido
BERT
Capa
Feed-Forward
Posicional
capa
presentada
paper
capa
feedforward
posicional
capa
relativamente
simple
comparación
capa
atención
múltiples
cabezales
entrada
transforma
hiddim
pfdim
pfdim
suele
hiddim
Transformer
original
usó
hiddim
512
pfdim
2048
función
activación
ReLU
dropout
aplican
vuelva
transformar
representación
hid_dim
BERT
función
activación
GELU
simplemente
cambiando
torch.relu
F.gelu
Construyendo
modelo
continuación
crearemos
modelo
trabajo
práctico
compone
encoder
decoder
encoder
codificando
oración
entrada
origen
inglés
vector
contexto
decoder
decodificando
vector
contexto
generar
oración
salida
objetivo
español
Bloque
Encoder
bloques
Encoder
magia
Encoder
pasamos
oración
origen
máscara
capa
atención
múltiples
cabezales
aplicamos
dropout
conexión
residual
capa
Normalización
capas
pasamos
capa
feed-forward
posicional
nuevamente
aplicamos
dropout
conexión
residual
capa
normalización
capas
obtener
salida
bloque
alimenta
bloque
parámetros
comparten
bloques
capa
atención
múltiples
cabezales
utilizada
bloque
encoder
prestar
atención
oración
origen
calculando
aplicando
atención
lugar
secuencia
llamamos
auto-atención
artículo
entra
detalles
normalización
capas
esencia
normaliza
valores
features
feature
media
desviación
estándar
permite
redes
neuronales
número
capas
Transformer
puedan
entrenar
fácilmente
Encoder
encoder
Transformer
intenta
comprimir
oración
fuente
vector
contexto
lugar
produce
secuencia
vectores
contexto
secuencia
entrada
tuviera
tokens
longitud
tendríamos
llamamos
secuencia
vectores
contexto
secuencia
variables
ocultas
variable
oculta
momento
RNN
visto
token
tokens
anteriores
vector
contexto
visto
tokens
posiciones
secuencia
entrada
tokens
pasan
capa
embedding
estándar
continuación
modelo
recurrencias
idea
orden
tokens
secuencia
Resolvemos
problema
usando
capa
incrustación
llamada
capa
embedding
posicional
capa
embedding
estándar
entrada
token
posición
token
secuencia
comenzando
token
token
bos
inicio
secuencia
posición
embedding
posicional
tamaño
vocabulario
100
significa
modelo
aceptar
oraciones
100
tokens
longitud
aumentar
manejar
oraciones
largas
implementación
Transformer
original
paper
Attention
is
All
You
Need
aprende
embedding
posicionales
lugar
utiliza
embedding
estático
fijo
arquitecturas
modernas
Transformer
BERT
embedding
posicionales
aprendidos
lugar
decidido
usarlas
tutoriales
Consulte
sección
leer
codificación
posicional
utilizada
modelo
Transformer
original
continuación
token
embeddings
posicionales
suman
elemento
elemento
obtener
vector
contiene
información
token
posición
secuencia
sumarlos
embeddings
tokens
multiplican
factor
escala
dimensión
capas
ocultas
hid_dim
supuestamente
reduce
variación
embeddings
modelo
difícil
entrenar
confiable
factor
escala
continuación
aplica
dropout
embeddings
combinados
embeddings
combinados
pasan
capas
encoder
obtener
envía
utilizado
decoder
máscara
origen
src_mask
simplemente
forma
oración
origen
valor
token
oración
origen
token
pad
capas
encoder
enmascarar
mecanismos
atención
múltiples
cabezales
calcular
aplicar
atención
oración
origen
modelo
presta
atención
tokens
pad
contienen
información
útil
Decoder
objetivo
decoder
tomar
representación
codificada
oración
origen
convertirla
tokens
predichos
oración
objetivo
comparamos
tokens
reales
oración
objetivo
calcular
pérdida
usará
calcular
gradientes
parámetros
optimizador
actualizar
pesos
mejorar
predicciones
decoder
similar
encoder
capas
atención
múltiples
cabezales
capa
auto-atención
múltiples
cabezales
enmascarada
secuencia
objetivo
capa
atención
múltiples
cabezales
representación
decoder
consulta
representación
encoder
clave
valor
decoder
utiliza
embedding
posicionales
combina
suma
elemento
elemento
embeddings
escalados
tokens
destino
seguidos
dropout
Nuevamente
embeddings
posicionales
vocabulario
100
significa
aceptar
secuencias
100
tokens
longitud
aumentar
desea
embeddings
combinados
pasan
capas
decoder
frase
origen
codificada
enc_src
máscaras
origen
destino
cantidad
capas
encoder
cantidad
capas
decoder
ambas
denotan
representación
decoder
capa
pasa
capa
densa
fc_out
PyTorch
operación
softmax
contenida
función
pérdida
necesitamos
explícitamente
capa
softmax
máscara
origen
hicimos
encoder
evitar
modelo
preste
atención
tokens
pad
máscara
destino
explicará
detalle
modelo
Seq2Seq
encapsula
encoder
decoder
esencial
procesando
tokens
destino
paralelo
necesitamos
método
evitar
decoder
haga
trampa
simplemente
mirando
token
secuencia
destino
emitiéndolo
salida
bloque
decoder
genera
valores
atención
normalizados
podamos
graficarlos
prestando
atención
modelo
Bloque
Decoder
anteriormente
bloque
decoder
similar
bloque
encoder
capas
atención
múltiples
cabezales
selfattention
encoderattention
realiza
auto-atención
encoder
utilizando
representación
decoder
momento
consulta
clave
valor
dropout
conexión
residual
normalización
capas
capa
selfattention
máscara
secuencia
objetivo
trgmask
evitar
decoder
haga
trampas
prestando
atención
tokens
procesando
actualmente
procesa
paralelo
tokens
oración
objetivo
realmente
alimentamos
oración
origen
codificada
encsrc
decoder
capa
atención
múltiples
cabezales
consultas
representaciones
decoder
momento
claves
valores
representaciones
encoder
máscara
origen
srcmask
evitar
capa
atención
múltiples
cabezales
preste
atención
tokens
pad
oración
origen
siguen
capa
dropout
conexión
residual
capa
normalización
capas
Finalmente
pasamos
capa
feed-forward
posicional
secuencia
dropout
conexión
residual
normalización
capa
bloque
decoder
introduciendo
concepto
conjunto
capas
bloque
encoder
ligeramente
Seq2Seq
Finalmente
módulo
Seq2Seq
encapsula
encoder
decoder
manejar
creación
máscaras
máscara
origen
crea
comprobando
secuencia
origen
token
pad
token
token
pad
unsqueeze
aplicar
correctamente
broadcast
aplicar
máscara
energía
forma
batch
size
heads
seq
len
seq
len
máscara
destino
complicada
creamos
máscara
tokens
pad
hicimos
máscara
origen
continuación
creamos
máscara
subsecuente
trgsubmask
usando
torch.tril
crea
matriz
diagonal
elementos
diagonal
cero
elementos
diagonal
establecerán
valga
tensor
entrada
caso
tensor
entrada
tensor
lleno
significa
trgsubmask
verá
objetivo
tokens
muestra
token
destino
fila
autorizado
mirar
columna
token
destino
máscara
significa
mirarse
token
destino
máscara
significa
token
destino
continuación
máscara
subsecuente
aplica
AND
lógico
máscara
relleno
combina
máscaras
garantiza
puedan
atender
tokens
posteriores
tokens
relleno
ejemplo
tokens
fueran
tokens
pad
máscara
vería
creadas
máscaras
utilizan
encoder
decoder
oraciones
origen
destino
obtener
oración
destino
predicha
output
atención
decoder
secuencia
origen
Entrenamiento
modelo
Seq2Seq
definir
encoder
decoder
modelo
significativamente
pequeño
Transformers
utilizan
investigación
ejecutar
rápidamente
GPU
utilizamos
definir
modelo
secuencia
secuencia
encapsulado
verificar
número
parámetros
documento
menciona
esquema
inicialización
pesos
usó
Xavier
común
modelos
Transformer
optimizador
utilizado
paper
original
Transformer
utiliza
Adam
tasa
aprendizaje
período
calentamiento
período
enfriamiento
BERT
modelos
Transformer
Adam
tasa
aprendizaje
fija
implementar
Consulte
enlace
obtener
detalles
programación
tasa
aprendizaje
Transformer
original
tasa
aprendizaje
baja
predeterminada
utilizada
Adam
contrario
aprendizaje
inestable
continuación
definimos
función
pérdida
asegurándonos
ignorar
pérdidas
calculadas
tokens
pad
definir
ciclo
entrenamiento
modelo
prediga
token
eos
entrada
modelo
simplemente
cortamos
token
eos
secuencia
denota
elemento
real
secuencia
objetivo
introducimos
modelo
obtener
secuencia
predicha
suerte
debería
predecir
token
eos
denota
elemento
secuencia
objetivo
predicho
calculamos
pérdida
usando
tensor
trg
original
ficha
sos
eliminada
frente
dejando
ficha
eos
calculamos
pérdidas
actualizamos
parámetros
estándar
ciclo
evaluación
ciclo
entrenamiento
cálculos
gradiente
actualizaciones
parámetros
definimos
pequeña
función
decirnos
tarda
época
Finalmente
entrenamos
modelo
Cargamos
mejores
parámetros
logramos
lograr
perplejidad
prueba
modelos
probamos
clases
anteriores
Inferencia
modelo
traducciones
función
translate_sentence
continuación
pasos
dados
tokenizar
oración
origen
tokenizado
añadir
tokens
bos
eos
numericalizar
oración
fuente
asignarle
entero
token
convertir
tensor
agregar
dimensión
lote
crear
máscara
oración
fuente
alimentar
frase
origen
máscara
encoder
crear
lista
contener
oración
salida
inicializada
token
sos
hayamos
alcanzado
longitud
máxima
convertir
predicción
actual
oración
salida
tensor
dimensión
lote
crear
máscara
oración
destino
colocar
salida
actual
salida
encoder
ambas
máscaras
decoder
obtener
próxima
predicción
token
salida
decoder
atención
agregar
predicción
lista
predicciones
oración
salida
actual
meter
break
predicción
token
eos
convertir
oración
salida
índices
tokens
devolver
oración
salida
token
sos
eliminado
atención
capa
definiremos
función
muestra
atención
oración
fuente
paso
decodificación
modelo
cabezales
atención
obtendremos
ejemplo
conjunto
entrenamiento
traducción
ve
modelo
cambia
is
walking
by
walks
by
significado
atención
cabezal
continuación
ciertamente
difícil
imposible
razonar
aprendido
realmente
prestar
atención
cabezales
prestan
atención
eine
traducen
absoluto
parecen
seguir
patrón
similar
escalera
descendente
atención
sacar
fichas
distribuye
fichas
oración
entrada
continuación
obtengamos
ejemplo
entrenado
modelo
conjunto
validación
modelo
traduce
cambiando
is
running
simplemente
runs
intercambio
aceptable
cabezales
prestan
atención
ein
prestan
atención
mayoría
cabezales
parecen
extender
atención
punto
tokens
eos
oración
origen
generan
punto
token
eos
oración
objetivo
predicha
parecen
prestar
atención
tokens
cercanos
comienzo
oración
Finalmente
veremos
ejemplo
datos
prueba
traducción
perfecta
