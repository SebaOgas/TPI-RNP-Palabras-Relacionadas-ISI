href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
CV/3CNNModernas/1RedesConvolucionales.ipynb
img
src='https://colab.research.google.com
assets
colab-badge.svg
/a
Redes
Convolucionales
Modernas
comprendemos
conceptos
básicos
armado
CNN
hagamos
recorrido
arquitecturas
CNN
modernas
recorrido
necesidad
incompleto
gracias
plétora
emocionantes
diseños
agregan
importancia
deriva
directamente
tareas
visión
sirven
generadores
características
básicas
tareas
avanzadas
seguimiento
tracking
segmentación
detección
objetos
transferencia
estilos
notebook
mayoría
secciones
corresponden
arquitectura
CNN
significativa
momento
actualmente
modelo
base
construyeron
proyectos
investigación
sistemas
implementados
redes
brevemente
arquitectura
dominante
ganadoras
finalistas
competencia
ImageNet
servido
barómetro
progreso
aprendizaje
supervisado
visión
computadora
2010
recientemente
transformer
comenzaron
desplazar
CNN
Cubriremos
desarrollo
clase
idea
redes
neuronales
profundas
simple
apilar
montón
capas
rendimiento
variar
enormemente
arquitecturas
opciones
hiperparámetros
redes
neuronales
descritas
capítulo
producto
intuición
conocimientos
matemáticos
ensayo
error
Presentamos
modelos
orden
cronológico
transmitir
sentido
historia
formar
intuiciones
dirige
campo
desarrollar
arquitecturas
ejemplo
normalización
lotes
conexiones
residuales
descritas
clase
ofrecido
ideas
populares
entrenar
diseñar
modelos
profundos
aplicado
arquitecturas
allá
visión
computadora
Computer
Vision
Pre-ImageNet
CNN
conocidas
comunidades
visión
computadora
aprendizaje
automático
introducción
LeNet
LeCun
et
1995
dominaron
inmediatamente
campo
LeNet
logró
resultados
conjuntos
datos
pequeños
establecido
rendimiento
viabilidad
entrenar
CNN
conjuntos
datos
realistas
tiempo
transcurrido
principios
década
1990
resultados
decisivos
2012
Krizhevsky
et
2012
redes
neuronales
superadas
métodos
aprendizaje
automático
métodos
kernel
Schölkopf
Smola
2002
métodos
ensamble
Freund
et
1996
estimación
estructurada
Taskar
et
2004
visión
computadora
comparación
precisa
entradas
redes
convolucionales
consisten
valores
píxeles
procesar
ligeramente
procesados
​​(por
ejemplo
centrados
profesionales
introducirían
píxeles
procesar
modelos
tradicionales
cambio
pipelines
típicos
visión
computadora
consistían
ingeniería
extracción
características
manual
SIFT
Lowe
2004
SURF
Bay
et
2006
bolsas
palabras
visuales
Sivic
Zisserman
2003
lugar
aprender
características
características
diseñadas
progreso
provino
ideas
inteligentes
extracción
características
comprensión
profunda
geometría
Hartley
Zisserman
2000
algoritmo
aprendizaje
pasaba
plano
aceleradores
redes
neuronales
disponibles
década
1990
suficientemente
potentes
crear
CNN
multicanal
multicapa
profundas
cantidad
parámetros
ejemplo
GeForce
256
NVIDIA
1999
procesar
máximo
480
millones
operaciones
MFLOP
marco
programación
significativo
operaciones
allá
juegos
aceleradores
actuales
rendir
300
TFLOP
dispositivo
NVIDIA's
Ampere
A100
FLOP
operaciones
punto
flotante
multiplicaciones
sumas
conjuntos
datos
relativamente
pequeños
OCR
60
000
imágenes
píxeles
tarea
desafiante
obstáculos
suman
trucos
clave
entrenar
redes
neuronales
incluyen
heurísticas
inicialización
parámetros
Glorot
Bengio
2010
variantes
inteligentes
descenso
gradiente
estocástico
Kingma
Ba
2014
funciones
activación
aplastamiento
Nair
Hinton
2010
faltaban
técnicas
regularización
efectivas
Srivastava
et
2014
lugar
entrenar
sistemas
extremo
extremo
píxel
clasificación
canalizaciones
clásicas
parecían
Obtener
conjunto
datos
interesante
conjuntos
datos
requerían
sensores
costosos
ejemplo
Apple
QuickTake
100
1994
lucía
increíble
resolución
0,3
megapíxeles
VGA
capaz
almacenar
imágenes
precio
1000
dólares
Preprocesar
conjunto
datos
características
hechas
mano
basadas
conocimientos
óptica
geometría
herramientas
analíticas
ocasionalmente
descubrimientos
fortuitos
afortunados
estudiantes
graduados
Alimentar
datos
conjunto
estándar
extractores
características
SIFT
transformación
características
invariantes
escala
Lowe
2004
SURF
características
robustas
aceleradas
Bay
et
2006
extractor
características
diseñadas
humanos
OpenCV
proporciona
extractores
SIFT
Vuelque
representaciones
resultantes
clasificador
favorito
probablemente
modelo
lineal
método
kernel
entrenar
clasificador
hubiera
podido
hablar
investigadores
aprendizaje
automático
momento
habrían
aprendizaje
automático
importante
hermoso
teorías
elegantes
demostraron
propiedades
clasificadores
Boucheron
et
2005
optimización
convexa
Boyd
Vandenberghe
2004
convirtió
pilar
obtenerlos
campo
aprendizaje
automático
próspero
riguroso
eminentemente
útil
hubiera
podido
hablar
investigador
visión
computadora
escucharía
historia
sucia
reconocimiento
imágenes
dirían
características
geometría
Hartley
Zisserman
2000
Hartley
Kahl
2009
ingeniería
lugar
algoritmos
aprendizaje
impulsaron
progreso
investigadores
visión
computadora
creían
justificadamente
conjunto
datos
limpio
canalización
extracción
características
ligeramente
mejorada
importaba
precisión
algoritmo
aprendizaje
ImageNet
modelos
profundos
capas
requieren
cantidades
datos
entrar
régimen
superan
significativamente
métodos
tradicionales
basados
​​en
optimizaciones
convexas
p.
ej.
métodos
lineales
kernel
dada
limitada
capacidad
almacenamiento
computadoras
costo
relativo
sensores
imágenes
presupuestos
investigación
comparativamente
ajustados
década
1990
mayoría
investigaciones
basaron
pequeños
conjuntos
datos
Numerosos
artículos
basaron
colección
conjuntos
datos
UCI
contenían
cientos
miles
imágenes
capturadas
baja
resolución
fondo
artificialmente
limpio
2009
lanzó
conjunto
datos
ImageNet
Deng
et
2009
desafiando
investigadores
aprender
modelos
millón
ejemplos
1000
imágenes
1000
categorías
distintas
objetos
categorías
basaron
nodos
sustantivos
populares
WordNet
Miller
1995
equipo
ImageNet
usó
búsqueda
imágenes
Google
prefiltrar
conjuntos
fotos
candidatas
categoría
empleó
servicio
crowdsourcing
Amazon
confirmar
imagen
pertenecía
categoría
asociada
escala
precedentes
superando
orden
magnitud
ejemplo
CIFAR-100
60
000
imágenes
aspecto
imágenes
tenían
resolución
relativamente
alta
píxeles
diferencia
conjunto
datos
TinyImages
80
millones
tamaño
píxeles
permitió
formación
características
nivel
superior
competencia
asociada
denominada
Desafío
reconocimiento
visual
escala
ImageNet
Russakovsky
et
2015
impulsó
investigación
visión
computadora
aprendizaje
automático
desafiando
investigadores
identificar
modelos
funcionaron
escala
académicos
considerado
previamente
conjuntos
datos
visión
LAION-5B
Schuhmann
et
2022
contienen
miles
millones
imágenes
metadatos
adicionales
AlexNet
modelos
aprendizaje
profundo
consumidores
voraces
ciclos
cómputo
entrenamiento
tomar
cientos
épocas
iteración
requiere
pasar
datos
capas
operaciones
álgebra
lineal
computacionalmente
costosas
razones
principales
década
1990
principios
2000
preferían
algoritmos
simples
basados
​​en
objetivos
convexos
optimizados
eficiente
unidades
procesamiento
gráfico
GPU
demostraron
cambio
juego
factible
aprendizaje
profundo
chips
desarrollado
tiempo
acelerar
procesamiento
gráficos
beneficio
juegos
computadora
particular
optimizaron
alto
rendimiento
productos
matriz-vector
necesarios
tareas
gráficos
computadora
Afortunadamente
matemáticas
sorprendentemente
similares
requeridas
calcular
capas
convolucionales
época
NVIDIA
ATI
comenzado
optimizar
GPU
operaciones
informáticas
generales
Fernando
2004
llegando
comercializarlas
GPU
propósito
general
GPGPU
Volvamos
2012
avance
produjo
Alex
Krizhevsky
Ilya
Sutskever
implementaron
CNN
profunda
podía
ejecutarse
GPU
cuellos
botella
computacionales
CNN
convoluciones
multiplicaciones
matrices
operaciones
paralelizarse
hardware
Utilizando
NVIDIA
GTX
580
GB
memoria
cualquiera
capacidad
1,5
TFLOP
desafío
mayoría
CPU
década
implementaron
convoluciones
rápidas
código
cuda-convnet
suficientemente
años
estándar
industria
impulsó
años
auge
aprendizaje
profundo
AlexNet
empleó
CNN
capas
ganó
ImageNet
Large
Scale
Visual
Recognition
Challenge
2012
amplio
margen
Russakovsky
et
2013
red
demostró
características
obtenidas
aprendizaje
trascender
características
diseñadas
manualmente
rompiendo
paradigma
visión
artificial
arquitecturas
AlexNet
LeNet
sorprendentemente
similares
ilustra
figura
8.1.2
proporcionamos
versión
ligeramente
optimizada
AlexNet
elimina
peculiaridades
diseño
necesitaban
2012
modelo
ajuste
GPU
pequeñas
diferencias
significativas
AlexNet
LeNet
AlexNet
profunda
comparativamente
pequeño
LeNet5
AlexNet
consta
capas
capas
convolucionales
capas
densas
ocultas
capa
densa
salida
lugar
AlexNet
usó
ReLU
lugar
sigmoida
función
activación
Profundicemos
detalles
continuación
Arquitectura
capa
AlexNet
forma
ventana
convolución
imágenes
ImageNet
altas
anchas
imágenes
MNIST
objetos
datos
ImageNet
tienden
ocupar
píxeles
detalles
visuales
consecuencia
necesita
ventana
convolución
capturar
objeto
forma
ventana
convolución
capa
reduce
seguida
capas
convolucionales
quinta
red
agrega
capas
max-pooling
forma
ventana
stride
AlexNet
canales
convolución
LeNet
capa
convolucional
enormes
capas
densas
4096
neuronas
salida
capas
requieren
parámetros
modelo
GB
memoria
limitada
primeras
GPU
AlexNet
original
usaba
diseño
flujo
datos
dual
GPU
responsable
almacenar
calcular
mitad
modelo
Afortunadamente
memoria
GPU
comparativamente
abundante
rara
necesitamos
dividir
modelos
GPU
versión
modelo
AlexNet
desvía
paper
original
aspecto
Funciones
activación
AlexNet
cambió
función
activación
sigmoidea
función
activación
ReLU
simple
cálculo
función
activación
ReLU
sencillo
ejemplo
operación
exponenciación
función
activación
sigmoidea
función
activación
ReLU
facilita
entrenamiento
modelo
utilizan
métodos
inicialización
parámetros
salida
función
activación
sigmoidea
cerca
gradiente
regiones
backpropagation
continuar
actualizando
parámetros
modelo
contrario
gradiente
función
activación
ReLU
intervalo
positivo
parámetros
modelo
inicializan
correctamente
función
sigmoidea
obtener
gradiente
intervalo
positivo
modelo
entrenar
efectiva
Image
Augmentation
aumentar
datos
ciclo
entrenamiento
AlexNet
cantidad
aumentación
imágenes
volteo
recorte
cambios
color
modelo
robusto
tamaño
muestra
reduce
efectivamente
sobreajuste
aumento
imágenes
genera
ejemplos
entrenamiento
similares
distintos
serie
cambios
aleatorios
imágenes
entrenamiento
amplía
tamaño
conjunto
entrenamiento
Alternativamente
aumento
imagen
motivado
ajustes
aleatorios
ejemplos
entrenamiento
permiten
modelos
dependan
atributos
mejorando
capacidad
generalización
ejemplo
recortar
imagen
maneras
objeto
interés
aparezca
posiciones
reduciendo
dependencia
modelo
posición
objeto
ajustar
factores
brillo
color
reducir
sensibilidad
modelo
color
Probablemente
aumento
imágenes
indispensable
éxito
AlexNet
momento
sección
discutiremos
técnica
ampliamente
utilizada
visión
artificial
investigación
métodos
comunes
aumento
imágenes
utilizaremos
imagen
ejemplo
mayoría
métodos
aumento
imágenes
grado
aleatoriedad
resulte
fácil
observar
efecto
aumento
imagen
continuación
definimos
función
auxiliar
apply
función
ejecuta
método
aumento
imagen
aug
imagen
entrada
img
muestra
resultados
Voltear
recortar
Voltear
imagen
dirección
horizontal
generalmente
cambia
categoría
objeto
utilizados
métodos
aumento
imágenes
continuación
módulo
transforms
PyTorch
crear
instancia
RandomHorizontalFlip
voltea
imagen
izquierda
derecha
50
probabilidad
Voltear
verticalmente
común
voltear
horizontalmente
imagen
ejemplo
voltear
abajo
dificulta
reconocimiento
imagen
ejemplo
gato
imagen
caso
general
clases
anteriores
explicamos
capa
pooling
reducir
sensibilidad
capa
convolucional
posición
destino
recortar
aleatoriamente
imagen
objetos
aparezcan
posiciones
imagen
escalas
reducir
sensibilidad
modelo
posición
destino
código
recortamos
azar
región
área
área
original
relación
ancho
alto
área
selecciona
azar
ancho
alto
región
escalan
200
píxeles
especifique
contrario
número
aleatorio
sección
refiere
valor
continuo
obtenido
muestreo
aleatorio
uniforme
intervalo
Cambio
colores
método
aumento
cambio
colores
cambiar
aspectos
color
imagen
brillo
contraste
saturación
tono
ejemplo
cambiamos
aleatoriamente
brillo
imagen
valor
50%
150%
imagen
original
cambiar
aleatoriamente
tono
imagen
crear
instancia
RandomColorJitter
configurar
cambiar
aleatoriamente
brillo
contraste
saturación
tono
imagen
tiempo
Combinación
métodos
aumento
imágenes
práctica
combinaremos
múltiples
métodos
aumento
imágenes
ejemplo
combinar
métodos
aumento
imágenes
definidos
anteriormente
aplicarlos
imagen
instancia
Compose
Entrenamiento
AlexNet
entrenó
ImageNet
Fashion-MNIST
entrenar
modelo
ImageNet
convergencia
horas
GPU
moderna
problemas
aplicar
AlexNet
directamente
Fashion-MNIST
imágenes
resolución
baja
píxeles
imágenes
ImageNet
cosas
funcionen
agrandamos
generalmente
práctica
inteligente
simplemente
aumenta
complejidad
computacional
agregar
información
obstante
fieles
arquitectura
AlexNet
Realizamos
cambio
tamaño
argumento
resize
constructor
FashionMNIST
comenzar
entrenar
AlexNet
comparación
LeNet
cambio
principal
tasa
aprendizaje
pequeña
entrenamiento
lento
red
profunda
amplia
resolución
imagen
alta
convoluciones
costosas
Redes
bloques
VGG
AlexNet
ofreció
evidencia
empírica
CNN
profundas
lograr
resultados
proporcionó
plantilla
general
guiar
investigadores
posteriores
diseño
redes
siguientes
secciones
presentaremos
conceptos
heurísticos
comúnmente
utilizados
diseñar
redes
profundas
diseño
arquitecturas
redes
neuronales
vuelto
abstracto
investigadores
pensar
términos
neuronas
individuales
capas
completas
bloques
patrones
repetitivos
capas
idea
bloques
surgió
Visual
Geometry
Group
VGG
Universidad
Oxford
red
VGG
nombre
fácil
implementar
estructuras
repetidas
código
framework
moderno
Deep
Learning
bucles
subrutinas
Bloques
VGG
componente
básico
CNN
secuencia
capa
convolucional
padding
mantener
resolución
ii
linealidad
ReLU
iii
capa
pooling
max-pooling
reducir
resolución
problemas
enfoque
resolución
espacial
disminuye
rapidez
particular
impone
límite
estricto
capas
convolucionales
red
agoten
dimensiones
ejemplo
caso
ImageNet
imposible
capas
convolucionales
forma
idea
clave
VGG
bloque
utilizara
múltiples
convoluciones
reducir
dimensionalidad
max-pooling
interesados
​​principalmente
determinar
redes
profundas
amplias
funcionan
ejemplo
aplicación
sucesiva
convoluciones
toca
píxeles
convolución
tiempo
tantos
parámetros
convoluciones
análisis
detallado
demostraron
redes
profundas
estrechas
superan
significativamente
contrapartes
superficiales
puso
aprendizaje
profundo
búsqueda
redes
profundas
100
capas
aplicaciones
típicas
Apilar
convoluciones
convertido
estándar
oro
redes
profundas
posteriores
Volviendo
VGG
bloque
VGG
consta
secuencia
convoluciones
kernels
padding
manteniendo
altura
ancho
seguida
capa
max-pooling
stride
reduciendo
mitad
altura
ancho
bloque
código
definimos
función
llamada
vgg_block
implementar
bloque
VGG
función
toma
argumentos
correspondientes
número
capas
convolucionales
numconvs
número
canales
salida
numchannels
Red
VGG
AlexNet
LeNet
red
VGG
dividir
partes
consiste
principalmente
capas
convolucionales
pooling
consiste
capas
densas
idénticas
AlexNet
diferencia
clave
capas
convolucionales
agrupan
transformaciones
lineales
dejan
dimensionalidad
cambios
seguido
paso
reducción
resolución
muestra
figura
AlexNet
VGG
diferencia
clave
VGG
consta
bloques
capas
capas
AlexNet
diseñadas
individualmente
convolucional
red
conecta
bloques
VGG
sucesión
agrupación
convoluciones
patrón
mantenido
cambios
década
elección
específica
operaciones
sufrido
modificaciones
considerables
variable
convarch
consiste
lista
tuplas
bloque
contiene
valores
número
capas
convolucionales
número
canales
salida
precisamente
argumentos
necesarios
llamar
función
vggblock
VGG
define
familia
redes
lugar
manifestación
específica
construir
red
específica
simplemente
iteramos
arch
componer
bloques
red
VGG
original
bloques
convolucionales
capa
convolucional
contienen
capas
convolucionales
bloque
64
canales
salida
bloque
subsiguiente
duplica
cantidad
canales
salida
número
llega
512
red
capas
convolucionales
capas
densas
denomina
VGG-11
redujimos
mitad
alto
ancho
bloque
alcanzando
finalmente
alto
ancho
aplanar
representaciones
capas
densas
red
procesen
paper
VGG
describió
variantes
red
convertido
norma
proponer
familias
redes
compromisos
velocidad
precisión
introducir
arquitectura
Entrenamiento
VGG-11
computacionalmente
exigente
AlexNet
construimos
red
menor
cantidad
canales
suficiente
entrenamiento
Fashion-MNIST
Nuevamente
observe
estrecha
coincidencia
validación
pérdida
entrenamiento
sugiere
pequeña
cantidad
sobreajuste
Redes
Capas
Densas
NiN
LeNet
AlexNet
VGG
comparten
patrón
diseño
común
extraer
características
explotan
estructura
espacial
secuencia
convoluciones
capas
pooling
postprocesar
representaciones
capas
densas
mejoras
LeNet
AlexNet
VGG
radican
principalmente
redes
posteriores
amplían
profundizan
módulos
diseño
plantea
desafíos
capas
densas
arquitectura
consumen
cantidad
parámetros
ejemplo
modelo
simple
VGG-11
requiere
matriz
monstruosa
25088
4096
ocupando
400
MB
RAM
precisión
simple
float32
impedimento
significativo
computación
particular
dispositivos
móviles
embebidos
teléfonos
móviles
actuales
gama
alta
GB
RAM
momento
inventó
VGG
orden
magnitud
menor
iPhone
4S
512
MB
hubiera
difícil
justificar
gastar
memoria
clasificador
imágenes
igualmente
imposible
agregar
capas
densas
red
aumentar
grado
linealidad
destruiría
estructura
espacial
requeriría
potencialmente
memoria
bloques
Network
in
Network
NiN
ofrecen
alternativa
capaz
resolver
problemas
estrategia
simple
propusieron
base
idea
simple
convoluciones
1×1
agregar
linealidades
locales
activaciones
canal
global
average
pooling
resumir
información
ubicaciones
capa
representación
Bloques
NiN
Recuerde
clase
discutimos
entradas
salidas
capas
convolucionales
consisten
tensores
dimensiones
ejes
correspondientes
ejemplo
canal
altura
ancho
Recuerde
entradas
salidas
capas
densas
suelen
tensores
bidimensionales
correspondientes
ejemplo
característica
convolutioninput.shape
batchsize
channels
heigth
width
linearinput.shape
batchsize
feature_dim
idea
NiN
aplicar
capa
densa
ubicación
píxel
alto
ancho
convolución
pensar
capa
densa
actúa
forma
independiente
canales
ubicación
píxel
figura
ilustra
principales
diferencias
estructurales
VGG
NiN
bloques
diferencia
bloques
NiN
convolución
inicial
seguida
convoluciones
VGG
retiene
convoluciones
necesitamos
capa
densa
gigante
Modelo
NiN
NiN
tamaños
convolución
iniciales
AlexNet
propuso
tamaños
kernel
respectivamente
cantidad
canales
salida
coincide
AlexNet
bloque
NiN
capa
max-pooling
stride
tamaño
ventana
diferencia
significativa
NiN
AlexNet
VGG
NiN
evita
completo
capas
densas
lugar
NiN
utiliza
bloque
NiN
cantidad
canales
salida
cantidad
clases
etiquetas
seguido
capa
average
pooling
global
produce
vector
logits
diseño
reduce
significativamente
cantidad
parámetros
modelo
requeridos
expensas
aumento
potencial
tiempo
entrenamiento
Creamos
ejemplo
datos
forma
salida
bloque
Entrenamiento
Fashion-MNIST
entrenar
modelo
usando
optimizador
AlexNet
VGG
Redes
Múltiples
Ramas
GoogLeNet
2014
GoogLeNet
ganó
ImageNet
Challenge
usando
estructura
combinaba
fortalezas
NiN
repetición
bloques
VGG
cóctel
kernels
convolución
decirse
red
exhibe
clara
distinción
base
ingreso
datos
cuerpo
procesamiento
datos
cabeza
predicción
CNN
patrón
diseño
persistido
diseño
redes
profundas
base
dada
primeras
2-3
convoluciones
operan
imagen
Extraen
características
nivel
imágenes
subyacentes
seguido
cuerpo
bloques
convolucionales
Finalmente
cabeza
asigna
características
obtenidas
momento
problema
requerido
clasificación
segmentación
detección
seguimiento
cuestión
contribución
clave
GoogLeNet
diseño
cuerpo
red
Resolvió
problema
seleccionar
núcleos
convolución
ingeniosa
trabajos
intentaron
identificar
convolución
oscilaba
GoogLeNet
simplemente
concatenó
múltiples
ramas
convoluciones
continuación
presentamos
versión
ligeramente
simplificada
GoogLeNet
diseño
original
incluía
serie
trucos
estabilizar
entrenamiento
funciones
pérdida
intermedia
aplicadas
múltiples
capas
red
necesarios
disponibilidad
algoritmos
entrenamiento
mejorados
Bloques
Inception
bloque
convolucional
básico
GoogLeNet
llama
bloque
Inception
derivado
meme
we
need
to
go
deeper
película
Inception
muestra
figura
bloque
inicio
consta
ramas
paralelas
primeras
ramas
capas
convolucionales
tamaños
ventana
extraer
información
tamaños
espaciales
ramas
agregan
convolución
entrada
reducir
cantidad
canales
reduce
complejidad
modelo
cuarta
rama
utiliza
capa
agrupación
máxima
seguida
capa
convolucional
cambiar
cantidad
canales
ramas
utilizan
cantidad
adecuada
padding
entrada
salida
altura
anchura
Finalmente
salidas
rama
concatenan
dimensión
canal
comprenden
salida
bloque
hiperparámetros
tunear
bloque
Inception
número
canales
salida
capa
ganar
intuición
red
funciona
considere
combinación
filtros
Exploran
imagen
variedad
tamaños
filtro
significa
detalles
extensiones
reconocer
eficiente
filtros
tamaños
tiempo
asignar
cantidades
parámetros
filtros
Modelo
GoogLeNet
muestra
figura
GoogLeNet
pila
bloques
inception
organizados
grupos
max-pooling
average
pooling
global
cabeza
generar
estimaciones
base
bloque
similar
AlexNet
LeNet
implementar
GoogLeNet
pieza
pieza
Comencemos
base
bloque
utiliza
capa
convolucional
64
canales
módulo
utiliza
capas
convolucionales
capa
convolucional
64
canales
seguida
capa
convolucional
triplica
número
canales
punto
192
canales
tercer
módulo
conecta
bloques
Inception
completos
serie
número
canales
salida
bloque
Inception
equivale
relación
número
canales
salida
ramas
Logrando
reducimos
dimensiones
entrada
rama
respectivamente
llegar
canales
respectivamente
número
canales
salida
bloque
Inception
incrementa
arroja
proporción
necesitamos
reducir
número
dimensiones
intermedias
tercer
canal
escala
respectivamente
suficiente
produciendo
canales
respectivamente
capturado
argumentos
siguientes
constructores
bloques
Inception
cuarto
módulo
complicado
Conecta
bloques
Inception
serie
canales
salida
respectivamente
número
canales
asignados
sucursales
similar
tercer
módulo
rama
capa
convolucional
genera
cantidad
canales
seguida
rama
capa
convolucional
rama
capa
convolucional
cuarta
rama
capa
max-pooling
ramas
reducirán
número
canales
proporción
proporciones
ligeramente
bloques
Inception
quinto
módulo
bloques
Inception
canales
salida
número
canales
asignados
rama
módulos
cuarto
difiere
valores
específicos
Cabe
señalar
quinto
bloque
capa
salida
bloque
utiliza
capa
avg-pooling
global
cambiar
altura
ancho
canal
NiN.
Finalmente
convertimos
salida
matriz
bidimensional
seguida
capa
densa
cuyo
número
salidas
número
clases
etiquetas
Now
that
we
defined
all
blocks
b1
through
b5
it's
just
matter
of
assembling
them
all
into
full
network
modelo
GoogLeNet
computacionalmente
complejo
cantidad
hiperparámetros
relativamente
arbitrarios
términos
cantidad
canales
elegidos
cantidad
bloques
reducción
dimensionalidad
partición
relativa
capacidad
canales
etc.
momento
GoogLeNet
introdujo
herramientas
automáticas
definición
redes
exploración
diseños
disponibles
ejemplo
alturas
damos
sentado
framework
deep
learning
competente
capaz
inferir
automáticamente
dimensionalidades
tensores
entrada
momento
configuraciones
tenían
especificadas
explícitamente
experimentador
ralentizaba
experimentación
activa
herramientas
necesarias
exploración
automática
cambiando
experimentos
iniciales
consistían
medida
costosas
exploraciones
fuerza
bruta
algoritmos
genéticos
estrategias
similares
única
modificación
realizaremos
reducir
altura
ancho
entrada
224
96
tiempo
entrenamiento
razonable
Fashion-MNIST
simplifica
cálculo
Echemos
vistazo
cambios
forma
salida
distintos
módulos
Eficiencia
Computacional
característica
clave
GoogLeNet
realidad
barato
calcular
predecesores
tiempo
proporciona
precisión
marca
comienzo
diseño
red
deliberado
compensa
costo
entrenar
red
reducción
errores
marca
comienzo
experimentación
nivel
bloque
hiperparámetros
diseño
red
momento
totalmente
manual
cantidad
parámetros
capa
convolucional
kernel
canales
entrada
canales
salida
cantidad
operaciones
necesarias
cálculo
Entrenamiento
entrenamos
modelo
utilizando
conjunto
datos
Fashion-MNIST
transformamos
resolución
pixel
invocar
procedimiento
entrenamiento
Batch
Normalization
trabajamos
datos
realizamos
preprocesamiento
entrenamiento
opciones
relacionadas
preprocesamiento
datos
marcan
enorme
diferencia
resultados
finales
Generalmente
paso
trabajar
datos
reales
estandarizar
características
entrada
media
cero
varianza
unitaria
observaciones
estrategia
cambiar
escala
vectores
tengan
norma
posiblemente
media
cero
observación
funcionar
ejemplo
datos
sensores
espaciales
técnicas
preprocesamiento
beneficiosas
mantener
problema
estimación
controlado
estandarización
vectores
agradable
efecto
secundario
restringir
complejidad
funciones
actúan
Intuitivamente
estandarización
funciona
optimizadores
coloca
parámetros
priori
escala
similar
natural
preguntarse
paso
normalización
red
profunda
beneficioso
exactamente
razonamiento
condujo
invención
normalización
lotes
forma
útil
entenderlo
primo
normalización
capas
marco
unificado
lugar
MLP
CNN
típico
entrenamos
variables
capas
intermedias
ejemplo
salidas
transformación
afines
MLP
tomar
valores
magnitudes
variables
capas
entrada
salida
unidades
capa
tiempo
actualizaciones
parámetros
modelo
inventores
normalización
lotes
postularon
informalmente
desviación
distribución
tales
variables
dificultar
convergencia
red
Intuitivamente
podríamos
conjeturar
capa
activaciones
variables
100
mayores
capa
requerir
ajustes
compensatorios
tasas
aprendizaje
Solvers
adaptables
AdaGrad
Adam
Yogi
objetivo
abordar
punto
vista
optimización
ejemplo
agregando
aspectos
métodos
orden
alternativa
evitar
ocurra
problema
simplemente
normalización
adaptativa
redes
profundas
complejas
tienden
fáciles
sobreajustar
significa
regularización
vuelve
crítica
técnica
común
regularización
inyección
ruido
constituye
base
dropout
resultado
casualidad
normalización
lotes
transmite
beneficios
preprocesamiento
estabilidad
numérica
regularización
normalización
lotes
aplica
capas
individuales
opcionalmente
iteración
entrenamiento
normalizamos
entradas
normalización
lotes
restando
media
dividiendo
desviación
estándar
estiman
base
estadísticas
minilote
actual
continuación
aplicamos
coeficiente
escala
desplazamiento
recuperar
grados
libertad
perdidos
precisamente
normalización
basada
estadísticas
lotes
normalización
lotes
deriva
nombre
intentáramos
aplicar
normalización
lotes
minilotes
tamaño
podríamos
aprender
restar
medias
unidad
oculta
tomaría
valor
suponer
dedicando
sección
completa
normalización
lotes
minilotes
suficientemente
enfoque
resulta
eficaz
estable
conclusión
aplicar
normalización
lotes
elección
tamaño
lote
significativo
normalización
lotes
necesita
calibración
adecuada
podríamos
ajustarlo
Denote
minilote
entrada
normalización
lotes
caso
normalización
lotes
define
media
muestral
desviación
estándar
muestra
minilote
aplicar
estandarización
minilote
resultante
media
cero
varianza
unitaria
elección
varianza
unitaria
frente
número
mágico
elección
arbitraria
Recuperamos
grado
libertad
incluir
elemento
parámetro
escala
parámetro
desplazamiento
forma
parámetros
aprenderse
entrenamiento
modelo
magnitudes
variables
capas
intermedias
divergir
entrenamiento
normalización
lotes
centra
vuelve
escalar
forma
activa
media
tamaño
determinados
experiencia
práctica
confirma
analizar
cambio
escala
características
normalización
lotes
permitir
tasas
aprendizaje
agresivas
Calculamos
ecuación
agregamos
pequeña
constante
estimación
varianza
garantizar
intentemos
dividir
cero
casos
estimación
varianza
empírica
pequeña
desaparecer
estimaciones
contrarrestan
problema
escala
estimaciones
ruidosas
media
varianza
pensar
ruido
debería
problema
contrario
realmente
beneficioso
resulta
tema
recurrente
aprendizaje
profundo
razones
caracterizadas
teóricamente
fuentes
ruido
optimización
conducen
entrenamiento
rápido
sobreajuste
variación
actuar
forma
regularización
particular
arroja
luz
enigma
normalización
lotes
funciona
tamaños
minilotes
moderados
rango
tamaño
particular
minibatch
inyectar
cantidad
correcta
ruido
capa
términos
escala
términos
compensación
minilote
regulariza
estimaciones
estables
minilotes
pequeños
destruyen
señal
útil
alta
varianza
Explorar
dirección
considerando
tipos
alternativos
preprocesamiento
filtrado
conducir
tipos
efectivos
regularización
normalizar
modelo
entrenado
pensar
preferiríamos
conjunto
datos
completo
estimar
media
varianza
completa
entrenamiento
querríamos
imagen
clasifique
lote
resida
entrenamiento
cálculo
exacto
factible
variables
intermedias
ejemplos
datos
cambian
actualizamos
modelo
entrena
modelo
calcular
medias
varianzas
variables
capa
función
conjunto
datos
completo
práctica
estándar
modelos
emplean
normalización
lotes
capas
normalización
lotes
funcionan
entrenamiento
normalización
estadísticas
minilotes
predicción
normalización
estadísticas
conjuntos
datos
forma
parecen
comportamiento
capa
dropout
ruido
inyecta
entrenamiento
Capas
normalización
lotes
implementaciones
normalización
lotes
capas
densas
capas
convolucionales
ligeramente
diferencia
clave
normalización
lotes
capas
normalización
lotes
opera
minilote
completo
simplemente
ignorar
dimensión
lote
hicimos
introducir
capas
Capas
densas
aplicar
normalización
lotes
capas
densas
paper
original
insertó
normalización
lotes
transformación
lineal
función
activación
lineal
aplicaciones
posteriores
experimentaron
inserción
normalización
lotes
justo
funciones
activación
Denotando
entrada
capa
densa
transformación
lineal
parámetro
peso
parámetro
sesgo
activación
expresar
cálculo
salida
capa
densa
habilitada
normalización
lotes
Recuerde
media
varianza
calculan
minilote
aplica
transformación
Capas
convolucionales
similar
capas
convolucionales
aplicar
normalización
lotes
convolución
función
activación
lineal
diferencia
clave
normalización
lotes
capas
densas
aplicamos
operación
canal
ubicaciones
compatible
suposición
invariancia
traslación
condujo
convoluciones
asumimos
ubicación
específica
patrón
imagen
crítica
propósito
comprensión
Supongamos
minilotes
contienen
ejemplos
canal
salida
convolución
altura
anchura
capas
convolucionales
llevamos
cabo
normalización
lote
elementos
canal
salida
simultáneamente
recopilamos
valores
ubicaciones
espaciales
calcular
media
varianza
consecuencia
aplicamos
media
varianza
canal
normalizar
valor
ubicación
espacial
canal
escala
parámetros
cambio
escalares
Normalización
lotes
predicción
mencionamos
anteriormente
normalización
lotes
generalmente
comporta
entrenamiento
predicción
ruido
media
varianza
muestra
surge
estimar
minilotes
deseables
entrenado
modelo
tengamos
lujo
calcular
estadísticas
normalización
lote
ejemplo
necesitemos
aplicar
modelo
predicción
general
entrenamiento
dataset
calcular
estimaciones
estables
variables
estadísticas
fijamos
momento
predicción
consecuencia
normalización
lotes
comporta
entrenamiento
momento
prueba
recuerda
dropout
exhibe
característica
Implementación
funciona
normalización
lotes
práctica
implementamos
cero
continuación
crear
capa
BatchNorm
propiamente
dicha
capa
mantendrá
parámetros
adecuados
escala
gamma
desplazamiento
beta
actualizarán
transcurso
entrenamiento
capa
mantendrá
medias
móviles
medias
varianzas
posterior
predicción
modelo
Dejando
detalles
algorítmicos
patrón
diseño
subyace
implementación
capa
general
definimos
matemáticas
función
separada
digamos
batch_norm
integramos
funcionalidad
capa
personalizada
cuyo
código
ocupa
principalmente
cuestiones
administración
mover
datos
contexto
dispositivo
correcto
asignar
inicializar
variable
requerida
seguimiento
medias
móviles
media
varianza
etc.
patrón
permite
clara
separación
matemáticas
código
repetitivo
conveniencia
preocupamos
inferir
automáticamente
forma
entrada
necesitamos
especificar
número
características
momento
frameworks
modernos
deep
learning
ofrecen
detección
automática
tamaño
forma
API
normalización
lotes
alto
nivel
práctica
usaremos
lugar
LeNet
normalización
lotes
aplicar
BatchNorm
contexto
continuación
aplicamos
modelo
LeNet
tradicional
Recuerde
aplica
normalización
lotes
capas
convolucionales
capas
densas
funciones
activación
correspondientes
Implementación
concisa
lugar
clase
BatchNorm
acabamos
definir
directamente
clase
BatchNorm
definida
torch
código
ve
prácticamente
idéntico
implementación
necesitamos
proporcionar
argumentos
adicionales
obtener
dimensiones
correctas
Redes
Conexiones
Residuales
ResNet
medida
diseñamos
redes
profundas
vuelve
imperativo
comprender
agregar
capas
aumentar
complejidad
expresividad
red
importante
capacidad
diseñar
redes
agregar
capas
redes
estrictamente
expresivas
lugar
simplemente
progreso
necesitamos
matemáticas
Familias
funciones
Considere
familia
funciones
alcanzar
arquitectura
red
específica
tasas
aprendizaje
configuraciones
hiperparámetros
conjunto
parámetros
p.
ej.
pesos
sesgos
obtener
entrenamiento
conjunto
datos
adecuado
Supongamos
función
realmente
gustaría
encontrar
encontrarla
general
tendremos
tanta
suerte
lugar
intentaremos
encontrar
apuesta
ejemplo
conjunto
datos
características
etiquetas
podríamos
intentar
encontrarlo
resolviendo
problema
optimización
regularización
controlar
complejidad
lograr
consistencia
tamaño
datos
entrenamiento
generalmente
conduce
mejores
razonable
suponer
diseñamos
arquitectura
poderosa
deberíamos
llegar
resultado
palabras
esperaríamos
garantía
suceda
ilustra
figura
familias
funciones
anidadas
clase
función
acerca
función
ejemplo
izquierda
figura
cerca
aleja
garantía
aumentar
complejidad
reducir
distancia
clases
funciones
anidadas
derecha
figura
evitar
problema
mencionado
familias
funciones
anidadas
familias
funciones
contienen
pequeñas
garantiza
aumentarlas
aumenta
estrictamente
expresivo
red
redes
neuronales
profundas
entrenar
capa
recién
agregada
función
identidad
modelo
efectivo
modelo
original
corazón
ResNet
red
residual
propuesta
2015
idea
capa
adicional
debería
contener
fácilmente
función
identidad
elementos
consideraciones
profundas
llevaron
solución
sorprendentemente
simple
bloque
residual
ResNet
ganó
Desafío
ImageNet
2015
diseño
profunda
influencia
construir
redes
neuronales
profundas
ejemplo
transformers
utilizan
apilar
capas
redes
eficiente
utiliza
redes
neuronales
basadas
grafos
concepto
básico
utilizado
ampliamente
visión
artificial
Bloques
Residuales
Centrémonos
local
red
neuronal
muestra
figura
entrada
denotada
supongamos
mapeo
subyacente
deseado
obtener
aprendizaje
usará
entrada
función
activación
superior
izquierda
cuadro
línea
punteada
aprender
directamente
mapeo
cambio
derecha
porción
cuadro
línea
punteada
necesita
aprender
mapeo
residual
bloque
residual
deriva
nombre
mapeo
identidad
mapeo
subyacente
deseado
mapeo
residual
establece
fácil
aprender
necesitamos
empujar
pesos
sesgos
capa
superior
cuadro
línea
puntos
cero
figura
derecha
ilustra
bloque
residual
ResNet
línea
continua
entrada
capa
operador
suma
denomina
conexión
residual
texto
negrita(o
conexión
acceso
directo
bloques
residuales
entradas
propagarse
rápido
conexiones
residuales
capas
bloque
residual
considerar
caso
especial
bloque
Inception
múltiples
ramas
ramas
mapeo
identidad
ResNet
diseño
completo
capa
convolucional
3×3
VGG
bloque
residual
capas
convolucionales
3×3
número
canales
salida
capa
convolucional
capa
normalización
lotes
función
activación
ReLU
saltamos
operaciones
convolución
agregamos
entrada
directamente
función
activación
ReLU
tipo
diseño
requiere
salida
capas
convolucionales
tengan
forma
entrada
puedan
sumarse
cambiar
cantidad
canales
debemos
introducir
capa
convolucional
adicional
transformar
entrada
forma
deseada
operación
suma
Echemos
vistazo
código
abajo
veamos
situación
entrada
salida
forma
necesita
convolución
opción
reducir
mitad
altura
ancho
salida
aumentamos
número
canales
salida
caso
convoluciones
use_1x1conv
True
resulta
útil
principio
bloque
ResNet
reducir
dimensionalidad
espacial
constrides=2
ResNet
primeras
capas
ResNet
GoogLeNet
capa
convolucional
64
canales
salida
stride
seguida
capa
max-pooling
stride
diferencia
capa
batch
normalization
agregada
capa
convolucional
ResNet
GoogLeNet
utiliza
módulos
compuestos
bloques
Inception
ResNet
utiliza
módulos
formados
bloques
residuales
utiliza
bloques
residuales
número
canales
salida
número
canales
módulo
número
canales
entrada
utilizado
capa
max_pooling
stride
necesario
reducir
altura
ancho
bloque
residual
módulos
subsiguientes
duplica
número
canales
módulo
reducen
mitad
altura
anchura
agregamos
módulos
ResNet
utilizan
bloques
residuales
módulo
GoogLeNet
agregamos
capa
avg-pooling
global
seguida
salida
capa
densa
capas
convolucionales
módulo
excluyendo
capa
convolucional
capa
convolucional
capa
densa
18
capas
modelo
conoce
comúnmente
ResNet-18
configurar
números
canales
bloques
residuales
módulo
crear
modelos
ResNet
profundo
ResNet-152
152
capas
arquitectura
principal
ResNet
similar
GoogLeNet
estructura
ResNet
simple
fácil
modificar
factores
resultado
rápido
generalizado
ResNet
entrenar
ResNet
observemos
cambia
forma
entrada
módulos
ResNet
arquitecturas
anteriores
resolución
disminuye
cantidad
canales
aumenta
punto
capa
avg-pooling
global
agrega
features
Entrenamiento
Entrenamos
ResNet
conjunto
datos
Fashion-MNIST
ResNet
arquitectura
poderosa
flexible
ResNeXt
desafíos
diseño
ResNet
equilibrio
linealidad
dimensionalidad
bloque
determinado
podríamos
agregar
linealidad
aumentando
número
capas
aumentando
ancho
convoluciones
estrategia
alternativa
aumentar
número
canales
transportar
información
bloques
Desafortunadamente
viene
penalización
cuadrática
costo
computacional
ingerir
canales
emitir
canales
proporcional
inspirarnos
bloque
Inception
GoogLeNet
información
fluye
bloque
grupos
separados
aplicar
idea
múltiples
grupos
independientes
bloque
ResNet
condujo
diseño
ResNeXt
2017
diferencia
mezcla
heterogénea
transformaciones
Inception
ResNeXt
adopta
transformación
ramas
minimizando
necesidad
tuneo
manual
rama
Dividir
convolución
canales
grupos
tamaño
generando
salidas
tamaño
llama
apropiadamente
convolución
agrupada
costo
computacional
proporcionalmente
reduce
rápido
cantidad
parámetros
necesarios
generar
salida
reduce
matriz
matrices
pequeñas
tamaño
nuevamente
reducción
asumimos
divisibles
único
desafío
diseño
intercambia
información
grupos
bloque
ResNeXt
figura
corrige
maneras
convolución
agrupada
kernel
intercala
convoluciones
cumple
doble
función
volver
cambiar
número
canales
beneficio
pagamos
costo
kernels
arreglárnoslas
costo
núcleos
Similar
implementación
bloque
residual
ResNet
conexión
residual
reemplazar
convolución
figura
derecha
proporciona
resumen
conciso
bloque
red
resultante
implementación
clase
ResNeXtBlock
toma
argumento
groups
botchannels
canales
intermedios
cuello
botella
necesitamos
reducir
altura
ancho
representación
agregamos
stride
configurando
use1x1conv
True
strides=2
completamente
análogo
ResNetBlock
discutido
anteriormente
ejemplo
use1x1conv
False
strides=1
entrada
salida
forma
Alternativamente
configurar
use1x1conv
True
strides=2
reduce
mitad
altura
ancho
salida
Redes
Densamente
Conectadas
ResNet
cambió
significativamente
visión
parametrizar
funciones
redes
profundas
DenseNet
red
convolucional
densa
punto
extensión
lógica
DenseNet
caracteriza
patrón
conectividad
capa
conecta
capas
anteriores
operación
concatenación
lugar
operador
suma
ResNet
preservar
reutilizar
características
capas
anteriores
entender
llegar
tomemos
pequeño
desvío
matemáticas
ResNet
DenseNet
Recuerde
expansión
Taylor
funciones
punto
escribir
punto
clave
descompone
función
términos
orden
alto
similar
ResNet
descompone
funciones
ResNet
descompone
término
lineal
simple
lineal
complejo
pasaría
quisiéramos
capturar
necesariamente
agregar
información
allá
términos
soluciones
DenseNet
muestra
figura
diferencia
clave
ResNet
DenseNet
caso
salidas
concatenan
lugar
agregarse
resultado
realizamos
mapeo
valores
aplicar
secuencia
funciones
compleja
funciones
combinan
MLP
reducir
nuevamente
cantidad
características
términos
implementación
simple
lugar
agregar
términos
concatenamos
nombre
DenseNet
surge
gráfico
dependencia
variables
vuelve
denso
capa
cadena
densamente
conectada
capas
anteriores
componentes
principales
componen
DenseNet
bloques
densos
capas
transición
definen
concatenan
entradas
salidas
segundos
controlan
número
canales
expansión
dimensiones
altas
Bloques
densos
bloque
denso
consta
bloques
convolución
utiliza
número
canales
salida
propagación
directa
concatenamos
entrada
salida
bloque
convolución
dimensión
canal
DenseNet
utiliza
estructura
modificada
normalización
lotes
activación
convolución
ResNet
implementamos
estructura
bloque
convolución
ejemplo
definimos
instancia
DenseBlock
bloques
convolución
10
canales
salida
entrada
canales
obtendremos
salida
canales
número
canales
bloque
convolución
controla
crecimiento
número
canales
salida
relación
número
canales
entrada
conoce
tasa
crecimiento
capa
recibe
mapas
activación
capas
anteriores
red
delgada
compacta
número
canales
menor
eficiencia
computacional
eficiencia
memoria
eficiencia
computacional
bloque
denso
ventajas
Mejora
problema
desvanecimiento
gradientes
unir
capas
salida
bloque
conexión
directa
backpropagation
modificar
facilmente
primeras
capas
Imgur
Permite
diversificación
features
clasificador
features
niveles
complejidad
tomar
decisión
img
src="https://i.imgur.com
BJyUSHa.png
width="660
Capas
transición
bloque
denso
aumentará
número
canales
agregar
demasiados
conducirá
modelo
excesivamente
complejo
utiliza
capa
transición
controlar
complejidad
modelo
Reduce
número
canales
usando
convolución
reduce
mitad
altura
ancho
avg_pooling
stride
img
src="https://i.imgur.com
NrBmAso.png
width="660
Aplicamos
capa
transición
10
canales
salida
bloque
denso
ejemplo
reduce
número
canales
salida
10
reduce
mitad
alto
ancho
Modelo
DenseNet
continuación
construiremos
modelo
DenseNet
DenseNet
base
ResNet
capa
convolucional
única
max-pool
similar
módulos
compuestos
bloques
residuales
ResNet
DenseNet
bloques
densos
Similar
ResNet
establecer
cantidad
capas
convolucionales
utilizadas
bloque
denso
configuramos
modelo
ResNet-18
establecemos
número
canales
tasa
crecimiento
capas
convolucionales
bloque
denso
32
agregarán
128
canales
bloque
denso
ResNet
altura
ancho
reducen
módulo
bloque
residual
paso
capa
transición
reducir
mitad
altura
ancho
reducir
mitad
número
canales
similar
ResNet
capa
avg-pooling
global
capa
totalmente
conectada
conectan
producir
salida
Entrenamiento
usando
red
profunda
sección
reduciremos
altura
ancho
entrada
224
96
simplificar
cálculo
términos
conexiones
capas
diferencia
ResNet
entradas
salidas
suman
DenseNet
concatena
entradas
salidas
dimensión
canal
operaciones
concatenación
reutilizan
características
lograr
eficiencia
computacional
desafortunadamente
conducen
alto
consumo
memoria
GPU
resultado
aplicación
DenseNet
requerir
implementaciones
eficientes
memoria
aumentar
tiempo
entrenamiento
