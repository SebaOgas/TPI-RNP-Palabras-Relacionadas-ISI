href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/4PyTorchAvanzado/1modelospersonalizados.ipynb
target="_parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
Modelos
Personalizados
Pytorch
introducido
conceptos
básicos
machine
learning
avanzando
modelos
deep
learning
completamente
funcionales
llegar
lejos
rápido
recurrimos
Pytorch
framework
Deep
Learning
pasamos
alto
detalles
avanzados
funciona
clase
abriremos
telón
profundizaremos
componentes
clave
cómputo
deep
learning
construcción
modelos
acceso
parámetros
inicialización
diseño
capas
bloques
personalizados
lectura
escritura
modelos
disco
aprovechamiento
GPU
lograr
resultados
aceleraciones
espectaculares
conocimientos
llevarán
usuario
usuario
avanzado
brindándonos
herramientas
necesarias
aprovechar
beneficios
framework
maduro
aprendizaje
profundo
conservamos
flexibilidad
implementar
modelos
complejos
incluidos
inventemos
Bloques
introdujimos
redes
neuronales
enfocamos
modelos
lineales
salida
modelo
consta
neurona
neurona
toma
conjunto
entradas
genera
salida
escalar
correspondiente
conjunto
parámetros
asociados
actualizarse
optimizar
función
objetivo
interés
comenzamos
pensar
redes
múltiples
salidas
aprovechamos
aritmética
vectorizada
caracterizar
capa
completa
neuronas
neuronas
individuales
capas
toma
conjunto
entradas
genera
salida
escalar
correspondiente
conjunto
parámetros
asociados
actualizarse
optimizar
función
objetivo
interés
trabajamos
regresión
softmax
capa
modelo
posteriormente
introdujimos
MLP
podíamos
pensar
modelo
conservaba
estructura
básica
toma
conjunto
entradas
genera
salida
escalar
correspondiente
conjunto
parámetros
asociados
actualizar
optimizar
función
objetiva
interés
pensar
neuronas
capas
modelos
darnos
suficientes
abstracciones
seguir
negocio
Resulta
resulta
conveniente
hablar
componentes
capa
individual
pequeños
modelo
completo
mayoría
arquitecturas
populares
visión
artificial
Posee
cientos
capas
capas
consisten
patrones
repetidos
grupos
capas
Implementar
red
tipo
capa
volverse
tedioso
preocupación
hipotética
tales
patrones
diseño
comunes
práctica
Arquitecturas
similares
capas
organizan
patrones
repetitivos
omnipresentes
dominios
incluido
procesamiento
lenguaje
natural
implementar
redes
complejas
presentamos
concepto
red
neuronal
bloque
bloque
describir
capa
componente
consta
capas
modelo
completo
ventaja
trabajar
abstracción
bloques
combinar
artefactos
recursivamente
ilustra
numref
fig_blocks
definir
código
generar
bloques
complejidad
arbitraria
demanda
escribir
código
sorprendentemente
compacto
implementar
redes
neuronales
complejas
punto
vista
programación
bloque
representado
clase
subclase
definir
función
propagación
directa
transforme
entrada
salida
almacenar
parámetros
necesarios
bloques
requieren
parámetro
absoluto
Finalmente
bloque
poseer
función
retropropagación
fines
cálculo
gradientes
Afortunadamente
magia
escena
proporcionada
diferenciación
automática
introducida
numref
sec_autograd
definir
bloque
debemos
preocuparnos
parámetros
función
propagación
comenzar
revisamos
código
implementar
MLPs
numref
secmlpconcise
código
genera
red
capa
oculta
completamente
conectada
256
unidades
activación
ReLU
seguida
capa
salida
completamente
conectada
10
unidades
función
activación
ejemplo
construimos
modelo
instanciando
nn
Sequential
capas
orden
deberían
ejecutarse
pasadas
argumentos
resumen
nn
Sequential
define
tipo
especial
Módulo
clase
presenta
bloque
PyTorch
Mantiene
lista
ordenada
Módulos
constituyentes
capas
completamente
conectadas
instancia
clase
Lineal
subclase
Módulo
función
propagación
forward
notablemente
simple
encadena
bloque
lista
pasando
salida
entrada
invocando
modelos
construcción
net(X
obtener
resultados
realidad
abreviatura
net.call(X
bloque
personalizado
forma
fácil
desarrollar
intuición
funciona
bloque
implementar
implementar
bloque
personalizado
resumimos
brevemente
funcionalidad
básica
proporcionar
bloque
Ingerir
datos
entrada
argumentos
función
propagación
directa
Generar
salida
función
propagación
directa
devuelva
valor
salida
forma
entrada
ejemplo
capa
completamente
conectada
modelo
ingiere
entrada
dimensión
20
devuelve
salida
dimensión
256
Calcular
gradiente
salida
entrada
acceder
función
backpropagation
general
sucede
automáticamente
Almacenar
proporcionar
acceso
parámetros
necesarios
ejecutar
cálculo
propagación
directa
Inicializar
parámetros
modelo
necesario
fragmento
codificamos
bloque
cero
correspondiente
MLP
capa
oculta
256
unidades
ocultas
capa
salida
10
dimensiones
clase
MLP
continuación
hereda
clase
Module
representa
bloque
Confiaremos
medida
funciones
clase
principal
proporcionando
constructor
función
init
Python
función
forward
propagación
Centrémonos
función
propagación
directa
toma
entrada
calcula
representación
oculta
función
activación
aplicada
emite
logits
implementación
MLP
ambas
capas
variables
instancia
razonable
imagine
instanciar
MLP
net1
net2
entrenarlos
datos
Naturalmente
esperaríamos
representar
modelos
aprendidos
Creamos
instancia
capas
MLP
constructor
posteriormente
invocamos
capas
llamada
función
propagación
directa
detalles
clave
función
init
personalizada
invoca
función
init
clase
principal
super().init
ahorrándonos
dolor
volver
establecer
código
repetitivo
aplicable
mayoría
bloques
instanciamos
capas
densas
asignándolas
self.hidden
self.out
implementemos
operador
debemos
preocuparnos
función
retropropagación
inicialización
parámetros
sistema
generará
funciones
automáticamente
Probemos
virtud
clave
abstracción
bloques
versatilidad
subclase
bloque
crear
capas
clase
capa
completamente
conectada
modelos
completos
clase
MLP
componentes
complejidad
intermedia
Explotamos
versatilidad
siguientes
clases
cursos
tratamos
redes
neuronales
convolucionales
Bloque
Sequential
echar
vistazo
cerca
funciona
clase
Sequential
Recuerde
Sequential
diseñado
conectar
cadena
bloques
construir
MySequential
simplificado
necesitamos
definir
funciones
clave
función
agregar
bloques
lista
función
propagación
directa
pasar
entrada
cadena
bloques
orden
agregaron
clase
MySequential
ofrece
funcionalidad
clase
Sequential
predeterminada
método
init
agregamos
bloque
diccionario
ordenado
modules
pregunte
Module
posee
atributo
modules
lugar
simplemente
definir
lista
Python
resumen
principal
ventaja
modules
inicialización
parámetros
bloque
sistema
buscar
diccionario
modules
encontrar
subbloques
cuyos
parámetros
necesitan
inicializarse
invoca
función
propagación
directa
MySequential
bloque
agregado
ejecuta
orden
agregaron
volver
implementar
MLP
usando
clase
MySequential
MySequential
idéntico
código
escribimos
anteriormente
clase
Sequential
Ejecutando
código
función
propagación
directa
clase
Sequential
facilita
construcción
modelos
permitiéndonos
ensamblar
arquitecturas
definir
clase
arquitecturas
simples
cadenas
serializadas
requiera
flexibilidad
querremos
definir
bloques
ejemplo
podríamos
querer
ejecutar
flujos
control
Python
función
propagación
directa
podríamos
querer
operaciones
matemáticas
arbitrarias
basan
simplemente
capas
redes
neuronales
predefinidas
notado
operaciones
redes
actuado
activaciones
red
parámetros
queramos
incorporar
términos
resultado
capas
anteriores
parámetros
actualizables
llamamos
parámetros
constantes
Digamos
ejemplo
capa
calcule
función
entrada
parámetro
constante
especificada
actualiza
optimización
implementamos
clase
FixedHiddenMLP
modelo
FixedHiddenMLP
implementamos
capa
oculta
cuyos
pesos
self.rand_weight
inicializan
aleatoriamente
creación
instancias
constantes
peso
parámetro
modelo
retropropagación
actualiza
red
pasa
salida
capa
fija
capa
completamente
conectada
devolver
salida
modelo
inusual
Ejecutamos
ciclo
while
probando
condición
norma
dividiendo
vector
salida
satisfaga
condición
Finalmente
devolvimos
suma
entradas
X.
red
neuronal
estándar
realiza
operación
operación
particular
útil
tarea
mundo
real
objetivo
mostrarle
integrar
código
arbitrario
flujo
cálculos
red
neuronal
Capas
personalizadas
factor
éxito
aprendizaje
profundo
disponibilidad
amplia
gama
capas
componer
formas
creativas
diseñar
arquitecturas
adecuadas
amplia
variedad
tareas
ejemplo
investigadores
inventado
capas
específicamente
manejar
imágenes
texto
recorrer
datos
secuenciales
programación
dinámica
encontrará
inventará
capa
marco
aprendizaje
profundo
casos
crear
capa
personalizada
sección
mostramos
Capas
Parámetros
empezar
construimos
capa
personalizada
parámetros
clase
CenteredLayer
simplemente
resta
media
entrada
construirlo
simplemente
necesitamos
heredar
clase
base
Layer
implementar
función
forward
Verifiquemos
capa
funcione
previsto
alimentandola
datos
incorporar
capa
componente
construcción
modelos
complejos
verificación
cordura
adicional
enviar
datos
aleatorios
red
verificar
media
tratando
números
coma
flotante
número
pequeño
distinto
cero
cuantización
Capas
parámetros
definir
capas
simples
pasemos
definir
capas
parámetros
ajustar
entrenamiento
clase
nn
Parameter
crear
parámetros
brinda
funciones
básicas
mantenimiento
particular
rige
acceso
inicialización
compartido
guardado
carga
parámetros
modelo
forma
beneficios
necesitaremos
escribir
rutinas
serialización
personalizadas
capa
personalizada
implementemos
versión
capa
densa
Recuerde
capa
requiere
parámetros
representar
pesos
sesgo
implementación
función
activación
ReLU
prefabricada
capa
requiere
argumentos
entrada
in_units
units
denotan
número
entradas
salidas
respectivamente
continuación
instanciamos
clase
MyLinear
acceder
parámetros
construir
modelos
usando
capas
personalizadas
Gestión
parámetros
elegido
arquitectura
establecido
hiperparámetros
procedemos
ciclo
entrenamiento
objetivo
encontrar
valores
parámetros
minimicen
función
pérdida
entrenamiento
necesitaremos
parámetros
futuras
predicciones
desearemos
extraer
parámetros
reutilizarlos
contexto
guardar
modelo
disco
ejecutarse
software
examinarlo
esperanza
obtener
comprensión
científica
mayoría
podremos
ignorar
detalles
esenciales
declaran
manipulan
parámetros
confiando
marcos
aprendizaje
profundo
trabajo
pesado
alejamos
arquitecturas
apiladas
capas
estándar
necesitaremos
entrar
maleza
declarar
manipular
parámetros
sección
cubrimos
Acceso
parámetros
depuración
diagnóstico
visualizaciones
Inicialización
parámetros
Compartir
parámetros
componentes
modelo
Comenzamos
enfocándonos
MLP
capa
oculta
Acceso
parámetros
Comencemos
acceder
parámetros
modelos
conoce
modelo
define
clase
Sequential
acceder
capa
indexando
modelo
lista
parámetros
capa
convenientemente
ubicados
atributo
inspeccionar
parámetros
capa
completamente
conectada
salida
cosas
importantes
capa
completamente
conectada
contiene
parámetros
correspondientes
pesos
sesgos
capa
respectivamente
almacenan
flotadores
precisión
simples
float32
nombres
parámetros
permiten
identificar
forma
única
parámetros
capa
red
contiene
cientos
capas
Indexado
parámteros
parámetro
representa
instancia
clase
parámetro
útil
parámetros
debemos
acceder
valores
numéricos
subyacentes
maneras
simples
generales
código
extrae
sesgo
capa
red
neuronal
devuelve
instancia
clase
Parameter
accede
valor
parámetro
parámetros
objetos
complejos
contienen
valores
gradientes
información
adicional
necesitamos
solicitar
valor
explícitamente
valor
parámetro
permite
acceder
gradiente
invocado
retropropagación
red
inicial
parámetros
necesitamos
operaciones
parámetros
acceder
volverse
tedioso
situación
volverse
especialmente
difícil
manejar
trabajamos
bloques
complejos
ejemplo
bloques
anidados
tendríamos
recurrir
árbol
extraer
parámetros
subbloque
continuación
demostramos
acceso
parámetros
capa
densa
frente
acceso
capas
proporciona
forma
acceder
parámetros
red
Recopilación
parámetros
bloques
anidados
Veamos
funcionan
convenciones
nomenclatura
parámetros
anidamos
bloques
definimos
función
produce
bloques
fábrica
bloques
decirlo
combinamos
bloques
diseñado
red
veamos
organizada
capas
anidadas
jerárquicamente
acceder
indexáramos
listas
anidadas
ejemplo
acceder
bloque
principal
sub-bloque
sesgo
capa
Inicialización
parámetros
acceder
parámetros
veamos
inicializarlos
correctamente
marco
aprendizaje
profundo
proporciona
inicializaciones
aleatorias
predeterminadas
capas
inicializar
pesos
protocolos
marco
proporciona
protocolos
utilizados
permite
crear
inicializador
personalizado
forma
predeterminada
PyTorch
inicializa
matrices
peso
sesgo
uniforme
dibujando
rango
calcula
dimensión
entrada
salida
módulo
nn.init
PyTorch
proporciona
variedad
métodos
inicialización
preestablecidos
Inicialización
integrada
Comencemos
llamando
inicializadores
incorporados
código
inicializa
parámetros
ponderación
variables
aleatorias
gaussianas
desviación
estándar
0,01
parámetros
sesgo
borran
cero
inicializar
parámetros
valor
constante
ejemplo
aplicar
inicializadores
bloques
ejemplo
continuación
inicializamos
capa
inicializador
Xavier
inicializamos
capa
valor
constante
42
Inicialización
personalizada
marco
aprendizaje
profundo
proporciona
métodos
inicialización
necesitamos
ejemplo
definimos
inicializador
parámetro
peso
usando
distribución
extraña
Nuevamente
implementamos
función
my_init
aplicar
net
opción
configurar
parámetros
directamente
Parámetros
vinculados
compartir
parámetros
capas
Veamos
elegancia
continuación
asignamos
capa
densa
parámetros
específicamente
establecer
capa
ejemplo
muestra
parámetros
capa
vinculados
iguales
representados
tensor
exacto
cambiamos
parámetros
cambia
pregunte
parámetros
vinculados
sucede
gradientes
parámetros
modelo
contienen
gradientes
gradientes
capa
oculta
capa
oculta
suman
retropropagación
Archivos
discutimos
procesar
datos
construir
entrenar
probar
modelos
aprendizaje
profundo
momento
suerte
estaremos
suficientemente
contentos
modelos
aprendidos
querremos
guardar
resultados
posterior
contextos
predicciones
producción
ejecuta
proceso
entrenamiento
práctica
guardar
periódicamente
resultados
intermedios
puntos
control
garantizar
perdamos
cómputo
tropezamos
enchufe
servidor
hora
aprender
cargar
almacenar
vectores
peso
individuales
modelos
completos
sección
aborda
temas
Cargando
guardando
tensores
tensores
individuales
invocar
directamente
funciones
load
save
leerlos
escribirlos
respectivamente
Ambas
funciones
requieren
proporcionemos
nombre
save
requiere
entrada
variable
guardará
volver
leer
datos
archivo
almacenado
cargarlos
vuelta
memoria
almacenar
lista
tensores
volver
leerlos
memoria
escribir
leer
diccionario
mapea
cadenas
tensores
conveniente
leer
escribir
pesos
modelo
Cargar
guardar
parámetros
modelo
Guardar
vectores
peso
individuales
tensores
útil
vuelve
tedioso
guardar
cargar
modelo
completo
tengamos
cientos
grupos
parámetros
esparcidos
partes
razón
framework
deep
learning
proporciona
funcionalidades
integradas
cargar
guardar
redes
enteras
detalle
importante
guarda
parámetros
modelo
modelo
ejemplo
MLP
capas
debemos
especificar
arquitectura
separado
razón
modelos
contener
código
arbitrario
serializar
forma
natural
restablecer
modelo
necesitamos
generar
arquitectura
código
cargar
parámetros
disco
Empecemos
familiar
MLP
continuación
almacenamos
parámetros
modelo
archivo
nombre
mlp.params
recuperar
modelo
instanciamos
clon
modelo
MLP
original
lugar
inicializar
aleatoriamente
parámetros
modelo
leemos
directamente
parámetros
almacenados
archivo
ambas
instancias
parámetros
modelo
resultado
computacional
entrada
debería
Verifiquemos
