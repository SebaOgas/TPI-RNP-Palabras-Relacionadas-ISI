href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/4PyTorchAvanzado/2pytorchgpu.ipynb
target="_parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
GPU
Deep
Learning
habrán
escuchado
entrenar
modelos
deep
learning
GPU
rápido
CPU
ocasiones
habrán
escuchado
elementos
claves
permitido
revolución
inteligencia
artificial
surja
desarrollo
aceleración
hardware
diferencia
CPU
GPU
funcionan
procesadores
internamente
modelos
deep
learning
entrenen
rápido
utilizamos
clase
meter
mundo
hardware
entender
desarrollar
intuición
CPUs
computadora
Unidad
Procesamiento
Central
CPU
siglas
inglés
encargada
ejecución
instrucciones
procesadores
ideados
propósito
general
estructura
busca
versatilidad
lugar
eficiencia
chip
tareas
diversas
procesadores
texto
navegador
internet
videojuegos
editores
video
etc.
CPUs
diseñadas
siguiendo
Arquitectura
Von
Neumann
propone
cuente
siguientes
elementos
básicos
Unidad
Aritmética-Lógica
ALU
circuito
lógico
implementa
operaciones
aritmética
binaria
típicamente
operaciones
básicas
representaciones
binario
complemento
lógicas
típicamente
AND
OR
EXOR
NOT
bit
bit
habitual
implemente
operaciones
desplazamiento
rotación
bits
Unidad
Control
circuito
secuencial
implementa
denominado
ciclo
instrucción
permitiendo
acceder
instrucción
programa
leer
operandos
efectuar
operación
indicada
ALU
guardar
resultado
Conjunto
Registros
serie
posiciones
especiales
memoria
ubicadas
físicamente
CPU
permiten
acceso
operandos
lugares
almacenamiento
resultados
veloz
estuvieran
sistema
memoria
normal
registros
interno
CPU
precisamente
Unidad
Control
accesibles
utilizables
programador
Conjunto
Instrucciones
conjunto
instrucciones
especificación
detalla
instrucciones
unidad
central
procesamiento
entender
ejecutar
conjunto
comandos
implementados
diseño
particular
CPU
instrucciones
implementadas
hardware
CPU
diversas
combinaciones
transistores
Procesadores
diseños
internos
compartir
conjunto
instrucciones
ejemplo
Intel
Pentium
AMD
Athlon
implementan
versiones
idénticas
conjunto
instrucciones
x86
diseños
mayoría
instrucciones
conjunto
existente
agrupan
siguientes
categorías
Transferencia
datos
Copian
datos
origen
destino
modificar
origen
normalmente
afectar
flags
indicadores
condición
Instrucciones
lógicas
aritméticas
efectuadas
ALU
suelen
cambiar
flags
indicadores
condición
Instrucciones
comparación
comparan
datos
fijandose
generalmente
Instrucciones
control
Permiten
modificar
secuencia
normal
ejecución
programa
hacerse
salto
condicional
relativo
absoluto
Instrucciones
entrada
salida
instrucciones
transferencia
origen
destino
flujo
puerto
dispositivo
entrada
salida
Teniendo
conjunto
instrucciones
CPU
debería
seguir
siguientes
pasos
calcular
ejemplo
función
ReLU
Leer
valor
entrada
memoria
Comparo
valor
entrada
devolver
valor
entrada
devolver
instrucciones
demora
ciclo
reloj
deberíamos
gastar
ciclos
calcular
función
pasaría
quisiéramos
optimizar
asegurarnos
función
ReLU
ejecute
rápidamente
calcularla
entrenamiento
red
única
opción
cambiar
diseño
electrónico
CPU
instrucción
ReLU
esté
incluída
conjunto
función
ReLU
estaría
programada
combinación
transistores
ejecutaría
tiempo
reloj
ejemplo
quedar
CPU
tareas
diversas
relativamente
sencillo
diseñar
procesadores
eficientes
propósitos
específicos
Analogía
CPU
MLP
pensar
CPU
procesadores
MLP
arquitecturas
redes
neuronales
matemáticamente
demostrado
MLP
capaz
aproximar
función
aproximar
tipo
problema
puedan
existir
arquitecturas
eficientes
resolver
problemas
específicos
GPUs
Unidades
Procesamiento
Gráfico
GPU
siglas
inglés
procesador
propósito
específico
conjunto
operaciones
optimizado
procesamiento
gráfico
ejemplo
GPU
actuales
optimizadas
cálculo
valores
coma
flotante
predominantes
gráficos
3D.
aplicaciones
gráficas
conllevan
alto
grado
paralelismo
inherente
unidades
fundamentales
cálculo
vértices
píxeles
completamente
independientes
últimamente
CPUs
apostando
arquitecturas
multinúcleo
órden
16
32
núcleos
chip
GPU
orden
miles
núcleos
iguales
núcleos
GPUs
nivel
circuitos
sencillos
especializan
operaciones
aritméticas
ALUs
CPUs
conjunto
operaciones
reducidas
Paralelismo
sección
vemos
GPUs
diseñadas
núcleos
realicen
tareas
simples
paralelo
llegar
útil
ejemplo
operaciones
algebraicas
operaciones
matrices
celda
resultado
operación
matricial
independiente
celdas
asignar
núcleo
GPU
resuelva
celda
rearmar
matriz
resultados
estudiaron
Álgebra
Lineal
resultará
conocido
concepto
transformaciones
visuales
cuerpo
expresar
operaciones
matriciales
GPUs
cuyo
propósito
principal
históricamente
ocuparse
procesamiento
gráfico
diseñadas
ejecutar
increíblemente
rápida
operaciones
básicas
Álgebra
Lineal
visto
curso
operaciones
esenciales
red
neuronal
expresar
operaciones
matriciales
Deep
Learning
tambbién
beneficiar
aptitudes
GPUs
paralelización
CUDA
GPGPUs
CUDA
siglas
Compute
Unified
Device
Architecture
referencia
plataforma
computación
paralelo
incluyendo
compilador
conjunto
herramientas
desarrollo
creadas
Nvidia
permiten
programadores
variación
lenguaje
programación
codificar
algoritmos
GPU
Nvidia
wrappers
Python
Fortran
Java
C++
lanzamiento
2007
CUDA
permitido
GPUs
transformen
procesadores
propósito
general
General
Purpose
Graphic
Processing
Units
Deep
Learning
catapultó
paper
2009
mostraba
entrenar
redes
GPUs
tardaba
70
CPUs
sección
comenzamos
discutir
ejecutar
código
PyTorch
GPUs
CUDA
Específicamente
discutiremos
GPU
NVIDIA
cálculos
asegúrese
GPU
NVIDIA
instalada
descargue
controlador
NVIDIA
CUDA
https://developer.nvidia.com/cuda-downloads
siga
indicaciones
establecer
ruta
adecuada
realizados
preparativos
comando
nvidia-smi
información
tarjeta
gráfica
PyTorch
tensor
device
llamamos
contexto
forma
predeterminada
variables
cálculos
asociados
asignado
CPU
general
contextos
GPU
cosas
complicarse
implementamos
trabajos
servidores
asignar
tensores
contextos
inteligente
minimizar
tiempo
dedicado
transferir
datos
dispositivos
ejemplo
entrenar
redes
neuronales
servidor
GPU
normalmente
preferimos
parámetros
modelo
vivan
GPU
continuación
debemos
confirmar
versión
GPU
PyTorch
instalada
instalada
versión
CPU
PyTorch
debemos
desinstalarla
ejemplo
use
comando
pip
uninstall
torch
instale
versión
PyTorch
correspondiente
versión
CUDA
Suponiendo
instalado
CUDA
10.0
instalar
versión
PyTorch
compatible
CUDA
10.0
pip
install
torch-cu100
Atributo
device
especificar
dispositivos
CPU
GPU
almacenamiento
cálculo
defecto
tensores
crean
memoria
principal
CPU
calcularlos
PyTorch
CPU
GPU
indicar
torch.device('cpu
torch.device('cuda
Cabe
señalar
dispositivo
cpu
significa
CPU
físicas
memoria
significa
cálculos
PyTorch
intentarán
núcleos
CPU
dispositivo
gpu
representa
tarjeta
memoria
correspondiente
GPU
torch.device(f'cuda:{i
representar
GPU
comienza
gpu:0
gpu
equivalentes
consultar
número
GPU
disponibles
definimos
funciones
convenientes
permiten
ejecutar
código
GPU
solicitadas
Tensores
GPU
defecto
tensores
crean
CPU
consultar
dispositivo
tensor
importante
queramos
operar
tensor
dispositivo
ejemplo
sumamos
tensores
debemos
asegurarnos
argumentos
vivan
dispositivo
contrario
framework
sabría
almacenar
resultado
decidir
cálculo
Almacenamiento
GPU
formas
almacenar
tensor
GPU
ejemplo
especificar
dispositivo
almacenamiento
crear
tensor
continuación
creamos
variable
tensor
gpu
tensor
creado
GPU
consume
memoria
GPU
comando
nvidia-smi
memoria
GPU
general
debemos
asegurarnos
crear
datos
superen
límite
memoria
GPU
Asumiendo
tenés
GPUs
código
generar
tensor
aleatorio
GPU
Copiando
calcular
debemos
decidir
operación
ejemplo
muestra
figura
transferir
GPU
operación
sume
directamente
resultará
excepción
motor
tiempo
ejecución
sabría
encontrar
datos
dispositivo
falla
vive
CPU
debemos
mover
GPU
sumarlos
datos
GPU
sumarlos
Imagina
variable
vive
GPU
pasa
llamamos
Z.cuda(0
Devolverá
lugar
copia
asignar
memoria
Notas
margen
personas
GPU
aprendizaje
automático
esperan
rápidas
transferencia
variables
dispositivos
lenta
esté
100
seguro
lento
dejemos
framework
aprendizaje
profundo
simplemente
copia
automáticamente
bloquearse
dé
habías
escrito
código
lento
transferencia
datos
dispositivos
CPU
GPU
máquinas
lento
cálculo
paralelización
difícil
esperar
envíen
datos
reciban
continuar
operaciones
razón
operaciones
copia
tomarse
cuidado
regla
general
operaciones
pequeñas
peores
operación
operaciones
mejores
operaciones
individuales
intercaladas
código
sepa
caso
tales
operaciones
bloquearse
dispositivo
esperar
cosa
imprimimos
tensores
convertimos
tensores
formato
NumPy
datos
memoria
principal
marco
copiará
memoria
principal
generará
sobrecarga
transmisión
adicional
Redes
neuronales
GPU
similar
modelo
red
neuronal
especificar
device
código
coloca
parámetros
modelo
GPU
entrada
tensor
GPU
modelo
calculará
resultado
GPU
Permítanos
confirmar
parámetros
modelo
almacenan
GPU
