href="https://colab.research.google.com
github
institutohumai
cursos-python
blob
master
DeepLearning/2RedesDeUnaCapa/1regresionlineal.ipynb
target="parent"><img
src="https://colab.research.google.com
assets
colab-badge.svg
alt="Open
in
Colab"/></a
Redes
Neuronales
Capa
Regresión
lineal
cero
estás
acá
principio
sabés
regresión
lineal
regresión
lineal
forma
analizar
variables
dependen
regresión
lineal
relaciona
variables
independientes
features
variable
dependiente
objetivo
sumás
productos
cantidades
independientes
busca
obtener
variable
dependiente
Consideremos
llamada
fórmula
Dulong
formula
Dulong
resultado
experimental
surgido
analizar
energía
liberda
combustión
combustibles
fósiles
formula
predice
valor
energía
liberada
función
proporción
elemento
combustible
proporción
masa
carbono
combustible
proporción
hidrógeno
oxígeno
vale
combustible
líquido
vale
combustible
sólido
ejemplo
quemar
gas
metáno
obtiene
siguientes
proporciones
fórmula
devuelve
frente
valor
reportado
tablas
Nota
valores
provienen
Fórmula
metano
Masa
molar
metano
Masa
molar
carbono
Masa
molar
hidrógeno
hidrógenos
punto
comentario
discutir
resultado
termódinámica
química
señalar
regresión
lineal
técnica
usada
años
areas
variopintas
lejos
formula
Dulong
resultado
siglo
XIX
teniendo
utilidad
coeficientes
versión
presentado
corresponden
resultado
2016
única
diferencia
modelo
presentado
decidido
notación
one-hot
vectors
combustible
Siguiento
ejemplo
consideremos
estudiado
combustibles
gas
metano
alcohol
etílico
|||||gas|líquido|sólido|
|---|---|---|---|---|---|---|---|
metano|0.75|0.25|0.00|1|0|0|50.01|
alcohol
etílico
|0.52|0.13|0.35|0|1|0|26.70|
Notar
repersentar
agregación
combustible
usado
codificación
one
hot
vectors
columna
reportamos
valor
medido
laboratorio
Consideremos
valores
variables
independientes
pongamos
matriz
\begin{align
\left[\begin{array}{cccccc
0.75&0.25&0.00&1&0&0\
0.52&0.13&0.35&0&1&0
\end{array}\right
\end{align
Consideremos
vector
guardaremos
valores
reales
ground
truth
nuestos
datos
\begin{align
\left[\begin{array}{c
50.01\
26.7
\end{array}\right
\end{align
matriz
conoce
matriz
diseño
permite
guardar
información
ejemplos
quiesieramos
estudiar
matriz
propiedad
interesante
Condiremos
coeficientes
fórmula
Dulong
guardemoslos
vector
\begin{align
w^T
\left[\begin{array}{cccccc
38.2&84.9&-10.6125&0&-0.5&-0.62
\end{array}\right
\end{align
Consideremos
vector
\begin{align
\hat{y
\left[\begin{array}{cccccc
0.75&0.25&0.00&1&0&0\
0.52&0.13&0.35&0&1&0
\end{array}\right]\left[\begin{array}{c
38.2\84.9\-10.6125\0\-0.5\-0.62
\end{array}\right
\end{align
\begin{align
\hat{y
\left[\begin{array}{cc
49.87\
26.69
\end{array}\right
\end{align
guardar
datos
parametros
matrices
calcular
predicciones
mera
multiplicación
matrices
estimar
error
absoluto
predicción
valor
real
operación
matricial
tratar
calcular
varianza
valores
vector
suponiendo
modelo
dispersión
media
valore
reales
formula
analisis
señalar
neurona
artificial
fórmula
Dulong
regresión
lineal
serie
variables
independientes
influyen
resultado
variable
dependiente
caso
proporcion
átomos
agregación
combustible
sólido
líquido
gas
estructura
similar
neurona
recibe
estimulos
produce
única
respuesta
decimos
neurona
Minimizar
varianza
error
abosoluto
equivalente
pedir
ajuste
lineal
mínimos
cuadrados
entrenamiento
neurona
consistirá
encontrar
parametos
aplicarlos
datos
permitan
obtener
mejores
predicciones
acerque
valores
reales
contexto
encontrar
parametros
equivalente
minimizar
varianza
cantidad
minimizar
llamaremos
función
pérdida
elegido
caso
análisis
calor
liberado
combustible
función
constituyentes
señalar
principio
opera
técnica
general
aplicarse
monton
areas
lejos
podríamos
paso
alla
lugar
entrenar
neurona
dé
energía
liberada
podríamos
entrenar
neurona
dé
masa
molar
combustible
tercer
neurona
entregue
propiedad
combustible
ejemplo
índice
refracción
caso
neuronas
parámetros
deberían
almacenados
matriz
pesos
\begin{align
\hat{y
\left[\begin{array}{cccccc
0.75&0.25&0.00&1&0&0\
0.52&0.13&0.35&0&1&0
\end{array}\right]\left[\begin{array}{cc
38.2&1\84.9&1\-1.6125&1\0&-1\-0.5&1\-0.62&2
\end{array}\right
\end{align
\begin{align
\hat{y
\left[\begin{array}{cc
49.87&0\
26.69&2
\end{array}\right
\end{align
columna
corresponde
neurona
aprendió
devolver
propiedad
combustibles
Esperamos
descripto
entienda
neurona
red
capa
neuronas
consiste
encontrar
parametros
óptimos
problema
Presentando
pipeline
describiremos
proceso
general
entrenar
red
Carga
datos
Separación
datos
lotes
Inicialización
parámetros
Definición
modelo
Definición
función
pérdida
Definición
algoritmo
optimización
ejemplo
haremos
trabajar
datos
sintéticos
tomaremos
datos
generados
modelo
lineal
intención
múltiple
mostrar
ejemplo
permita
entender
significado
parámetros
modelo
estimaciones
modelo
sencillo
permita
analizar
paso
pipeline
punto
principal
motivo
general
frameworks
deep
learning
multiples
herramientas
permite
simplificar
pasos
común
necesitemos
ajustar
detalles
modelo
usaremos
sentido
reinventar
rueda
ayudar
entender
funcionan
herramientas
preexistentes
frameworks
usaremos
Dataset
Comencemos
modelo
lineal
sencillo
añadiremos
ruido
gaussiano
importante
dimensionalidad
features
etiquetas
graficar
etiqueta
features
comportamiento
lineal
Cargando
datos
entrenamiento
usando
mini-lotes
datos
conveniente
función
encarga
generar
minilotes
necesitemos
necesitamos
función
tome
matriz
diseño
tome
etiquetas
genere
lotes
entrenamiento
tamaño
Valores
iniciales
modelo
buscando
mínimos
fución
pérdida
elegir
iniciar
valor
optimizador
encargará
encontrar
mínimos
adecuados
parametros
iniciales
condiciones
empezar
entrenar
red
buscar
parámetros
representen
comportamiento
datos
Debemos
recordar
descenso
gradiente
herramientas
diferenciación
automática
problemas
Definiendo
modelo
caso
modelo
análogo
usado
generar
datos
Definiendo
función
pérdida
regresión
lineal
minimizamos
cantidad
llamada
mínimos
cuadrados
Definiendo
algoritmo
optimización
continuación
mostramos
pequeño
ejemplo
funciona
descenso
gradiente
estocástico
Entrenamiento
continuación
esbozamos
ciclo
entrenamiento
Iniciamos
parametros
Repetimos
concluir
Calculamos
función
pérdida
Calculamos
gradiente
minilotes
actualizamos
parámetros
Nota
pasos
pasos
función
sdg
definida
Llamamos
época
iteramos
datos
parametro
llamamos
tasa
aprendizaje
learning
rate
valor
moveremos
dirección
mínimo
cantidad
épocas
recorrer
tasa
aprendizaje
hiperparametros
Encontrar
hiperparámetros
apropiados
datos
modelos
tarea
sencilla
daremos
valores
arbitrarios
aprender
encontrar
valores
correctos
arte
datos
sintéticos
comparar
estimaciones
valores
reales
Regresión
lineal
concisa
notebook
vimos
ejemplo
implementar
red
neuronal
cero
mala
idea
principal
razón
mala
idea
cosas
hicimos
consisten
reinventar
rueda
bibliotecas
herramientas
hicimos
implementación
eficiente
implementación
usada
generar
tiempos
espera
evitados
código
estuviera
implementado
distinta
razón
recomendable
bibliotecas
preexistentes
Recordemos
ejemplo
pensado
perdamos
miedo
bibliotecas
preexistentes
entendamos
funcionan
aprender
implementar
cosas
llegamos
necesitarlo
Veamos
implementariamos
biblioteca
pytorch
Datos
sintéticos
función
anteriormente
Cargando
datos
caso
enviar
datos
metodos
preexistentes
pytorch
generar
minilotes
pedir
mezcle
datos
deje
generan
minilotes
debemos
imprimierlos
pantalla
diferencia
implementación
método
DataLoader
genera
iterable
debemos
convertirlo
recorrerlo
necesitemos
Definiendo
modelo
continuación
presentamos
versión
concisa
modelo
discutiremos
clase
Sequential
define
capas
aplicar
secuencial
modelo
trabajamos
regresión
lineal
usaremos
capa
capa
llama
capa
totalmente
conectada
representada
matriz
aplica
vector
features
aplicar
matriz
encontramos
salida
neurona
caso
tipo
capas
conoce
Linear
reciben
entrada
numerodeentradas
numerodesalidas
modelo
features
etiqueta
capa
densa
capa
densa
completamente
conectada
forma
básica
red
neuronal
entrada
influencia
salida
pesos
modelo
entradas
salidas
matriz
pesos
vector
sesgos
bias
tendra
dimensión
Inicialización
parametros
modelo
general
frameworks
prexistentes
implementaciones
defecto
inicializar
parámetros
iniciarlos
similar
accedemos
única
capa
usando
net[0
accedemos
pesos
sesgos
weight.data
and
bias.data
Finalmente
rellenamos
valores
teníamos
pensado
Definiendo
función
pérdida
Definiendo
algoritmo
optimización
principal
diferencia
hicimos
debemos
pasarle
SDG
parametros
optimizar
resto
detalles
manejados
implementación
pytorch
caso
pasando
tasa
aprendizaje
clase
SGD
pytorch
incluye
valor
defecto
optimizador
torch
defecto
serie
métodos
interesan
usaremos
Optimizer.step
método
propiamente
aplica
algoritmo
SGD
algoritmo
fueramos
implementar
Optimizer.zero_grad
defecto
Optimizer
suma
sucesivos
gradientes
calculados
principio
época
entrenamiento
debamos
setear
gradiente
método
clase
Optimizer
Entrenamiento
veníamos
reduciendo
lineas
código
impresionante
ciclo
entrenamiento
identico
habíamos
visto
Repetimos
concluir
Calculamos
función
pérdida
Calculamos
gradiente
minilotes
Actualizamos
parámetros
trabajado
problema
regresión
deseamos
clasificar
clases
discretas
veremos
logros
redes
neuronales
area
clasificación
continuación
hablaremos
Regresión
Softmax
aplicación
clasificación
