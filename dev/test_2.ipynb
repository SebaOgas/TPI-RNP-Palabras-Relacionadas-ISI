{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d383a337",
   "metadata": {},
   "source": [
    "# Procesamiento mediante el módulo Isinet\n",
    "\n",
    "En este notebook, se demostrará el uso del módulo para la generación del dataset y entrenamiento de una red.\n",
    "\n",
    "Se utilizará como archivos originales (raw) a jupyter notebooks con información básica sobre deep learning.\n",
    "\n",
    "El primer paso es importar el módulo Isinet y generar la estructura de directorios. Si se tiene al directorio /data_2 con los notebooks, la siguientes celdas moverán todos los notebooks al subdirectorio /data_2/raw y crearán un archivo banned.txt, a ser usado más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80307ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your data_path is relative, it should be used from the current working directory: c:\\Users\\saoga\\OneDrive\\Escritorio\\Repos\\TPI-RNP-Palabras-Relacionadas-ISI\\dev\n"
     ]
    }
   ],
   "source": [
    "import isinet\n",
    "\n",
    "attrs = isinet.Attributes(\"../data_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b854ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs.init_data_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b77aaf",
   "metadata": {},
   "source": [
    "A continuación, se instanciará un proceso de Isinet. Esta clase funciona de wrapper para las funciones necesarias para procesar el dataset y entrenar la red.\n",
    "\n",
    "Se define, además, una función especial para convertir el contenido de un notebook jupyter a texto plano. Esta función convierte las celdas con markdown a texto, ignorando símbolos especiales y expresiones matemáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = isinet.Process(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51710fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mConvirtiendo archivo: 1_Detección_de_Objetos.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1_Embeddings.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1_Redes_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1_VisualizacionFeatures.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2_AnchorBoxes.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2_Entrenamiento_Word2Vec.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2_FineTuning.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2_Implementacion_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3_SSD.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3_TransferenciaEstilos.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: BERT.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: CLIP_Imágenes_y_Lenguaje_Natural.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Copia_de_Capas_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Deep_Learning_y_GPU.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Evaluación_de_los_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: GANs.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: MecanismosDeAtencion.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: ModelosDeDifusión.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Modelos_Personalizados.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: NLPDataset.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Optimización.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Regresion_Lineal_1.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Regresion_Softmax_1.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: SAM.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Segmentacion.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Selección_de_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Transformers_Vision.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: Técnicas_para_Evitar_el_Overfitting.ipynb\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: VAEs.ipynb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def strip_markdown_syntax(text):\n",
    "    # Remove headers: lines starting with #, keep the text after #\n",
    "    text = re.sub(r'^\\s{0,3}#{1,6}\\s*(.*)', r'\\1', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove emphasis: *text* or _text_ or **text** or __text__\n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)  # bold\n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)    # italic\n",
    "\n",
    "    # Remove inline code: `code`\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Remove links but keep link text: [text](url)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "\n",
    "    # Remove images syntax: ![alt](url) — remove whole thing\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove blockquotes: lines starting with >\n",
    "    text = re.sub(r'^\\s{0,3}>\\s?', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove unordered list markers (-, *, +)\n",
    "    text = re.sub(r'^\\s*([-*+])\\s+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove ordered list markers (1., 2., etc.)\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove horizontal rules (---, ***, ___)\n",
    "    text = re.sub(r'^\\s*((\\*\\s*){3,}|(-\\s*){3,}|(_\\s*){3,})\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove extra backslashes used for escaping markdown special chars\n",
    "    text = re.sub(r'\\\\([\\\\`\\*_{}\\[\\]()#+\\-.!])', r'\\1', text)\n",
    "\n",
    "    # Remove math expressions ($...$ or $$...$$)\n",
    "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\$(?:[^$\\\\]|\\\\.)*?\\$', '', text)\n",
    "\n",
    "    # Remove leftover lines starting with !\n",
    "    text = re.sub(r'^\\s*!\\s*.*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Clean multiple blank lines\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def ipynb_to_plain(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    markdown_cells = [\n",
    "        strip_markdown_syntax(''.join(cell['source']))  # Each cell['source'] is a list of lines\n",
    "        for cell in notebook.get('cells', [])\n",
    "        if cell.get('cell_type') == 'markdown'\n",
    "    ]\n",
    "\n",
    "    ret = '\\n\\n'.join(markdown_cells)\n",
    "\n",
    "    return ret\n",
    "\n",
    "process.raw_to_plain(ipynb_to_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4f66",
   "metadata": {},
   "source": [
    "A continuación, a se tokenizará los archivos con el texto plano.\n",
    "\n",
    "Primero se debe definir una función para tokenizar, y que a su vez indique por cada token si se trata de un token limpio o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b28af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTokenizando archivo: 1_Detección_de_Objetos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1_Embeddings.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1_Redes_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1_VisualizacionFeatures.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2_AnchorBoxes.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2_Entrenamiento_Word2Vec.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2_FineTuning.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2_Implementacion_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3_SSD.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3_TransferenciaEstilos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: BERT.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: CLIP_Imágenes_y_Lenguaje_Natural.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Copia_de_Capas_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Deep_Learning_y_GPU.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Evaluación_de_los_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: GANs.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: MecanismosDeAtencion.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: ModelosDeDifusión.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Modelos_Personalizados.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: NLPDataset.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Optimización.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Regresion_Lineal_1.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Regresion_Softmax_1.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: SAM.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Segmentacion.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Selección_de_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Transformers_Vision.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: Técnicas_para_Evitar_el_Overfitting.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando archivo: VAEs.ipynb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "esp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def tokenizer_esp_spacy(txt):\n",
    "    def is_clean_token(token):\n",
    "        return not (\n",
    "            token.is_punct or\n",
    "            token.is_space or\n",
    "            token.is_stop or\n",
    "            len(token) == 1)\n",
    "    \n",
    "    tokens = esp.tokenizer(txt)\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    for token in tokens:\n",
    "        ret.append(isinet.Token(token.text, is_clean_token(token)))\n",
    "\n",
    "    return ret\n",
    "\n",
    "process.tokenize(tokenizer_esp_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d04596",
   "metadata": {},
   "source": [
    "Teniendo los tokens limpios, se generará los conceptos candidatos con las siguientes líneas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7baa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6d2845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mDetectando conceptos en archivo: 1_Detección_de_Objetos.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1_Embeddings.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1_Redes_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1_VisualizacionFeatures.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2_AnchorBoxes.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2_Entrenamiento_Word2Vec.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2_FineTuning.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2_Implementacion_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3_SSD.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3_TransferenciaEstilos.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: BERT.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: CLIP_Imágenes_y_Lenguaje_Natural.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Copia_de_Capas_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Deep_Learning_y_GPU.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Evaluación_de_los_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: GANs.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: MecanismosDeAtencion.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: ModelosDeDifusión.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Modelos_Personalizados.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: NLPDataset.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Optimización.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Regresion_Lineal_1.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Regresion_Softmax_1.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: SAM.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Segmentacion.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Selección_de_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Transformers_Vision.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: Técnicas_para_Evitar_el_Overfitting.ipynb\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: VAEs.ipynb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "candidates = process.get_candidate_concepts(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42a937",
   "metadata": {},
   "source": [
    "Teniendo los conceptos candidatos, se experimenta para generar vocabularios con diversos rangos de frecuencias.\n",
    "\n",
    "Para esto, se definió la función auxiliar `div_by_2`.\n",
    "\n",
    "Tras prueba y error, se optó por utilizar el vocabulario con el ragno de frecuencias [(4.4,5.6), (3.3, 4.2), (2.2, 2.8), (1.1, 1.4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1755e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mGenerando vocabulario: 0 ([(4.4, 5.6), (3.3000000000000003, 4.199999999999999), (2.2, 2.8), (1.1, 1.4)])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "div_by_2 = lambda start_min, start_max: [(start_min*(4-i), start_max*(4-i)) for i in range(0,window_size-1)]\n",
    "\n",
    "freq_ranges = [\n",
    "    div_by_2(1.1,1.4)\n",
    "]\n",
    "\n",
    "process.make_vocabs(candidates, window_size, freq_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b004b",
   "metadata": {},
   "source": [
    "Tras escribir ciertos tokens en banned.txt, se generaron nuevamente los conceptos candidatos y el vocabulario.\n",
    "\n",
    "Es importante que el formato de rompimiento de línea del archivo banned.txt coincida con el establecido en Attributes.\n",
    "\n",
    "Finalmente, se debe seleccionar el vocabulario creado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc5a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario: 182\n"
     ]
    }
   ],
   "source": [
    "vocab_len = process.select_vocabulary(\"vocab_0\")\n",
    "\n",
    "print(f\"Longitud del vocabulario: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488fade",
   "metadata": {},
   "source": [
    "Teniendo seleccionado el vocabulario, se lo debe usar para tokenizar por conceptos los archivos con todos los tokens (no solo los limpios).\n",
    "\n",
    "Se debe definir un factor de extensión de la ventana, para compensar por el agregado de los tokens no limpios respecto al momento en que se detectó los conceptos candidatos. Como la conversión de los archivos raw a plain es altamente confiable (no aparecen caracteres adicionales en lugares extraños), se usará un factor de 2 (menor al factor por defecto, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc0cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTokenizando por conceptos: 1_Detección_de_Objetos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1_Embeddings.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1_Redes_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1_VisualizacionFeatures.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2_AnchorBoxes.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2_Entrenamiento_Word2Vec.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2_FineTuning.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2_Implementacion_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3_SSD.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3_TransferenciaEstilos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: BERT.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: CLIP_Imágenes_y_Lenguaje_Natural.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Copia_de_Capas_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Deep_Learning_y_GPU.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Evaluación_de_los_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: GANs.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: MecanismosDeAtencion.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: ModelosDeDifusión.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Modelos_Personalizados.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: NLPDataset.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Optimización.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Regresion_Lineal_1.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Regresion_Softmax_1.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: SAM.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Segmentacion.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Selección_de_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Transformers_Vision.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: Técnicas_para_Evitar_el_Overfitting.ipynb\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: VAEs.ipynb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "window_size_extension_factor = 2\n",
    "\n",
    "process.tokenize_by_concepts(window_size, window_size_extension_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e2431",
   "metadata": {},
   "source": [
    "Mediante una llamada a una función, se generarán múltiples datasets, variando el tamaño de la ventana.\n",
    "\n",
    "Además, se debe especificar el número K, que indica cuántos ejemplos negativos habrá por cada positivo (es decir, concepto en el contexto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7992e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mArmando dataset con: 1_Detección_de_Objetos.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1_Embeddings.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1_Redes_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1_VisualizacionFeatures.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2_AnchorBoxes.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2_Entrenamiento_Word2Vec.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2_FineTuning.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2_Implementacion_Transformers.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3_SSD.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3_TransferenciaEstilos.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: BERT.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: CLIP_Imágenes_y_Lenguaje_Natural.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Copia_de_Capas_Convolucionales.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Deep_Learning_y_GPU.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Evaluación_de_los_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: GANs.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: MecanismosDeAtencion.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: ModelosDeDifusión.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Modelos_Personalizados.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: NLPDataset.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Optimización.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Regresion_Lineal_1.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Regresion_Softmax_1.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: SAM.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Segmentacion.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Selección_de_Modelos.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Transformers_Vision.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: Técnicas_para_Evitar_el_Overfitting.ipynb\u001b[0m\n",
      "\u001b[94mArmando dataset con: VAEs.ipynb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "concept_window_sizes = [25, 50, 75, 100, 150, 200]\n",
    "K = 5\n",
    "\n",
    "process.make_datasets(concept_window_sizes, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a0c57",
   "metadata": {},
   "source": [
    "Finalmente, se entrenarán múltiples modelos, variando los tamaños de embedding y de lote, por cada dataset generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca5db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mEntrenando modelo dataset-100-128-64\u001b[0m\n",
      "loss 0.692, 30285.8 tokens/sec on cpu\n",
      "loss 0.688, 58441.1 tokens/sec on cpu\n",
      "loss 0.683, 90470.4 tokens/sec on cpu\n",
      "loss 0.675, 121924.1 tokens/sec on cpu\n",
      "loss 0.662, 161907.9 tokens/sec on cpu\n",
      "loss 0.645, 182426.0 tokens/sec on cpu\n",
      "loss 0.625, 228346.5 tokens/sec on cpu\n",
      "loss 0.604, 195405.8 tokens/sec on cpu\n",
      "loss 0.583, 284657.5 tokens/sec on cpu\n",
      "loss 0.566, 306399.1 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-128-128\u001b[0m\n",
      "loss 0.692, 27191.5 tokens/sec on cpu\n",
      "loss 0.685, 58174.6 tokens/sec on cpu\n",
      "loss 0.676, 87467.4 tokens/sec on cpu\n",
      "loss 0.662, 113152.9 tokens/sec on cpu\n",
      "loss 0.641, 148692.8 tokens/sec on cpu\n",
      "loss 0.615, 179837.1 tokens/sec on cpu\n",
      "loss 0.588, 205376.4 tokens/sec on cpu\n",
      "loss 0.564, 260584.9 tokens/sec on cpu\n",
      "loss 0.543, 221846.1 tokens/sec on cpu\n",
      "loss 0.526, 292068.4 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-128-256\u001b[0m\n",
      "loss 0.690, 22035.6 tokens/sec on cpu\n",
      "loss 0.678, 41920.1 tokens/sec on cpu\n",
      "loss 0.660, 58593.3 tokens/sec on cpu\n",
      "loss 0.632, 84611.0 tokens/sec on cpu\n",
      "loss 0.598, 122630.0 tokens/sec on cpu\n",
      "loss 0.564, 117907.8 tokens/sec on cpu\n",
      "loss 0.536, 140025.5 tokens/sec on cpu\n",
      "loss 0.513, 191853.7 tokens/sec on cpu\n",
      "loss 0.493, 202741.1 tokens/sec on cpu\n",
      "loss 0.475, 188628.8 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-128-512\u001b[0m\n",
      "loss 0.687, 15143.8 tokens/sec on cpu\n",
      "loss 0.666, 30662.9 tokens/sec on cpu\n",
      "loss 0.632, 50932.8 tokens/sec on cpu\n",
      "loss 0.586, 56805.7 tokens/sec on cpu\n",
      "loss 0.545, 77843.1 tokens/sec on cpu\n",
      "loss 0.511, 101030.2 tokens/sec on cpu\n",
      "loss 0.484, 102165.7 tokens/sec on cpu\n",
      "loss 0.461, 120004.8 tokens/sec on cpu\n",
      "loss 0.441, 140329.9 tokens/sec on cpu\n",
      "loss 0.423, 127016.2 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-256-64\u001b[0m\n",
      "loss 0.693, 29863.0 tokens/sec on cpu\n",
      "loss 0.691, 63605.7 tokens/sec on cpu\n",
      "loss 0.689, 99226.7 tokens/sec on cpu\n",
      "loss 0.686, 117825.0 tokens/sec on cpu\n",
      "loss 0.682, 167907.7 tokens/sec on cpu\n",
      "loss 0.678, 214785.4 tokens/sec on cpu\n",
      "loss 0.672, 243927.1 tokens/sec on cpu\n",
      "loss 0.664, 297234.8 tokens/sec on cpu\n",
      "loss 0.654, 300979.2 tokens/sec on cpu\n",
      "loss 0.643, 390921.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-256-128\u001b[0m\n",
      "loss 0.693, 36331.3 tokens/sec on cpu\n",
      "loss 0.688, 51824.5 tokens/sec on cpu\n",
      "loss 0.683, 92687.4 tokens/sec on cpu\n",
      "loss 0.678, 119686.2 tokens/sec on cpu\n",
      "loss 0.670, 139227.9 tokens/sec on cpu\n",
      "loss 0.660, 171230.2 tokens/sec on cpu\n",
      "loss 0.648, 209680.8 tokens/sec on cpu\n",
      "loss 0.632, 230875.5 tokens/sec on cpu\n",
      "loss 0.616, 263425.4 tokens/sec on cpu\n",
      "loss 0.598, 288370.4 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-256-256\u001b[0m\n",
      "loss 0.691, 24588.8 tokens/sec on cpu\n",
      "loss 0.683, 35733.3 tokens/sec on cpu\n",
      "loss 0.674, 64767.5 tokens/sec on cpu\n",
      "loss 0.663, 88852.9 tokens/sec on cpu\n",
      "loss 0.648, 114486.3 tokens/sec on cpu\n",
      "loss 0.629, 115739.6 tokens/sec on cpu\n",
      "loss 0.607, 144217.5 tokens/sec on cpu\n",
      "loss 0.584, 164358.7 tokens/sec on cpu\n",
      "loss 0.562, 167223.2 tokens/sec on cpu\n",
      "loss 0.543, 241763.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-256-512\u001b[0m\n",
      "loss 0.690, 15123.7 tokens/sec on cpu\n",
      "loss 0.678, 34922.9 tokens/sec on cpu\n",
      "loss 0.663, 54082.4 tokens/sec on cpu\n",
      "loss 0.643, 71916.3 tokens/sec on cpu\n",
      "loss 0.616, 79952.4 tokens/sec on cpu\n",
      "loss 0.587, 100896.6 tokens/sec on cpu\n",
      "loss 0.558, 119257.1 tokens/sec on cpu\n",
      "loss 0.533, 137765.8 tokens/sec on cpu\n",
      "loss 0.512, 151765.8 tokens/sec on cpu\n",
      "loss 0.493, 162956.2 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-128-64\u001b[0m\n",
      "loss 0.692, 25374.6 tokens/sec on cpu\n",
      "loss 0.688, 50384.7 tokens/sec on cpu\n",
      "loss 0.682, 76125.3 tokens/sec on cpu\n",
      "loss 0.671, 110183.4 tokens/sec on cpu\n",
      "loss 0.653, 137083.0 tokens/sec on cpu\n",
      "loss 0.631, 152500.9 tokens/sec on cpu\n",
      "loss 0.606, 178623.5 tokens/sec on cpu\n",
      "loss 0.583, 209327.2 tokens/sec on cpu\n",
      "loss 0.564, 283349.4 tokens/sec on cpu\n",
      "loss 0.547, 255749.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-128-128\u001b[0m\n",
      "loss 0.691, 28329.3 tokens/sec on cpu\n",
      "loss 0.683, 45027.2 tokens/sec on cpu\n",
      "loss 0.671, 70025.6 tokens/sec on cpu\n",
      "loss 0.650, 100434.4 tokens/sec on cpu\n",
      "loss 0.622, 131743.4 tokens/sec on cpu\n",
      "loss 0.592, 125062.6 tokens/sec on cpu\n",
      "loss 0.565, 177658.2 tokens/sec on cpu\n",
      "loss 0.543, 164897.1 tokens/sec on cpu\n",
      "loss 0.524, 211919.5 tokens/sec on cpu\n",
      "loss 0.508, 207203.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-128-256\u001b[0m\n",
      "loss 0.689, 13599.5 tokens/sec on cpu\n",
      "loss 0.674, 33906.4 tokens/sec on cpu\n",
      "loss 0.649, 45224.9 tokens/sec on cpu\n",
      "loss 0.612, 71231.1 tokens/sec on cpu\n",
      "loss 0.574, 74427.1 tokens/sec on cpu\n",
      "loss 0.542, 99628.0 tokens/sec on cpu\n",
      "loss 0.517, 117033.6 tokens/sec on cpu\n",
      "loss 0.496, 155821.4 tokens/sec on cpu\n",
      "loss 0.478, 136865.8 tokens/sec on cpu\n",
      "loss 0.462, 176398.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-128-512\u001b[0m\n",
      "loss 0.685, 12006.3 tokens/sec on cpu\n",
      "loss 0.658, 24583.7 tokens/sec on cpu\n",
      "loss 0.612, 32020.3 tokens/sec on cpu\n",
      "loss 0.562, 49115.7 tokens/sec on cpu\n",
      "loss 0.523, 60191.1 tokens/sec on cpu\n",
      "loss 0.494, 68169.6 tokens/sec on cpu\n",
      "loss 0.470, 64519.0 tokens/sec on cpu\n",
      "loss 0.449, 86749.8 tokens/sec on cpu\n",
      "loss 0.431, 103040.1 tokens/sec on cpu\n",
      "loss 0.415, 114019.4 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-256-64\u001b[0m\n",
      "loss 0.693, 22596.2 tokens/sec on cpu\n",
      "loss 0.690, 50121.4 tokens/sec on cpu\n",
      "loss 0.688, 74168.1 tokens/sec on cpu\n",
      "loss 0.684, 87619.9 tokens/sec on cpu\n",
      "loss 0.679, 126717.6 tokens/sec on cpu\n",
      "loss 0.673, 156601.3 tokens/sec on cpu\n",
      "loss 0.664, 167303.6 tokens/sec on cpu\n",
      "loss 0.653, 199763.5 tokens/sec on cpu\n",
      "loss 0.641, 227019.5 tokens/sec on cpu\n",
      "loss 0.627, 264347.4 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-256-128\u001b[0m\n",
      "loss 0.692, 22450.2 tokens/sec on cpu\n",
      "loss 0.687, 43695.5 tokens/sec on cpu\n",
      "loss 0.682, 65238.3 tokens/sec on cpu\n",
      "loss 0.675, 82591.8 tokens/sec on cpu\n",
      "loss 0.664, 116873.3 tokens/sec on cpu\n",
      "loss 0.651, 120954.7 tokens/sec on cpu\n",
      "loss 0.634, 160395.8 tokens/sec on cpu\n",
      "loss 0.616, 174822.6 tokens/sec on cpu\n",
      "loss 0.597, 193566.6 tokens/sec on cpu\n",
      "loss 0.579, 197964.7 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-256-256\u001b[0m\n",
      "loss 0.691, 14078.9 tokens/sec on cpu\n",
      "loss 0.683, 31055.3 tokens/sec on cpu\n",
      "loss 0.673, 42802.6 tokens/sec on cpu\n",
      "loss 0.660, 63556.0 tokens/sec on cpu\n",
      "loss 0.642, 84565.1 tokens/sec on cpu\n",
      "loss 0.619, 111193.7 tokens/sec on cpu\n",
      "loss 0.594, 110076.7 tokens/sec on cpu\n",
      "loss 0.571, 133572.9 tokens/sec on cpu\n",
      "loss 0.551, 146297.2 tokens/sec on cpu\n",
      "loss 0.533, 182105.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-150-256-512\u001b[0m\n",
      "loss 0.689, 9866.1 tokens/sec on cpu\n",
      "loss 0.677, 20802.1 tokens/sec on cpu\n",
      "loss 0.659, 30795.8 tokens/sec on cpu\n",
      "loss 0.635, 36808.0 tokens/sec on cpu\n",
      "loss 0.604, 53376.5 tokens/sec on cpu\n",
      "loss 0.572, 65932.2 tokens/sec on cpu\n",
      "loss 0.545, 67403.1 tokens/sec on cpu\n",
      "loss 0.522, 85282.6 tokens/sec on cpu\n",
      "loss 0.503, 93954.3 tokens/sec on cpu\n",
      "loss 0.486, 106918.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-128-64\u001b[0m\n",
      "loss 0.691, 20189.3 tokens/sec on cpu\n",
      "loss 0.686, 34433.8 tokens/sec on cpu\n",
      "loss 0.677, 54353.6 tokens/sec on cpu\n",
      "loss 0.662, 77996.2 tokens/sec on cpu\n",
      "loss 0.639, 98454.7 tokens/sec on cpu\n",
      "loss 0.613, 110799.9 tokens/sec on cpu\n",
      "loss 0.587, 138320.1 tokens/sec on cpu\n",
      "loss 0.565, 166157.4 tokens/sec on cpu\n",
      "loss 0.547, 172614.3 tokens/sec on cpu\n",
      "loss 0.531, 200132.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-128-128\u001b[0m\n",
      "loss 0.690, 16365.7 tokens/sec on cpu\n",
      "loss 0.680, 32905.3 tokens/sec on cpu\n",
      "loss 0.661, 47152.4 tokens/sec on cpu\n",
      "loss 0.631, 66839.9 tokens/sec on cpu\n",
      "loss 0.597, 85829.3 tokens/sec on cpu\n",
      "loss 0.566, 103411.2 tokens/sec on cpu\n",
      "loss 0.542, 119393.7 tokens/sec on cpu\n",
      "loss 0.522, 139108.1 tokens/sec on cpu\n",
      "loss 0.505, 152889.3 tokens/sec on cpu\n",
      "loss 0.490, 152724.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-128-256\u001b[0m\n",
      "loss 0.688, 11514.6 tokens/sec on cpu\n",
      "loss 0.671, 20589.3 tokens/sec on cpu\n",
      "loss 0.639, 29355.7 tokens/sec on cpu\n",
      "loss 0.595, 48565.2 tokens/sec on cpu\n",
      "loss 0.557, 54854.8 tokens/sec on cpu\n",
      "loss 0.527, 76402.7 tokens/sec on cpu\n",
      "loss 0.504, 90556.6 tokens/sec on cpu\n",
      "loss 0.484, 102266.3 tokens/sec on cpu\n",
      "loss 0.467, 113355.9 tokens/sec on cpu\n",
      "loss 0.453, 122008.7 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-128-512\u001b[0m\n",
      "loss 0.684, 8789.3 tokens/sec on cpu\n",
      "loss 0.650, 16838.6 tokens/sec on cpu\n",
      "loss 0.595, 21299.2 tokens/sec on cpu\n",
      "loss 0.545, 33077.9 tokens/sec on cpu\n",
      "loss 0.509, 39031.9 tokens/sec on cpu\n",
      "loss 0.482, 38709.7 tokens/sec on cpu\n",
      "loss 0.459, 54838.9 tokens/sec on cpu\n",
      "loss 0.440, 62296.0 tokens/sec on cpu\n",
      "loss 0.423, 63909.1 tokens/sec on cpu\n",
      "loss 0.409, 86190.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-256-64\u001b[0m\n",
      "loss 0.693, 19818.1 tokens/sec on cpu\n",
      "loss 0.690, 36833.5 tokens/sec on cpu\n",
      "loss 0.687, 60600.6 tokens/sec on cpu\n",
      "loss 0.682, 82064.3 tokens/sec on cpu\n",
      "loss 0.676, 92677.7 tokens/sec on cpu\n",
      "loss 0.666, 119694.8 tokens/sec on cpu\n",
      "loss 0.654, 121007.8 tokens/sec on cpu\n",
      "loss 0.639, 146264.5 tokens/sec on cpu\n",
      "loss 0.623, 165237.8 tokens/sec on cpu\n",
      "loss 0.607, 237270.9 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-256-128\u001b[0m\n",
      "loss 0.691, 15456.2 tokens/sec on cpu\n",
      "loss 0.686, 33218.1 tokens/sec on cpu\n",
      "loss 0.680, 49402.2 tokens/sec on cpu\n",
      "loss 0.670, 69105.4 tokens/sec on cpu\n",
      "loss 0.657, 79100.3 tokens/sec on cpu\n",
      "loss 0.639, 90875.7 tokens/sec on cpu\n",
      "loss 0.618, 96660.7 tokens/sec on cpu\n",
      "loss 0.597, 125211.0 tokens/sec on cpu\n",
      "loss 0.577, 137318.8 tokens/sec on cpu\n",
      "loss 0.559, 156989.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-256-256\u001b[0m\n",
      "loss 0.690, 12686.2 tokens/sec on cpu\n",
      "loss 0.682, 21312.4 tokens/sec on cpu\n",
      "loss 0.670, 32852.1 tokens/sec on cpu\n",
      "loss 0.651, 44907.1 tokens/sec on cpu\n",
      "loss 0.627, 61166.0 tokens/sec on cpu\n",
      "loss 0.599, 71245.8 tokens/sec on cpu\n",
      "loss 0.573, 87007.2 tokens/sec on cpu\n",
      "loss 0.551, 112225.6 tokens/sec on cpu\n",
      "loss 0.532, 102280.6 tokens/sec on cpu\n",
      "loss 0.516, 118490.8 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-200-256-512\u001b[0m\n",
      "loss 0.688, 9001.1 tokens/sec on cpu\n",
      "loss 0.673, 16632.4 tokens/sec on cpu\n",
      "loss 0.649, 24444.9 tokens/sec on cpu\n",
      "loss 0.616, 35389.5 tokens/sec on cpu\n",
      "loss 0.580, 39610.2 tokens/sec on cpu\n",
      "loss 0.548, 49447.2 tokens/sec on cpu\n",
      "loss 0.524, 57719.3 tokens/sec on cpu\n",
      "loss 0.503, 68034.8 tokens/sec on cpu\n",
      "loss 0.486, 76460.5 tokens/sec on cpu\n",
      "loss 0.470, 80639.8 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-128-64\u001b[0m\n",
      "loss 0.693, 36930.2 tokens/sec on cpu\n",
      "loss 0.690, 525565.7 tokens/sec on cpu\n",
      "loss 0.687, 88413.3 tokens/sec on cpu\n",
      "loss 0.684, 224355.6 tokens/sec on cpu\n",
      "loss 0.682, 342802.6 tokens/sec on cpu\n",
      "loss 0.679, 340675.6 tokens/sec on cpu\n",
      "loss 0.676, 349478.1 tokens/sec on cpu\n",
      "loss 0.673, 2903658.4 tokens/sec on cpu\n",
      "loss 0.670, 325156.6 tokens/sec on cpu\n",
      "loss 0.666, 765397.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-128-128\u001b[0m\n",
      "loss 0.693, 54489.8 tokens/sec on cpu\n",
      "loss 0.688, 130309.7 tokens/sec on cpu\n",
      "loss 0.683, 177018.0 tokens/sec on cpu\n",
      "loss 0.678, 267537.0 tokens/sec on cpu\n",
      "loss 0.673, 356831.6 tokens/sec on cpu\n",
      "loss 0.668, 404979.4 tokens/sec on cpu\n",
      "loss 0.662, 405470.4 tokens/sec on cpu\n",
      "loss 0.657, 505508.1 tokens/sec on cpu\n",
      "loss 0.651, 555234.3 tokens/sec on cpu\n",
      "loss 0.645, 616912.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-128-256\u001b[0m\n",
      "loss 0.693, 44050.6 tokens/sec on cpu\n",
      "loss 0.684, 97772.3 tokens/sec on cpu\n",
      "loss 0.675, 124841.7 tokens/sec on cpu\n",
      "loss 0.667, 175882.1 tokens/sec on cpu\n",
      "loss 0.659, 233147.0 tokens/sec on cpu\n",
      "loss 0.650, 271613.2 tokens/sec on cpu\n",
      "loss 0.641, 316223.8 tokens/sec on cpu\n",
      "loss 0.632, 331945.1 tokens/sec on cpu\n",
      "loss 0.622, 346030.5 tokens/sec on cpu\n",
      "loss 0.612, 416125.7 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-128-512\u001b[0m\n",
      "loss 0.691, 36400.9 tokens/sec on cpu\n",
      "loss 0.677, 73853.1 tokens/sec on cpu\n",
      "loss 0.663, 106930.1 tokens/sec on cpu\n",
      "loss 0.650, 145138.1 tokens/sec on cpu\n",
      "loss 0.637, 166614.2 tokens/sec on cpu\n",
      "loss 0.623, 209122.1 tokens/sec on cpu\n",
      "loss 0.608, 261708.5 tokens/sec on cpu\n",
      "loss 0.593, 327280.1 tokens/sec on cpu\n",
      "loss 0.576, 279845.4 tokens/sec on cpu\n",
      "loss 0.559, 456127.6 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-256-64\u001b[0m\n",
      "loss 0.694, 36934.4 tokens/sec on cpu\n",
      "loss 0.692, 12340000.0 tokens/sec on cpu\n",
      "loss 0.690, 112044.2 tokens/sec on cpu\n",
      "loss 0.688, 24680000.0 tokens/sec on cpu\n",
      "loss 0.686, 169410.8 tokens/sec on cpu\n",
      "loss 0.684, 37020000.0 tokens/sec on cpu\n",
      "loss 0.682, 227818.3 tokens/sec on cpu\n",
      "loss 0.680, 403011.1 tokens/sec on cpu\n",
      "loss 0.678, 55530000.0 tokens/sec on cpu\n",
      "loss 0.676, 324992.9 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-256-128\u001b[0m\n",
      "loss 0.694, 6170000.0 tokens/sec on cpu\n",
      "loss 0.689, 59371.5 tokens/sec on cpu\n",
      "loss 0.686, 231378.0 tokens/sec on cpu\n",
      "loss 0.682, 274227.6 tokens/sec on cpu\n",
      "loss 0.678, 385630.0 tokens/sec on cpu\n",
      "loss 0.675, 1470807.4 tokens/sec on cpu\n",
      "loss 0.671, 43190000.0 tokens/sec on cpu\n",
      "loss 0.668, 269975.7 tokens/sec on cpu\n",
      "loss 0.664, 55530000.0 tokens/sec on cpu\n",
      "loss 0.660, 347899.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-256-256\u001b[0m\n",
      "loss 0.694, 40835.8 tokens/sec on cpu\n",
      "loss 0.687, 102836.7 tokens/sec on cpu\n",
      "loss 0.681, 154258.1 tokens/sec on cpu\n",
      "loss 0.675, 201926.2 tokens/sec on cpu\n",
      "loss 0.669, 258773.0 tokens/sec on cpu\n",
      "loss 0.663, 326026.0 tokens/sec on cpu\n",
      "loss 0.657, 1439427.8 tokens/sec on cpu\n",
      "loss 0.651, 238517.5 tokens/sec on cpu\n",
      "loss 0.645, 443755.9 tokens/sec on cpu\n",
      "loss 0.639, 527730.7 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-25-256-512\u001b[0m\n",
      "loss 0.692, 35028.7 tokens/sec on cpu\n",
      "loss 0.682, 83757.1 tokens/sec on cpu\n",
      "loss 0.672, 918775.9 tokens/sec on cpu\n",
      "loss 0.663, 155953.1 tokens/sec on cpu\n",
      "loss 0.653, 101556.6 tokens/sec on cpu\n",
      "loss 0.644, 231440.1 tokens/sec on cpu\n",
      "loss 0.634, 311723.7 tokens/sec on cpu\n",
      "loss 0.625, 647882.5 tokens/sec on cpu\n",
      "loss 0.615, 363927.1 tokens/sec on cpu\n",
      "loss 0.604, 393015.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-128-64\u001b[0m\n",
      "loss 0.693, 51259.9 tokens/sec on cpu\n",
      "loss 0.689, 288204.9 tokens/sec on cpu\n",
      "loss 0.686, 93759.9 tokens/sec on cpu\n",
      "loss 0.682, 219623.2 tokens/sec on cpu\n",
      "loss 0.678, 316228.3 tokens/sec on cpu\n",
      "loss 0.673, 303433.8 tokens/sec on cpu\n",
      "loss 0.667, 426466.9 tokens/sec on cpu\n",
      "loss 0.660, 343747.1 tokens/sec on cpu\n",
      "loss 0.651, 499807.2 tokens/sec on cpu\n",
      "loss 0.640, 425639.4 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-128-128\u001b[0m\n",
      "loss 0.692, 45750.1 tokens/sec on cpu\n",
      "loss 0.685, 70617.9 tokens/sec on cpu\n",
      "loss 0.679, 124792.9 tokens/sec on cpu\n",
      "loss 0.672, 196275.2 tokens/sec on cpu\n",
      "loss 0.663, 236450.3 tokens/sec on cpu\n",
      "loss 0.653, 271591.0 tokens/sec on cpu\n",
      "loss 0.640, 339195.2 tokens/sec on cpu\n",
      "loss 0.625, 327506.9 tokens/sec on cpu\n",
      "loss 0.608, 409663.5 tokens/sec on cpu\n",
      "loss 0.590, 453387.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-128-256\u001b[0m\n",
      "loss 0.691, 37969.0 tokens/sec on cpu\n",
      "loss 0.680, 75439.4 tokens/sec on cpu\n",
      "loss 0.670, 114282.8 tokens/sec on cpu\n",
      "loss 0.657, 140347.6 tokens/sec on cpu\n",
      "loss 0.642, 179749.6 tokens/sec on cpu\n",
      "loss 0.623, 214300.3 tokens/sec on cpu\n",
      "loss 0.601, 264557.1 tokens/sec on cpu\n",
      "loss 0.577, 302464.5 tokens/sec on cpu\n",
      "loss 0.553, 323427.4 tokens/sec on cpu\n",
      "loss 0.530, 359520.5 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-128-512\u001b[0m\n",
      "loss 0.690, 25679.3 tokens/sec on cpu\n",
      "loss 0.673, 49632.4 tokens/sec on cpu\n",
      "loss 0.654, 76577.5 tokens/sec on cpu\n",
      "loss 0.633, 106175.6 tokens/sec on cpu\n",
      "loss 0.605, 133488.1 tokens/sec on cpu\n",
      "loss 0.574, 151059.0 tokens/sec on cpu\n",
      "loss 0.543, 179394.6 tokens/sec on cpu\n",
      "loss 0.513, 230808.5 tokens/sec on cpu\n",
      "loss 0.487, 210773.3 tokens/sec on cpu\n",
      "loss 0.464, 290986.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-256-64\u001b[0m\n",
      "loss 0.693, 41875.6 tokens/sec on cpu\n",
      "loss 0.690, 122929.8 tokens/sec on cpu\n",
      "loss 0.688, 153610.5 tokens/sec on cpu\n",
      "loss 0.686, 56879.5 tokens/sec on cpu\n",
      "loss 0.683, 241475.7 tokens/sec on cpu\n",
      "loss 0.681, 569813.8 tokens/sec on cpu\n",
      "loss 0.678, 342607.0 tokens/sec on cpu\n",
      "loss 0.675, 414832.7 tokens/sec on cpu\n",
      "loss 0.671, 360833.9 tokens/sec on cpu\n",
      "loss 0.667, 594927.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-256-128\u001b[0m\n",
      "loss 0.693, 57471.4 tokens/sec on cpu\n",
      "loss 0.688, 133276.5 tokens/sec on cpu\n",
      "loss 0.684, 117239.1 tokens/sec on cpu\n",
      "loss 0.680, 221987.0 tokens/sec on cpu\n",
      "loss 0.676, 281120.2 tokens/sec on cpu\n",
      "loss 0.671, 376488.8 tokens/sec on cpu\n",
      "loss 0.665, 323189.2 tokens/sec on cpu\n",
      "loss 0.659, 473034.3 tokens/sec on cpu\n",
      "loss 0.652, 912022.6 tokens/sec on cpu\n",
      "loss 0.644, 435061.8 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-256-256\u001b[0m\n",
      "loss 0.693, 36067.7 tokens/sec on cpu\n",
      "loss 0.685, 59949.3 tokens/sec on cpu\n",
      "loss 0.679, 100152.4 tokens/sec on cpu\n",
      "loss 0.672, 177337.7 tokens/sec on cpu\n",
      "loss 0.664, 129546.9 tokens/sec on cpu\n",
      "loss 0.656, 233565.5 tokens/sec on cpu\n",
      "loss 0.647, 279273.7 tokens/sec on cpu\n",
      "loss 0.636, 309205.8 tokens/sec on cpu\n",
      "loss 0.624, 239426.6 tokens/sec on cpu\n",
      "loss 0.610, 378978.7 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-50-256-512\u001b[0m\n",
      "loss 0.692, 25846.0 tokens/sec on cpu\n",
      "loss 0.680, 47281.8 tokens/sec on cpu\n",
      "loss 0.668, 64357.3 tokens/sec on cpu\n",
      "loss 0.656, 109044.5 tokens/sec on cpu\n",
      "loss 0.643, 172674.7 tokens/sec on cpu\n",
      "loss 0.628, 162732.5 tokens/sec on cpu\n",
      "loss 0.610, 173110.2 tokens/sec on cpu\n",
      "loss 0.590, 179534.7 tokens/sec on cpu\n",
      "loss 0.569, 243627.5 tokens/sec on cpu\n",
      "loss 0.548, 269015.9 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-128-64\u001b[0m\n",
      "loss 0.693, 43993.3 tokens/sec on cpu\n",
      "loss 0.689, 106614.7 tokens/sec on cpu\n",
      "loss 0.685, 98262.6 tokens/sec on cpu\n",
      "loss 0.680, 204311.4 tokens/sec on cpu\n",
      "loss 0.672, 181411.6 tokens/sec on cpu\n",
      "loss 0.660, 232497.6 tokens/sec on cpu\n",
      "loss 0.646, 293979.9 tokens/sec on cpu\n",
      "loss 0.628, 338877.5 tokens/sec on cpu\n",
      "loss 0.609, 367466.0 tokens/sec on cpu\n",
      "loss 0.591, 398930.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-128-128\u001b[0m\n",
      "loss 0.692, 44172.1 tokens/sec on cpu\n",
      "loss 0.685, 71592.2 tokens/sec on cpu\n",
      "loss 0.677, 106472.7 tokens/sec on cpu\n",
      "loss 0.665, 135592.1 tokens/sec on cpu\n",
      "loss 0.649, 108435.1 tokens/sec on cpu\n",
      "loss 0.629, 189984.5 tokens/sec on cpu\n",
      "loss 0.605, 289832.4 tokens/sec on cpu\n",
      "loss 0.581, 289446.4 tokens/sec on cpu\n",
      "loss 0.559, 353230.7 tokens/sec on cpu\n",
      "loss 0.540, 363410.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-128-256\u001b[0m\n",
      "loss 0.691, 33690.3 tokens/sec on cpu\n",
      "loss 0.679, 45566.2 tokens/sec on cpu\n",
      "loss 0.665, 79452.3 tokens/sec on cpu\n",
      "loss 0.645, 130484.1 tokens/sec on cpu\n",
      "loss 0.617, 115600.4 tokens/sec on cpu\n",
      "loss 0.586, 166462.3 tokens/sec on cpu\n",
      "loss 0.557, 176451.3 tokens/sec on cpu\n",
      "loss 0.531, 222495.2 tokens/sec on cpu\n",
      "loss 0.509, 292408.2 tokens/sec on cpu\n",
      "loss 0.489, 232789.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-128-512\u001b[0m\n",
      "loss 0.688, 19354.0 tokens/sec on cpu\n",
      "loss 0.669, 40265.7 tokens/sec on cpu\n",
      "loss 0.642, 56727.4 tokens/sec on cpu\n",
      "loss 0.606, 90004.9 tokens/sec on cpu\n",
      "loss 0.565, 86820.7 tokens/sec on cpu\n",
      "loss 0.529, 115942.1 tokens/sec on cpu\n",
      "loss 0.499, 141380.9 tokens/sec on cpu\n",
      "loss 0.474, 177057.3 tokens/sec on cpu\n",
      "loss 0.451, 153695.6 tokens/sec on cpu\n",
      "loss 0.431, 201197.3 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-256-64\u001b[0m\n",
      "loss 0.693, 36607.3 tokens/sec on cpu\n",
      "loss 0.691, 76937.4 tokens/sec on cpu\n",
      "loss 0.688, 138765.7 tokens/sec on cpu\n",
      "loss 0.686, 163388.0 tokens/sec on cpu\n",
      "loss 0.683, 193144.2 tokens/sec on cpu\n",
      "loss 0.679, 242467.7 tokens/sec on cpu\n",
      "loss 0.674, 456492.8 tokens/sec on cpu\n",
      "loss 0.669, 273333.7 tokens/sec on cpu\n",
      "loss 0.662, 319623.8 tokens/sec on cpu\n",
      "loss 0.654, 429914.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-256-128\u001b[0m\n",
      "loss 0.693, 41007.9 tokens/sec on cpu\n",
      "loss 0.688, 70911.9 tokens/sec on cpu\n",
      "loss 0.683, 110075.1 tokens/sec on cpu\n",
      "loss 0.678, 160450.4 tokens/sec on cpu\n",
      "loss 0.672, 162591.3 tokens/sec on cpu\n",
      "loss 0.664, 226431.3 tokens/sec on cpu\n",
      "loss 0.654, 279452.0 tokens/sec on cpu\n",
      "loss 0.643, 312066.3 tokens/sec on cpu\n",
      "loss 0.629, 362927.8 tokens/sec on cpu\n",
      "loss 0.615, 432491.2 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-256-256\u001b[0m\n",
      "loss 0.692, 28231.9 tokens/sec on cpu\n",
      "loss 0.684, 68305.1 tokens/sec on cpu\n",
      "loss 0.677, 76308.5 tokens/sec on cpu\n",
      "loss 0.668, 141247.7 tokens/sec on cpu\n",
      "loss 0.658, 129057.4 tokens/sec on cpu\n",
      "loss 0.645, 166002.0 tokens/sec on cpu\n",
      "loss 0.629, 200080.2 tokens/sec on cpu\n",
      "loss 0.611, 302928.1 tokens/sec on cpu\n",
      "loss 0.592, 227898.0 tokens/sec on cpu\n",
      "loss 0.573, 352768.0 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-75-256-512\u001b[0m\n",
      "loss 0.690, 18738.8 tokens/sec on cpu\n",
      "loss 0.679, 43648.3 tokens/sec on cpu\n",
      "loss 0.666, 61519.4 tokens/sec on cpu\n",
      "loss 0.651, 88906.6 tokens/sec on cpu\n",
      "loss 0.632, 110513.0 tokens/sec on cpu\n",
      "loss 0.609, 132005.1 tokens/sec on cpu\n",
      "loss 0.583, 149594.1 tokens/sec on cpu\n",
      "loss 0.558, 157898.9 tokens/sec on cpu\n",
      "loss 0.535, 199700.9 tokens/sec on cpu\n",
      "loss 0.514, 203614.5 tokens/sec on cpu\n"
     ]
    }
   ],
   "source": [
    "lr = 0.002\n",
    "num_epochs = 10\n",
    "embed_sizes = [64, 128, 256, 512]\n",
    "batch_sizes = [128, 256]\n",
    "\n",
    "process.train_multiple(lr, num_epochs, embed_sizes, batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eadf7d",
   "metadata": {},
   "source": [
    "Habiendo entrenado estos modelos, solo resta probarlos. Esto puede hacerse mediante la función `get_related_concepts`.\n",
    "\n",
    "En primer lugar, se debe cargar el modelo (con la clase SkipGram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d895b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dataset-25-256-512\"\n",
    "\n",
    "embed_size= int(model_name.split(\"-\")[3])\n",
    "\n",
    "model = isinet.SkipGram(process.vocabulary, embed_size)\n",
    "model.load(attrs, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2781f1",
   "metadata": {},
   "source": [
    "Algunos ejemplos de uso de la función para obtener los 10 conceptos más relacionados serían:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e23a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('práctica',),\n",
       " ('métodos',),\n",
       " ('datos', 'falsos'),\n",
       " ('residual',),\n",
       " ('memoria',),\n",
       " ('sintetizada',),\n",
       " ('métodos', 'aumento'),\n",
       " ('predecir', 'palabra'),\n",
       " ('propagación',),\n",
       " ('clase', 'etiqueta')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('datos', 'conjunto')\n",
    "isinet.get_related_concepts(process.vocabulary, model, 8, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35543c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('salida', 'capas'),\n",
       " ('veamos',),\n",
       " ('clase', 'etiqueta'),\n",
       " ('término', 'regularización'),\n",
       " ('complejidad',),\n",
       " ('algoritmo', 'aprendizaje'),\n",
       " ('red', 'profunda'),\n",
       " ('boxes', 'bounding'),\n",
       " ('paper',),\n",
       " ('entrada', 'convolución')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('aprendizaje', 'supervisado')\n",
    "isinet.get_related_concepts(process.vocabulary, model, 38, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956df29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memoria',),\n",
       " ('stride',),\n",
       " ('centrales', 'palabras'),\n",
       " ('parametro', 'optimizar'),\n",
       " ('clase', 'etiqueta'),\n",
       " ('activación', 'sigmoidea'),\n",
       " ('principal',),\n",
       " ('calculado', 'parametro'),\n",
       " ('entrenar', 'red'),\n",
       " ('cpu', 'gpu')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('relu',)\n",
    "isinet.get_related_concepts(process.vocabulary, model, 50, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766a8eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('relu',),\n",
       " ('atención', 'tokens'),\n",
       " ('entrada', 'original'),\n",
       " ('average', 'pooling'),\n",
       " ('semántica',),\n",
       " ('imagen', 'texto'),\n",
       " ('cabezales', 'atención'),\n",
       " ('optimización',),\n",
       " ('contexto', 'palabras'),\n",
       " ('acceso', 'parámetros')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('activación', 'sigmoidea')\n",
    "isinet.get_related_concepts(process.vocabulary, model, 55, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19f2e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('suma',),\n",
       " ('stride',),\n",
       " ('eficiencia', 'computacional'),\n",
       " ('entrada', 'altura'),\n",
       " ('datos', 'fashion-mnist'),\n",
       " ('machine', 'learning'),\n",
       " ('cabezales',),\n",
       " ('algoritmo', 'aprendizaje'),\n",
       " ('tensor', 'kernel'),\n",
       " ('entrenamos', 'modelo')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('traducción', 'automática')\n",
    "isinet.get_related_concepts(process.vocabulary, model, 83, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2febdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atención', 'tokens'),\n",
       " ('práctica',),\n",
       " ('dropout', 'conexión'),\n",
       " ('funciona',),\n",
       " ('complejidad',),\n",
       " ('multiplicaciones', 'matrices'),\n",
       " ('clip', 'personalizado'),\n",
       " ('sintetizada',),\n",
       " ('padding',),\n",
       " ('implementamos', 'función')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('segmentación', 'instancias')\n",
    "isinet.get_related_concepts(process.vocabulary, model, 176, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
