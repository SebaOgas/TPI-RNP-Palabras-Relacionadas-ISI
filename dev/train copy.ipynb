{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmh3bN-o6LYh"
   },
   "source": [
    "# Palabras Relacionadas - Dataset\n",
    "\n",
    "En el presente notebook se documenta el preprocesamiento y creación del dataset del proyecto.\n",
    "## Preprocesamiento documentos\n",
    "- **Documentos a procesar:** son pertenecientes a las distintas materias, según lo especificado en el [plan de Ingeniería en Sistemas de Información 2023, UTN - FRM](https://www.lamanuelsavio.org/wp-content/uploads/2024/02/Plan-Sistemas.pdf).\n",
    "- **Ubicación de los documentos a procesar:** desde el root del proyecto `/data/raw`.\n",
    "- **Ubicación de los documentos procesados:** desde el root en `/data/plain`\n",
    "- **Nombre de los documentos:** consta de la siguiente sintaxis `<nivel materia> - <abreviatura materia> - <título del material>`\n",
    "    - `<nivel materia>`: año (entero) de cursado según plan de estudios. Ej. `5` para quinto año\n",
    "    - `<abreviatura materia>`: abreviatura del nombre de la materia. Ej. `SO` para Sistemas Operativos\n",
    "    - `<título del material>`: nombre original significativo del pdf\n",
    "\n",
    "Los **documentos a procesar están en formato .pdf**, por lo que se utiliza la librería [pypdf](https://pypdf.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3630,
     "status": "ok",
     "timestamp": 1749157967269,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "8LOgWv-f8Ems",
    "outputId": "ed04fe33-eec5-43ee-d047-ee224073420f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.0-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.0-cp39-abi3-win_amd64.whl (18.5 MB)\n",
      "   ---------------------------------------- 0.0/18.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.5 MB 16.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.7/18.5 MB 18.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 8.4/18.5 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 12.8/18.5 MB 19.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.6/18.5 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.5/18.5 MB 19.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saoga\\OneDrive\\Escritorio\\Repos\\TPI-RNP-Palabras-Relacionadas-ISI\\dev\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Seteo el path al root del proyecto\n",
    "dev_folder = 'dev'\n",
    "if(os.getcwd().split('/')[-1] == dev_folder):\n",
    "    os.chdir('../')\n",
    "print(os.getcwd()) # debugging, debe imprimir el path al root del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el método `raw_to_plain` que dados:\n",
    "- `raw_path`: path de los archivos a procesar\n",
    "- `plain_path`: path de los archivos procesados \n",
    " \n",
    "Crea un archivo de texto plano (`.txt`) por cada archivo procesado y lo guarda en el `plain_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 894532,
     "status": "ok",
     "timestamp": 1749159253392,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "OUVAxYUy7vN7",
    "outputId": "142c1885-aac0-4f81-d7e9-dc816eade658"
   },
   "outputs": [],
   "source": [
    "def raw_to_plain(raw_path, plain_path):\n",
    "    raw_files = [os.path.splitext(f)[0] for f in listdir(raw_path) if isfile(join(raw_path, f))]\n",
    "    \n",
    "    for file in raw_files:\n",
    "        print(\"\\033[94mConvirtiendo archivo: \" + file + \"\\033[0m\")\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(f'{raw_path}/{file}.pdf')\n",
    "            \n",
    "            with open(f'{plain_path}/{file}.txt', 'w', encoding='utf-8') as plain_file:\n",
    "                for page in doc:\n",
    "                    text = page.get_text(\"text\") \n",
    "                    plain_file.write(text)\n",
    "                    \n",
    "            doc.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\033[91mError processing {file}: {str(e)}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mConvirtiendo archivo: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - SO - Tanenbaum\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Ergonomia - 4°9°\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"./data/raw\"\n",
    "plain_path = \"./data/plain\"\n",
    "\n",
    "raw_to_plain(raw_path, plain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "parameters",
     "hide_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Año 1:\n",
      "\tMateria: AyED - 0.07MB\n",
      "\tMateria: SyO - 1.16MB\n",
      "\tMateria: MD - 0.71MB\n",
      "\tMateria: AC - 0.44MB\n",
      "Año 4:\n",
      "\tMateria: ICS - 0.24MB\n",
      "\tMateria: AS - 2.36MB\n",
      "\tMateria: TA - 0.06MB\n",
      "\tMateria: UXUI - 0.11MB\n",
      "\tMateria: IO - 0.03MB\n",
      "\tMateria: RD - 5.17MB\n",
      "\tMateria: S - 0.02MB\n",
      "Año 3:\n",
      "\tMateria: DS - 1.28MB\n",
      "\tMateria: BD - 0.26MB\n",
      "\tMateria: AP - 0.14MB\n",
      "\tMateria: CD - 4.85MB\n",
      "Año 2:\n",
      "\tMateria: SO - 5.8MB\n",
      "\tMateria: AS - 0.59MB\n",
      "Año 5:\n",
      "\tMateria: SSI - 1.47MB\n",
      "\tMateria: GG - 0.01MB\n"
     ]
    }
   ],
   "source": [
    "def get_metrics(plain_path, split_char='-'):\n",
    "    metrics = {}\n",
    "\n",
    "    plain_files = [os.path.splitext(f)[0] for f in listdir(plain_path) if isfile(join(plain_path, f))]\n",
    "\n",
    "    for file in plain_files:\n",
    "\n",
    "        sf = file.split(split_char)\n",
    "        anio = sf[0].strip()\n",
    "        materia = sf[1].strip()\n",
    "\n",
    "        if(not anio in metrics):\n",
    "            metrics[anio] = {}\n",
    "        if(not materia in metrics[anio]):\n",
    "            metrics[anio][materia] = 0\n",
    "        metrics[anio][materia] += os.path.getsize(f'{plain_path}/{file}.txt')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for anio, materias in metrics.items():\n",
    "        print(f\"Año {anio}:\")\n",
    "        for materia, tamano in materias.items():\n",
    "            print(f\"\\tMateria: {materia} - {str(round(tamano/1000000,2))}MB\")\n",
    "            # TODO: tal vez mostrar la sumatoria de los tamaños\n",
    "            \n",
    "print_metrics(get_metrics(plain_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Oh3rLH97OW"
   },
   "source": [
    "## Tokenización\n",
    "A continuación se tokenizan los documentos procesados planos `.txt`, generando un nuevo archivo por cada documento procesado fuente, donde cada línea representa un token. \n",
    "\n",
    "Para ello se utiliza la librería [spacy](https://spacy.io/), la cual tomará un rol importante para obtención de tokens, filtrado y transformación de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22248,
     "status": "ok",
     "timestamp": 1749159426548,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "8dxbA4r29x02",
    "outputId": "ebbd9d1e-009c-491b-c147-e44856e84623"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.8/12.9 MB 12.6 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.8/12.9 MB 19.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.9 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.9 MB 16.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.9/12.9 MB 3.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "esp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9mpV1K6D5H0n"
   },
   "outputs": [],
   "source": [
    "def tokenize_files(plain_path, tokens_path, banned_tokens):\n",
    "    raw_files = [os.path.splitext(f)[0] for f in listdir(plain_path) if isfile(join(plain_path, f))]\n",
    "\n",
    "    def is_clean_token(token):\n",
    "        return not (\n",
    "            token.is_punct or\n",
    "            token.is_space or\n",
    "            token.is_stop or\n",
    "            len(token.text) == 1 or\n",
    "            token.text in banned_tokens or \n",
    "            bool(re.search(r'(^[0-9\\.\\,]+$)|(-$)|(^.\\.$)', token.text)))\n",
    "\n",
    "    for f in raw_files:\n",
    "        print(\"\\033[94mTokenizando archivo: \" + f + \"\\033[0m\")\n",
    "\n",
    "        with open(f\"{plain_path}/{f}.txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "            tokens = esp.tokenizer(txt)\n",
    "            with open(f\"{tokens_path}/{f}.txt\", \"wb\") as tf:\n",
    "                for token in tokens:\n",
    "                    if (is_clean_token(token)):\n",
    "                        tf.write((token.text + \"\\n\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - Tanenbaum\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 4 - AS - Ergonomia - 4°9°\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokens_path = \"./data/tokens\"\n",
    "banned_tokens = set([\"capítulo\", \"página\", \"figura\", \"cap\", \"ejemplo\", \"catedra\", \"mendoza\", \"argentina\", \"muñoz\", \"facchini\", \"cesari\", \"xsd\", \"infoleg\"])\n",
    "tokenize_files(plain_path, tokens_path, banned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh5kn_mP_qZF"
   },
   "source": [
    "### Tokenización con ventana deslizante\n",
    "Una vez que se tienen los archivos con los tokens, deseamos **detectar conceptos adicionales**. Aquellos conceptos que adquieren significado con la combinación de palabras.\n",
    "\n",
    "Para ello se utilizará un **algoritmo de ventana deslizante** con un `window_size = 4`. Lo que buscamos es dada una secuencia de tokens `[\"sistemas\", \"operativos\", \"distribuídos\"]`formar los siguientes conceptos: `\"sistemas operativos\"`, `\"sistemas operativos distribuídos\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1749163391436,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "8mWFXJbSQq5F"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "fMtkg6yC_5o4"
   },
   "outputs": [],
   "source": [
    "related_table = {}\n",
    "raw_files = [os.path.splitext(f)[0] for f in listdir(plain_path) if isfile(join(plain_path, f))]\n",
    "banned = [\"capítulo\", \"página\", \"figura\", \"cap\", \"ejemplo\", \"catedra\", \"mendoza\", \"argentina\", \"muñoz\", \"facchini\", \"cesari\", \"xsd\", \"infoleg\"]\n",
    "\n",
    "for f in raw_files:\n",
    "    print(\"\\033[94mDetectando conceptos en archivo: \" + f + \"\\033[0m\")\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    with open(tokens_path + \"/\" + f + \".txt\", \"rb\") as tf:\n",
    "        tokens = tf.read().decode(\"utf-8\").split(\"\\n\")\n",
    "\n",
    "    for i in range(len(tokens) - window_size):\n",
    "        window = tokens[i:i+window_size]\n",
    "\n",
    "        def get_subarrays(arr):\n",
    "            result = []\n",
    "            n = len(arr)\n",
    "            for r in range(1, n+1):  # sizes from 1 to n\n",
    "                for indices in combinations(range(n), r):\n",
    "                    subarray = [arr[i] for i in indices]\n",
    "                    result.append(subarray)\n",
    "            return result\n",
    "        \n",
    "        arrays = get_subarrays(window[1:])\n",
    "\n",
    "        # arrays.insert(0, []) # Permite formar conceptos de una sola palabra\n",
    "\n",
    "        for arr in arrays:\n",
    "            arr.insert(0, window[0])\n",
    "            arr = [s.lower() for s in arr]\n",
    "\n",
    "            if any(re.search(r'(^[0-9\\.\\,]+$)|(-$)|(^.\\.$)', s) for s in arr):\n",
    "                continue\n",
    "\n",
    "            if len(arr) != len(set(arr)):\n",
    "                continue\n",
    "\n",
    "            if any(ban in arr for ban in banned):\n",
    "                continue\n",
    "\n",
    "            arr.sort()\n",
    "            \n",
    "            t = tuple(arr)\n",
    "            if (not t in related_table):\n",
    "                related_table[t] = 0\n",
    "            related_table[t] += 1\n",
    "\n",
    "print(\"\\033[92mCantidad de conceptos candidatos:\" + str(len(related_table)) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhn-HksfDhKN"
   },
   "source": [
    "### Creación Vocabulario\n",
    "Teniendo los tokens conceptuales, creamos el vocabulario. Para crear el vocabulario, se realizará un filtro al diccionario `related_table` resultante de los pasos anteriores. Definimos los parámetros `min_freq` y `max_freq` para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 50\n",
    "max_freq = 100\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for tokens, freq in related_table.items():\n",
    "    if (freq >= min_freq and freq <= max_freq):\n",
    "        vocabulary.append(tokens)\n",
    "\n",
    "concepts_file = \"./data/vocabulary.txt\"\n",
    "\n",
    "with open(concepts_file, \"wb\") as cf:\n",
    "    for concept in vocabulary:\n",
    "        cf.write((str(concept) + \"\\n\").encode(\"utf-8\"))\n",
    "\n",
    "print(\"\\033[92mTamaño del vocabulario:\" + str(len(vocabulary)) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waCeLFNPFJbO"
   },
   "source": [
    "En este punto, ya hemos seleccionado conjuntos de tokens que suelen aparecer cerca.\n",
    "\n",
    "Estos conjuntos serán los conceptos, y pasarán a formar nuestro vocabulario.\n",
    "\n",
    "Ahora, se debe tokenizar nuevamente los textos planos, utilizando los conceptos.\n",
    "\n",
    "Para esto, se recorrerá cada archivo de /data/plain mediante una ventana deslizante del mismo tamaño utilizado para detectar conceptos, separando en palabras siempre y cuando no se encuentre dentro de la ventana las palabras de un concepto.\n",
    "\n",
    "Se generarán nuevos tokens, siendo estos numéricos (/data/tokens_num). LLos positivos (o 0) corresponden al índice de un concepto en el vocabulario, mientras que los negativos indican la cantidad de tokens no reconocidos (\\<unk\\>). Esto se realizó de esta forma para ahorrar espacio y tiempo de procesamiento.\n",
    "\n",
    "Si en una ventana se detectara más de un concepto, se agregarán todos los que se encuentre. Debido al procesamiento que se realizará más adelante, no debería importar el orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTamaño del vocabulario:2726\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "concepts_file = \"./data/vocabulary.txt\"\n",
    "vocabulary = []\n",
    "\n",
    "with open(concepts_file, \"rb\") as cf:\n",
    "    lines = cf.read().decode(\"utf-8\").split(\"\\n\")[:-1]\n",
    "    vocabulary = [ast.literal_eval(l) for l in lines]\n",
    "\n",
    "print(\"\\033[92mTamaño del vocabulario:\" + str(len(vocabulary)) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RQzvFvNPHcfL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Tokenizando archivo por conceptos: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Ergonomia - 4°9°\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[Tokenizando archivo por conceptos: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "tokens_conceptos_path = \"./data/tokens_conceptos\"\n",
    "\n",
    "metrics_2 = {}\n",
    "\n",
    "window_size_concept_tokenization = window_size * 3\n",
    "\n",
    "for f in raw_files:\n",
    "    print(\"\\033[Tokenizando archivo por conceptos: \" + f + \"\\033[0m\")\n",
    "    metrics_2[f] = {}\n",
    "    found_concepts = 0\n",
    "\n",
    "    recent_concepts = {}\n",
    "\n",
    "    with open(plain_path + \"/\" + f + \".txt\", \"rb\") as pf:\n",
    "        txt = pf.read().decode(\"utf-8\")\n",
    "        tokens = [t.text for t in esp.tokenizer(txt)]\n",
    "\n",
    "        metrics_2[f][\"tokens\"] = len(tokens)\n",
    "\n",
    "        with open(tokens_conceptos_path + \"/\" + f + \".txt\", \"wb\") as tnf:\n",
    "\n",
    "            unks = 0\n",
    "\n",
    "            for i in range(len(tokens) - window_size_concept_tokenization):\n",
    "                window = tokens[i:i+window_size_concept_tokenization]\n",
    "                for k, v in recent_concepts.items():\n",
    "                    if v > 0:\n",
    "                        recent_concepts[k] -= 1\n",
    "\n",
    "                unks += 1\n",
    "            \n",
    "                for ix, concept in enumerate(vocabulary):\n",
    "                    if all(word in window for word in concept) and (not ix in recent_concepts or recent_concepts[ix] == 0):\n",
    "                        tnf.write((\"-\" + str((unks-1)) + \" \" + str(ix) + \" \").encode(\"utf-8\"))\n",
    "                        unks = 0\n",
    "                        recent_concepts[ix] = window_size_concept_tokenization\n",
    "                        found_concepts += 1\n",
    "\n",
    "                    \n",
    "    metrics_2[f][\"concepts\"] = found_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "Archivo 3 - AP - AdProy_2_Trabajo en Equipo_2022:\n",
      "\ttokens: 27662\n",
      "\tconcepts: 275\n",
      "Archivo 3 - AP - respuestas:\n",
      "\ttokens: 0\n",
      "\tconcepts: 0\n",
      "Archivo 3 - BD - caselli_manual-de-base-de-datos-2019:\n",
      "\ttokens: 28732\n",
      "\tconcepts: 388\n",
      "Archivo 3 - BD - Guía 1:\n",
      "\ttokens: 11365\n",
      "\tconcepts: 390\n",
      "Archivo 3 - BD - Guía 2:\n",
      "\ttokens: 9548\n",
      "\tconcepts: 143\n",
      "Archivo 3 - CD - capitulo2:\n",
      "\ttokens: 10412\n",
      "\tconcepts: 236\n",
      "Archivo 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings:\n",
      "\ttokens: 455695\n",
      "\tconcepts: 17102\n",
      "Archivo 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion:\n",
      "\ttokens: 530998\n",
      "\tconcepts: 19677\n",
      "Archivo 3 - DS - Actor. Definicion. Clasificacion (1):\n",
      "\ttokens: 1490\n",
      "\tconcepts: 33\n",
      "Archivo 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados:\n",
      "\ttokens: 4415\n",
      "\tconcepts: 41\n",
      "Archivo 3 - DS - Libro UML y Patrones - Larman:\n",
      "\ttokens: 253057\n",
      "\tconcepts: 3868\n",
      "Archivo 3 - DS - MerFNConceptos:\n",
      "\ttokens: 2342\n",
      "\tconcepts: 42\n",
      "Archivo 4 - AS - Analisis PEST:\n",
      "\ttokens: 1004\n",
      "\tconcepts: 4\n",
      "Archivo 4 - AS - Backups_raids:\n",
      "\ttokens: 1576\n",
      "\tconcepts: 31\n",
      "Archivo 4 - AS - Benchmark en Tecnología:\n",
      "\ttokens: 279\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Benchmark:\n",
      "\ttokens: 1543\n",
      "\tconcepts: 25\n",
      "Archivo 4 - AS - Chiavenato-cap8:\n",
      "\ttokens: 11040\n",
      "\tconcepts: 121\n",
      "Archivo 4 - AS - chiavenato-cap9:\n",
      "\ttokens: 29412\n",
      "\tconcepts: 323\n",
      "Archivo 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.:\n",
      "\ttokens: 355693\n",
      "\tconcepts: 4712\n",
      "Archivo 4 - AS - EL PROCESO ADMINISTRATIVO:\n",
      "\ttokens: 1645\n",
      "\tconcepts: 17\n",
      "Archivo 4 - AS - Elaboracion_de_programas_de_capacitacion:\n",
      "\ttokens: 9854\n",
      "\tconcepts: 47\n",
      "Archivo 4 - AS - Ergonomia - 4°9°:\n",
      "\ttokens: 796\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Ergonomía 4k10:\n",
      "\ttokens: 735\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Gestión CPD:\n",
      "\ttokens: 849\n",
      "\tconcepts: 4\n",
      "Archivo 4 - AS - Habilidades Blandas 4k9:\n",
      "\ttokens: 1335\n",
      "\tconcepts: 6\n",
      "Archivo 4 - AS - Implementación de un Data Center:\n",
      "\ttokens: 1719\n",
      "\tconcepts: 7\n",
      "Archivo 4 - AS - Ingeniería Social 4k9:\n",
      "\ttokens: 967\n",
      "\tconcepts: 7\n",
      "Archivo 4 - AS - Ingeniería Social:\n",
      "\ttokens: 989\n",
      "\tconcepts: 5\n",
      "Archivo 4 - AS - Inteligencia Emocional - 4°10°:\n",
      "\ttokens: 2950\n",
      "\tconcepts: 12\n",
      "Archivo 4 - AS - Inteligencia Emocional - 4°9°:\n",
      "\ttokens: 1809\n",
      "\tconcepts: 8\n",
      "Archivo 4 - AS - Planes-de-Contingencia:\n",
      "\ttokens: 971\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Resumen Global Administración de Sistemas de Información:\n",
      "\ttokens: 2950\n",
      "\tconcepts: 33\n",
      "Archivo 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS:\n",
      "\ttokens: 938\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Sniffers_y_escaneo_de_puertos:\n",
      "\ttokens: 828\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Teletrabajo-4k10:\n",
      "\ttokens: 729\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Teletrabajo-4k9:\n",
      "\ttokens: 795\n",
      "\tconcepts: 6\n",
      "Archivo 4 - AS - Trello-4°9°:\n",
      "\ttokens: 897\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - WLAN TT:\n",
      "\ttokens: 677\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - WLAN:\n",
      "\ttokens: 1353\n",
      "\tconcepts: 1\n",
      "Archivo 4 - ICS - 2.-principiosingenieriasoftware:\n",
      "\ttokens: 4371\n",
      "\tconcepts: 68\n",
      "Archivo 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American:\n",
      "\ttokens: 10449\n",
      "\tconcepts: 28\n",
      "Archivo 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx:\n",
      "\ttokens: 1838\n",
      "\tconcepts: 0\n",
      "Archivo 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software:\n",
      "\ttokens: 10348\n",
      "\tconcepts: 127\n",
      "Archivo 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software:\n",
      "\ttokens: 1275\n",
      "\tconcepts: 16\n",
      "Archivo 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia:\n",
      "\ttokens: 11515\n",
      "\tconcepts: 101\n",
      "Archivo 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD:\n",
      "\ttokens: 651\n",
      "\tconcepts: 0\n",
      "Archivo 4 - IO - U5 - ADMINISTRACION DE PROYECTOS:\n",
      "\ttokens: 3256\n",
      "\tconcepts: 44\n",
      "Archivo 4 - IO - U6_1-GESTION DE INVENTARIOS:\n",
      "\ttokens: 1884\n",
      "\tconcepts: 9\n",
      "Archivo 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC:\n",
      "\ttokens: 712\n",
      "\tconcepts: 3\n",
      "Archivo 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC:\n",
      "\ttokens: 0\n",
      "\tconcepts: 0\n",
      "Archivo 4 - RD - Kurose-Ross:\n",
      "\ttokens: 514636\n",
      "\tconcepts: 18855\n",
      "Archivo 4 - RD - Resumen Redes de Datos:\n",
      "\ttokens: 21439\n",
      "\tconcepts: 898\n",
      "Archivo 4 - RD - Tanenbaum:\n",
      "\ttokens: 526547\n",
      "\tconcepts: 11978\n",
      "Archivo 4 - S - Resumen 2do Parcial:\n",
      "\ttokens: 3044\n",
      "\tconcepts: 29\n",
      "Archivo 4 - TA - Guía 1. Sistemas de control automático:\n",
      "\ttokens: 11294\n",
      "\tconcepts: 286\n",
      "Archivo 4 - UXUI - Diseño UX UNIDAD 1:\n",
      "\ttokens: 23304\n",
      "\tconcepts: 447\n",
      "Archivo 5 - GG - Resumen Cambio Organizacional:\n",
      "\ttokens: 415\n",
      "\tconcepts: 2\n",
      "Archivo 5 - GG - Resumen Parcial 26-5:\n",
      "\ttokens: 1588\n",
      "\tconcepts: 17\n",
      "Archivo 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8:\n",
      "\ttokens: 23621\n",
      "\tconcepts: 502\n",
      "Archivo 5 - SSI - Clase U2-3 Disposicion ONTI 2015:\n",
      "\ttokens: 6023\n",
      "\tconcepts: 191\n",
      "Archivo 5 - SSI - DE641-2021_Anexo:\n",
      "\ttokens: 4845\n",
      "\tconcepts: 137\n",
      "Archivo 5 - SSI - Disposicion 1-2015_PSI:\n",
      "\ttokens: 54786\n",
      "\tconcepts: 1488\n",
      "Archivo 5 - SSI - Guia_apoyo_SGSI:\n",
      "\ttokens: 13025\n",
      "\tconcepts: 247\n",
      "Archivo 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad:\n",
      "\ttokens: 8180\n",
      "\tconcepts: 200\n",
      "Archivo 5 - SSI - M5-Privacidad:\n",
      "\ttokens: 23502\n",
      "\tconcepts: 703\n",
      "Archivo 5 - SSI - Magerit_v3_libro1_metodo:\n",
      "\ttokens: 61717\n",
      "\tconcepts: 1461\n",
      "Archivo 5 - SSI - Magerit_v3_libro2_catálogo de elementos:\n",
      "\ttokens: 23621\n",
      "\tconcepts: 502\n",
      "Archivo 5 - SSI - Magerit_v3_libro3_guía de técnicas:\n",
      "\ttokens: 17946\n",
      "\tconcepts: 303\n",
      "Archivo 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI:\n",
      "\ttokens: 6089\n",
      "\tconcepts: 97\n",
      "Archivo 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información:\n",
      "\ttokens: 10286\n",
      "\tconcepts: 331\n",
      "Archivo 5 - SSI - Resumen U2 - Disposición ONTI 1 2015:\n",
      "\ttokens: 4017\n",
      "\tconcepts: 168\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[92m\")\n",
    "for archivo, item in metrics_2.items():\n",
    "    print(\"Archivo \" + archivo + \":\")\n",
    "    for nombre, valor in item.items():\n",
    "        print(\"\\t\" + nombre + \": \" + str(valor))\n",
    "print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyqXH3y2IKST"
   },
   "source": [
    "# Creación dataset\n",
    "A partir de los tokens numéricos, se iterará por cada secuencia de token con un nuevo tamaño de ventana, mayor, tratando de distinguir conceptos relacionados.\n",
    "\n",
    "Esta ventana se centrará en cada token (no -1), almacenando en un diccionario el token central, los tokens en el contexto y ejemplos negativos (para evitar que la red neuronal, al entrenar, aprenda que todos los tokens siempre están relacionados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saoga\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.7.1-cp312-cp312-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/216.1 MB 12.6 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 5.5/216.1 MB 16.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 8.9/216.1 MB 15.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 13.6/216.1 MB 17.5 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 18.1/216.1 MB 18.4 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 22.5/216.1 MB 18.8 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 26.0/216.1 MB 18.9 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 29.9/216.1 MB 18.8 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 33.6/216.1 MB 18.4 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 38.0/216.1 MB 18.6 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 38.3/216.1 MB 18.6 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 39.3/216.1 MB 15.9 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 43.0/216.1 MB 16.0 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 46.7/216.1 MB 16.1 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 49.0/216.1 MB 15.8 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 51.9/216.1 MB 15.7 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 52.2/216.1 MB 15.7 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 53.2/216.1 MB 14.4 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 55.6/216.1 MB 14.1 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 57.9/216.1 MB 13.9 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 59.8/216.1 MB 13.8 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 62.1/216.1 MB 13.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 64.7/216.1 MB 13.5 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 66.8/216.1 MB 13.4 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 69.5/216.1 MB 13.4 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 72.1/216.1 MB 13.3 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 73.9/216.1 MB 13.2 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 76.0/216.1 MB 13.1 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 78.1/216.1 MB 12.9 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 80.5/216.1 MB 12.9 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 82.8/216.1 MB 12.9 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 85.2/216.1 MB 12.8 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 87.6/216.1 MB 12.7 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 90.2/216.1 MB 12.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 90.4/216.1 MB 12.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 90.4/216.1 MB 12.7 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 91.0/216.1 MB 11.8 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 93.1/216.1 MB 11.8 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 95.9/216.1 MB 11.8 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 99.6/216.1 MB 11.9 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 103.3/216.1 MB 12.1 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 107.7/216.1 MB 12.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 112.5/216.1 MB 12.6 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 118.0/216.1 MB 12.9 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 121.9/216.1 MB 13.0 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 124.0/216.1 MB 13.0 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 128.7/216.1 MB 13.1 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 132.1/216.1 MB 13.2 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 135.0/216.1 MB 13.2 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 139.2/216.1 MB 13.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 142.1/216.1 MB 13.3 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 146.5/216.1 MB 13.5 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 149.4/216.1 MB 13.5 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 154.1/216.1 MB 13.7 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 158.6/216.1 MB 13.8 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 163.8/216.1 MB 14.0 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 169.6/216.1 MB 14.2 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 174.3/216.1 MB 14.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 179.0/216.1 MB 14.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 183.5/216.1 MB 14.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 188.5/216.1 MB 14.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 193.5/216.1 MB 15.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 199.0/216.1 MB 15.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 205.0/216.1 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 209.7/216.1 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/216.1 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.0/216.1 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 5.8/6.3 MB 27.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 25.7 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 networkx-3.5 sympy-1.14.0 torch-2.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JampbY-K6fZ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import collections\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "\"\"\"\n",
    "def subsample(oraciones):\n",
    "  # Subsample high-frequency words.\n",
    "  # Comprueba que oraciones es una lista de listas\n",
    "  if oraciones and isinstance(oraciones[0], list):\n",
    "    #Transforma una lista anidada en una lista simple\n",
    "    tokens = [token for line in oraciones for token in line]\n",
    "  counter_obj = collections.Counter()\n",
    "  counter_obj.update(tokens)\n",
    "  num_tokens = sum(counter_obj.values())\n",
    "\n",
    "  # Devuelve true si hay que conservar el token\n",
    "  def keep(token):\n",
    "      return(random.uniform(0, 1) <\n",
    "              math.sqrt(1e-4 / counter_obj[token] * num_tokens))\n",
    "\n",
    "  return ([[token for token in line if keep(token)] for line in oraciones],\n",
    "          counter_obj)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "  \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
    "  centers, contexts = [], []\n",
    "  for line in corpus:\n",
    "    # Para formar un par de \"palabra central--palabra de contexto\",\n",
    "    # cada oración debe tener al menos 2 palabras\n",
    "    if len(line) < 2:\n",
    "      continue\n",
    "    centers += line\n",
    "    for i in range(len(line)):  # Ventana de contexto centrada en `i`\n",
    "      window_size = random.randint(1, max_window_size)\n",
    "      indices = list(range(max(0, i - window_size),\n",
    "                            min(len(line), i + 1 + window_size)))\n",
    "      # Excluir la palabra central de las palabras de contexto\n",
    "      indices.remove(i)\n",
    "      contexts.append([line[idx] for idx in indices])\n",
    "  return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "  \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
    "  def __init__(self, sampling_weights):\n",
    "    # Exclude\n",
    "    self.population = list(range(1, len(sampling_weights) + 1))\n",
    "    self.sampling_weights = sampling_weights\n",
    "    self.candidates = []\n",
    "    self.i = 0\n",
    "\n",
    "  def draw(self):\n",
    "    if self.i == len(self.candidates):\n",
    "      # Cache `k` random sampling results\n",
    "      self.candidates = random.choices(\n",
    "          self.population, self.sampling_weights, k=10000)\n",
    "      self.i = 0\n",
    "    self.i += 1\n",
    "    return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"Devuelve palabras ruidosas para muestreo negativo.\"\"\"\n",
    "    # Pesos de muestreo para palabras con índices 1, 2, ...\n",
    "    # (índice 0 es el token <unk> excluido) en el vocabulario\n",
    "    tokens = vocab.get_itos()\n",
    "    sampling_weights = [counter[tokens[i]]**0.75\n",
    "                        for i in range(1, len(tokens))]\n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # Las palabras ruidosas no pueden ser de contexto\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clean_token(token: \"spacy.tokens.Token\") -> bool:\n",
    "    \"\"\"Return True if the token should be kept for downstream processing.\"\"\"\n",
    "    return not (\n",
    "        token.is_punct\n",
    "        or token.is_space\n",
    "        or token.is_stop\n",
    "        or len(token.text) == 1\n",
    "    )\n",
    "\n",
    "\n",
    "def load_sentences_from_plain(\n",
    "    plain_path,\n",
    "    window_size,\n",
    "    lowercase = True,\n",
    "):\n",
    "    \n",
    "    sentences= []\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    raw_files = [os.path.splitext(f)[0] for f in listdir(plain_path) if isfile(join(plain_path, f))]\n",
    "    for f in raw_files:\n",
    "        with open(f\"{plain_path}/{f}.txt\", \"rb\") as pf:\n",
    "            text = pf.read().decode(\"utf-8\")\n",
    "            doc = nlp(text)\n",
    "            for sent in doc.sents:\n",
    "                tokens = [tok.text for tok in sent if is_clean_token(tok)]\n",
    "                if lowercase:\n",
    "                    tokens = [t.lower() for t in tokens]\n",
    "                if not tokens:\n",
    "                    continue\n",
    "\n",
    "                if window_size > 0:\n",
    "                    # Break long sentences into fixed-size chunks\n",
    "                    for i in range(0, len(tokens), window_size):\n",
    "                        chunk = tokens[i : i + window_size]\n",
    "                        if chunk:\n",
    "                            sentences.append(chunk)\n",
    "                else:\n",
    "                    sentences.append(tokens)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(data):\n",
    "  max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "  centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "  for center, context, negative in data:\n",
    "    centers += [center]\n",
    "    cur_len = len(context) + len(negative)\n",
    "    contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "    masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "  return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
    "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_isi(batch_size, max_window_size, num_noise_words):\n",
    "    sentences = load_sentences_from_plain(\"./data/plain\", window_size=max_window_size)\n",
    "    sentences_ss, counter = subsample(sentences)\n",
    "    corpus = sentences_ss            # here they’re already token lists\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocabulary, counter, num_noise_words)\n",
    "\n",
    "    class ISIDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, centers, contexts, negatives):\n",
    "            assert len(centers) == len(contexts) == len(negatives)\n",
    "            self.centers = centers\n",
    "            self.contexts = contexts\n",
    "            self.negatives = negatives\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return (self.centers[index], self.contexts[index],\n",
    "                    self.negatives[index])\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.centers)\n",
    "\n",
    "    dataset = ISIDataset(all_centers, all_contexts, all_negatives)\n",
    "\n",
    "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                      collate_fn=collate_batch)\n",
    "    return data_iter, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, vocabulario = load_data_isi(512, 5, 5)\n",
    "names = ['centers', 'contexts_negatives', 'lengths', 'labels']\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-QSj_DQK63i"
   },
   "source": [
    "Hecho esto, podemos finalmente armar nuestro dataset. El mismo retornará (mediante get_item()) un centro, su contexto y sus ejemplos negativos.\n",
    "\n",
    "A partir de este Dataset, a su vez, se generará un DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nBaNJdUJNp_7"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec-elIZ3LhXY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m window_size = \u001b[32m5\u001b[39m\n\u001b[32m     63\u001b[39m num_negatives = \u001b[32m5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m train_loader = create_data_loader(tokens_path, \u001b[43mvocabulary\u001b[49m, \n\u001b[32m     66\u001b[39m                                 batch_size=batch_size,\n\u001b[32m     67\u001b[39m                                 window_size=window_size, \n\u001b[32m     68\u001b[39m                                 num_negatives=num_negatives)\n",
      "\u001b[31mNameError\u001b[39m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u6Gf_hIMYnm"
   },
   "source": [
    "A continuación, se armará la estructura de la red neuronal mediante skipgram, utilizando capas Embedding de pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjNlBeIxM1my"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOVN22WTM1-j"
   },
   "source": [
    "Como función de pérdida, se utilizará entropía cruzada binaria (Sigmoidea). Esto es así pues requerimos clasificar dos conceptos según si están o no relacionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dcyjlt4YND8Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0f_w07NNEZ9"
   },
   "source": [
    "Se optó por abarcar todo el entrenamiento en una misma función. La misma incluye la inicialización de variables y el ciclo de entrenamiento en sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary from file\n",
    "with open(\"./data/vocabulary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # Each line contains a word\n",
    "    vocabulary = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def tokenize_concepts(raw_path, plain_path, tokens_conceptos_path, vocabulary, window_size, esp):\n",
    "    \"\"\"\n",
    "    Tokenize documents by identifying words from vocabulary using a sliding window approach.\n",
    "    \n",
    "    Args:\n",
    "        raw_path (str): Path to raw documents\n",
    "        plain_path (str): Path to plain text documents\n",
    "        tokens_conceptos_path (str): Path to save concept tokens\n",
    "        vocabulary (list): List of words to identify in the text\n",
    "        window_size (int): Base window size (will be multiplied by 3)\n",
    "        esp: spaCy model for Spanish tokenization\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics about tokens and concepts found per file\n",
    "    \"\"\"\n",
    "    raw_files = [os.path.splitext(f)[0] for f in listdir(raw_path) if isfile(join(raw_path, f))]\n",
    "    metrics = {}\n",
    "    window_size_concept_tokenization = window_size * 3\n",
    "\n",
    "    for f in raw_files:\n",
    "        print(\"\\033[94mTokenizando archivo por conceptos: \" + f + \"\\033[0m\")\n",
    "        metrics[f] = {}\n",
    "        found_concepts = 0\n",
    "        recent_concepts = {}  # Track recently found concepts\n",
    "\n",
    "        # Read and tokenize the plain text file\n",
    "        with open(f\"{plain_path}/{f}.txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "            tokens = [t.text for t in esp.tokenizer(txt)]\n",
    "            metrics[f][\"tokens\"] = len(tokens)\n",
    "\n",
    "            # Write concept tokens to output file\n",
    "            with open(f\"{tokens_conceptos_path}/{f}.txt\", \"wb\") as tnf:\n",
    "                unks = 0  # Counter for unknown tokens\n",
    "\n",
    "                for i in range(len(tokens) - window_size_concept_tokenization):\n",
    "                    window = tokens[i:i+window_size_concept_tokenization]\n",
    "                    \n",
    "                    # Decrease counters for recent concepts and cleanup\n",
    "                    for k, v in list(recent_concepts.items()):\n",
    "                        if v > 0:\n",
    "                            recent_concepts[k] -= 1\n",
    "                        if recent_concepts[k] == 0:\n",
    "                            del recent_concepts[k]\n",
    "\n",
    "                    unks += 1\n",
    "\n",
    "                    # Check each word in vocabulary against the current window\n",
    "                    for ix, word in enumerate(vocabulary):\n",
    "                        # If the word is in the window and not recently found\n",
    "                        if word in window and ix not in recent_concepts:\n",
    "                            tnf.write(f\"-{unks-1} {ix} \".encode(\"utf-8\"))\n",
    "                            unks = 0\n",
    "                            recent_concepts[ix] = window_size_concept_tokenization\n",
    "                            found_concepts += 1\n",
    "\n",
    "            metrics[f][\"concepts\"] = found_concepts\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Run concept tokenization\n",
    "metrics = tokenize_concepts(\n",
    "    raw_path=raw_path,\n",
    "    plain_path=plain_path,\n",
    "    tokens_conceptos_path=tokens_conceptos_path,\n",
    "    vocabulary=vocabulary,\n",
    "    window_size=5,\n",
    "    esp=esp\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "for file, file_metrics in metrics.items():\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Total tokens: {file_metrics['tokens']}\")\n",
    "    print(f\"Concepts found: {file_metrics['concepts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_concepts(raw_path, plain_path, tokens_conceptos_path, vocabulary, window_size, esp):\n",
    "    \"\"\"\n",
    "    Tokenize documents by identifying pre-defined concepts using a sliding window approach.\n",
    "    \n",
    "    Args:\n",
    "        raw_path (str): Path to raw documents\n",
    "        plain_path (str): Path to plain text documents\n",
    "        tokens_conceptos_path (str): Path to save concept tokens\n",
    "        vocabulary (list): List of concepts to identify\n",
    "        window_size (int): Base window size (will be multiplied by 3)\n",
    "        esp: spaCy model for Spanish tokenization\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics about tokens and concepts found per file\n",
    "    \"\"\"\n",
    "    raw_files = [os.path.splitext(f)[0] for f in listdir(raw_path) if isfile(join(raw_path, f))]\n",
    "    metrics = {}\n",
    "    window_size_concept_tokenization = window_size * 3\n",
    "\n",
    "    for f in raw_files:\n",
    "        print(\"\\033[94mTokenizando archivo por conceptos: \" + f + \"\\033[0m\")\n",
    "        metrics[f] = {}\n",
    "        found_concepts = 0\n",
    "        recent_concepts = {}  # Track recently found concepts\n",
    "\n",
    "        # Read and tokenize the plain text file\n",
    "        with open(f\"{plain_path}/{f}.txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "            tokens = [t.text for t in esp.tokenizer(txt)]\n",
    "            metrics[f][\"tokens\"] = len(tokens)\n",
    "\n",
    "            # Write concept tokens to output file\n",
    "            with open(f\"{tokens_conceptos_path}/{f}.txt\", \"wb\") as tnf:\n",
    "                unks = 0  # Counter for unknown tokens\n",
    "\n",
    "                for i in range(len(tokens) - window_size_concept_tokenization):\n",
    "                    window = tokens[i:i+window_size_concept_tokenization]\n",
    "                    \n",
    "                    # Decrease counters for recent concepts and cleanup\n",
    "                    for k, v in list(recent_concepts.items()):\n",
    "                        if v > 0:\n",
    "                            recent_concepts[k] -= 1\n",
    "                        if recent_concepts[k] == 0:\n",
    "                            del recent_concepts[k]\n",
    "\n",
    "                    unks += 1\n",
    "\n",
    "                    # Check each concept against the current window\n",
    "                    for ix, concept in enumerate(vocabulary):\n",
    "                        # If all words of the concept are in the window and concept not recently found\n",
    "                        if all(word in window for word in concept) and ix not in recent_concepts:\n",
    "                            tnf.write(f\"-{unks-1} {ix} \".encode(\"utf-8\"))\n",
    "                            unks = 0\n",
    "                            recent_concepts[ix] = window_size_concept_tokenization\n",
    "                            found_concepts += 1\n",
    "\n",
    "            metrics[f][\"concepts\"] = found_concepts\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import spacy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# Load Spanish language model\n",
    "esp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Paths\n",
    "raw_path = \"./data/raw\"\n",
    "plain_path = \"./data/plain\"\n",
    "tokens_path = \"./data/tokens\"\n",
    "tokens_conceptos_path = \"./data/tokens_conceptos\"\n",
    "os.makedirs(tokens_conceptos_path, exist_ok=True)\n",
    "\n",
    "# Create vocabulary from tokens with frequency filtering\n",
    "vocabulary = Vocab(token_iterator(tokens_path), min_freq=50, max_freq=100)\n",
    "\n",
    "# Convert vocabulary to list of tuples for concept detection\n",
    "# Each concept is a single word in this case, but we keep it as a tuple for consistency\n",
    "vocab_concepts = [(token,) for token in vocabulary.idx_to_token if token != '<unk>']\n",
    "print(\"\\033[92mTamaño del vocabulario: \" + str(len(vocab_concepts)) + \"\\033[0m\")\n",
    "\n",
    "# Run concept tokenization\n",
    "metrics = tokenize_concepts(\n",
    "    raw_path=raw_path,\n",
    "    plain_path=plain_path,\n",
    "    tokens_conceptos_path=tokens_conceptos_path,\n",
    "    vocabulary=vocab_concepts,  # Pass the list of tuples instead of Vocab object\n",
    "    window_size=5,\n",
    "    esp=esp\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "for file, file_metrics in metrics.items():\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Total tokens: {file_metrics['tokens']}\")\n",
    "    print(f\"Concepts found: {file_metrics['concepts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import spacy\n",
    "\n",
    "# Load Spanish language model\n",
    "esp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Paths\n",
    "raw_path = \"./data/raw\"\n",
    "plain_path = \"./data/plain\"\n",
    "tokens_conceptos_path = \"./data/tokens_conceptos\"\n",
    "os.makedirs(tokens_conceptos_path, exist_ok=True)\n",
    "\n",
    "# Load vocabulary from file\n",
    "with open(\"./data/vocabulary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # Each line contains a tuple of words that form a concept\n",
    "    vocabulary = [eval(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "# Run concept tokenization\n",
    "metrics = tokenize_concepts(\n",
    "    raw_path=raw_path,\n",
    "    plain_path=plain_path,\n",
    "    tokens_conceptos_path=tokens_conceptos_path,\n",
    "    vocabulary=vocabulary,\n",
    "    window_size=5,\n",
    "    esp=esp\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "for file, file_metrics in metrics.items():\n",
    "    print(f\"\\nFile: {file}\")\n",
    "    print(f\"Total tokens: {file_metrics['tokens']}\")\n",
    "    print(f\"Concepts found: {file_metrics['concepts']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3vBgobNNbKZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLUJawwxNbkC"
   },
   "source": [
    "Se generó una función auxiliar para el entrenamiento por medio de GPU, en caso de estar disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_yUK2giNmTR"
   },
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o86EEabpNsqR"
   },
   "source": [
    "Finalmente, se realizó el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QimpOHcN2XJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FATzuHy4N5Bo"
   },
   "source": [
    "Para verificar la funcionalidad final que buscamos en el proyecto, se planteó la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUEb4ATSOFNs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U7YBaVVOGVX"
   },
   "source": [
    "Algunos ejemplos de la misma serían:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adehXDcCOF_D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkOFf8bDH8q9ApCjhlpGB6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
