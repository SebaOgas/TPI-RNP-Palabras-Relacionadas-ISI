{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmh3bN-o6LYh"
   },
   "source": [
    "# Palabras Relacionadas - Dataset\n",
    "\n",
    "En este notebook se explica en detalle el dataset y el procesamiento que se requirió para dejarlo listo para la siguiente etapa del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se realizaron algunas importaciones, se configuró el directorio de trabajo y se definió una función de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3630,
     "status": "ok",
     "timestamp": 1749157967269,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "8LOgWv-f8Ems",
    "outputId": "ed04fe33-eec5-43ee-d047-ee224073420f"
   },
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saoga\\OneDrive\\Escritorio\\Repos\\TPI-RNP-Palabras-Relacionadas-ISI\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pathlib\n",
    "\n",
    "# Seteo el path al root del proyecto\n",
    "dev_folder = 'dev'\n",
    "folders = os.getcwd().split('/')\n",
    "if (len(folders) == 1):\n",
    "    folders = folders[0].split('\\\\')\n",
    "if(folders[-1] == dev_folder):\n",
    "    os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(path):\n",
    "    return [os.path.splitext(f)[0] for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, establecimos los subdirectorios con los que se trabajó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"./data/raw\"\n",
    "plain_path = \"./data/plain\"\n",
    "tokens_clean_path = \"./data/tokens_clean\"\n",
    "tokens_full_path = \"./data/tokens_full\"\n",
    "vocabularies_path = \"./data/vocabularies\"\n",
    "tokens_concepts_path = \"./data/tokens_concepts\"\n",
    "datasets_path = \"./data/datasets\"\n",
    "models_path = \"./data/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se buscó y seleccionó múltiples textos en formato PDF pertenencientes a distintas materias de la carrera Ingeniería en Sistemas de Información, Universidad Tecnológica Nacional Facultad Regional Mendoza.\n",
    "\n",
    "Estos, fueron cargados en la carpeta /data/raw.\n",
    "\n",
    "Se siguió como convención para los nombres el número del año de la materia, seguido de un guión, una abreviatura del nombre de la materia, otro guión y el nombre original del material.\n",
    "\n",
    "Utilizando algunas librerías de python, se convirtió cada archivo PDF en un archivo txt con su contenido, en /data/plain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_plain(filename):\n",
    "    reader = PdfReader(filename)\n",
    "    content = \"\"\n",
    "    for p in reader.pages:\n",
    "        content += p.extract_text(\n",
    "            extraction_mode=\"plain\",\n",
    "            layout_mode_space_vertically=False)\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 894532,
     "status": "ok",
     "timestamp": 1749159253392,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "OUVAxYUy7vN7",
    "outputId": "142c1885-aac0-4f81-d7e9-dc816eade658"
   },
   "outputs": [],
   "source": [
    "def raw_to_plain(raw_path, plain_path, converter):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    raw_files = get_filenames(raw_path)\n",
    "\n",
    "    pathlib.Path(plain_path).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    for f in raw_files:\n",
    "        print(\"\\033[94mConvirtiendo archivo: \" + f + \"\\033[0m\")\n",
    "\n",
    "        sf = f.split(\"-\")\n",
    "        anio = sf[0].strip()\n",
    "        materia = sf[1].strip()\n",
    "        if (not anio in metrics):\n",
    "            metrics[anio] = {}\n",
    "\n",
    "        if (not materia in metrics[anio]):\n",
    "            metrics[anio][materia] = 0\n",
    "\n",
    "        metrics[anio][materia] += os.path.getsize(raw_path + \"/\" + f + \".pdf\")\n",
    "\n",
    "        content = converter(raw_path + \"/\" + f + \".pdf\")\n",
    "        \n",
    "        with open(plain_path + \"/\" + f + \".txt\", \"ab\") as t:\n",
    "            t.write(content.encode(\"utf-8\"))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mConvirtiendo archivo: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mConvirtiendo archivo: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m raw_to_plain_metrics = \u001b[43mraw_to_plain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_pdf_to_plain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[33m[92m\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a, materias \u001b[38;5;129;01min\u001b[39;00m raw_to_plain_metrics.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mraw_to_plain\u001b[39m\u001b[34m(raw_path, plain_path, converter)\u001b[39m\n\u001b[32m     19\u001b[39m     metrics[anio][materia] = \u001b[32m0\u001b[39m\n\u001b[32m     21\u001b[39m metrics[anio][materia] += os.path.getsize(raw_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m.pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m content = \u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(plain_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mab\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[32m     26\u001b[39m     t.write(content.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mconvert_pdf_to_plain\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m      3\u001b[39m content = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m reader.pages:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     content += \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextraction_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayout_mode_space_vertically\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\_page.py:2350\u001b[39m, in \u001b[36mPageObject.extract_text\u001b[39m\u001b[34m(self, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, extraction_mode, *args, **kwargs)\u001b[39m\n\u001b[32m   2347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   2348\u001b[39m     orientations = (orientations,)\n\u001b[32m-> \u001b[39m\u001b[32m2350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2352\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2353\u001b[39m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\_page.py:2092\u001b[39m, in \u001b[36mPageObject._extract_text\u001b[39m\u001b[34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[39m\n\u001b[32m   2090\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2091\u001b[39m     xobj = resources_dict[\u001b[33m\"\u001b[39m\u001b[33m/XObject\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mxobj\u001b[49m\u001b[43m[\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33m/Subtype\u001b[39m\u001b[33m\"\u001b[39m] != \u001b[33m\"\u001b[39m\u001b[33m/Image\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2093\u001b[39m         text = \u001b[38;5;28mself\u001b[39m.extract_xform_text(\n\u001b[32m   2094\u001b[39m             xobj[operands[\u001b[32m0\u001b[39m]],  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2095\u001b[39m             orientations,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2099\u001b[39m             visitor_text,\n\u001b[32m   2100\u001b[39m         )\n\u001b[32m   2101\u001b[39m         output += text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:477\u001b[39m, in \u001b[36mDictionaryObject.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) -> PdfObject:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_base.py:370\u001b[39m, in \u001b[36mIndirectObject.get_object\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_object\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[\u001b[33m\"\u001b[39m\u001b[33mPdfObject\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\_reader.py:468\u001b[39m, in \u001b[36mPdfReader.get_object\u001b[39m\u001b[34m(self, indirect_reference)\u001b[39m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PdfReadError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected loop with self reference for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindirect_reference\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    467\u001b[39m \u001b[38;5;28mself\u001b[39m._known_objects.add(current_object)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m retval = \u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;28mself\u001b[39m._known_objects.remove(current_object)\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# override encryption is used for the /Encrypt dictionary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1461\u001b[39m, in \u001b[36mread_object\u001b[39m\u001b[34m(stream, pdf, forced_encoding)\u001b[39m\n\u001b[32m   1459\u001b[39m     stream.seek(-\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# reset to start\u001b[39;00m\n\u001b[32m   1460\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m peek == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<<\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDictionaryObject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_hex_string_from_stream(stream, forced_encoding)\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tok == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:596\u001b[39m, in \u001b[36mDictionaryObject.read_from_stream\u001b[39m\u001b[34m(stream, pdf, forced_encoding)\u001b[39m\n\u001b[32m    594\u001b[39m     tok = read_non_whitespace(stream)\n\u001b[32m    595\u001b[39m     stream.seek(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m     value = \u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pdf.strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1455\u001b[39m, in \u001b[36mread_object\u001b[39m\u001b[34m(stream, pdf, forced_encoding)\u001b[39m\n\u001b[32m   1453\u001b[39m stream.seek(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# reset to start\u001b[39;00m\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tok == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNameObject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tok == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1457\u001b[39m     \u001b[38;5;66;03m# hexadecimal string OR dictionary\u001b[39;00m\n\u001b[32m   1458\u001b[39m     peek = stream.read(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\generic\\_base.py:863\u001b[39m, in \u001b[36mNameObject.read_from_stream\u001b[39m\u001b[34m(stream, pdf)\u001b[39m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name != NameObject.prefix:\n\u001b[32m    862\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PdfReadError(\u001b[33m\"\u001b[39m\u001b[33mName read error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m name += \u001b[43mread_until_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNameObject\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelimiter_pattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    865\u001b[39m     \u001b[38;5;66;03m# Name objects should represent irregular characters\u001b[39;00m\n\u001b[32m    866\u001b[39m     \u001b[38;5;66;03m# with a '#' followed by the symbol's hex number\u001b[39;00m\n\u001b[32m    867\u001b[39m     name = NameObject.unnumber(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pypdf\\_utils.py:243\u001b[39m, in \u001b[36mread_until_regex\u001b[39m\u001b[34m(stream, regex)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    242\u001b[39m     stream.seek(m.start() - (\u001b[38;5;28mlen\u001b[39m(name) + \u001b[38;5;28mlen\u001b[39m(tok)), \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     name = (name + tok)[: \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    245\u001b[39m name += tok\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "raw_to_plain_metrics = raw_to_plain(raw_path, plain_path, convert_pdf_to_plain)\n",
    "\n",
    "print(\"\\033[92m\")\n",
    "for a, materias in raw_to_plain_metrics.items():\n",
    "    print(\"Año \" + a + \":\")\n",
    "    for m, tamano in materias.items():\n",
    "        print(\"\\tMateria: \" + m + \" - \" + str(round(tamano/1000000,2)) +\"MB\")\n",
    "print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Oh3rLH97OW"
   },
   "source": [
    "A continuación, se tokenizó los archivos planos txt, generando dos nuevos archivos txt nuevos por cada archivo plano donde cada línea es un token. El archivo \"full\" contiene todos los tokens, mientras que el \"clean\" solo contiene los que se consideran potencialmente relevantes para detectar conceptos.\n",
    "\n",
    "Por ejemplo, no se considera limpios a los tokens correspondientes a signos de puntuación, espacios o palabras muy frecuentes en el lenguaje.\n",
    "\n",
    "Esto se logró gracias a Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22248,
     "status": "ok",
     "timestamp": 1749159426548,
     "user": {
      "displayName": "Seba O",
      "userId": "01825810927577519836"
     },
     "user_tz": 180
    },
    "id": "8dxbA4r29x02",
    "outputId": "ebbd9d1e-009c-491b-c147-e44856e84623"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "esp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token():\n",
    "    def __init__(self, text, is_clean):\n",
    "        self.text = text\n",
    "        self.is_clean = is_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_esp_spacy(txt):\n",
    "    def is_clean_token(token):\n",
    "        return not (\n",
    "            token.is_punct or\n",
    "            token.is_space or\n",
    "            token.is_stop or\n",
    "            len(token) == 1)\n",
    "    \n",
    "    tokens = esp.tokenizer(txt)\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    for token in tokens:\n",
    "        ret.append(Token(token.text, is_clean_token(token)))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9mpV1K6D5H0n"
   },
   "outputs": [],
   "source": [
    "def tokenize(plain_path, tokens_full_path, tokens_clean_path, tokenizer):\n",
    "    plain_files = get_filenames(plain_path)\n",
    "\n",
    "    pathlib.Path(tokens_full_path).mkdir(parents=True, exist_ok=True) \n",
    "    pathlib.Path(tokens_clean_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    for f in plain_files:\n",
    "        print(\"\\033[94mTokenizando archivo: \" + f + \"\\033[0m\")\n",
    "\n",
    "        with open(plain_path + \"/\" + f + \".txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "            tokens = tokenizer(txt)\n",
    "            with open(tokens_clean_path + \"/\" + f + \".txt\", \"wb\") as tcf:\n",
    "                with open(tokens_full_path + \"/\" + f + \".txt\", \"wb\") as tff:\n",
    "                    for token in tokens:\n",
    "                        enc_token = (token.text + \"\\n\").encode(\"utf-8\")\n",
    "                        tff.write(enc_token)\n",
    "                        if (token.is_clean):\n",
    "                            tcf.write(enc_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTokenizando archivo: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mTokenizando archivo: 2 - SO - Tanenbaum\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_full_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_clean_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_esp_spacy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(plain_path, tokens_full_path, tokens_clean_path, tokenizer)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(plain_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pf:\n\u001b[32m     11\u001b[39m     txt = pf.read().decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokens_clean_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tcf:\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tokens_full_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tff:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtokenizer_esp_spacy\u001b[39m\u001b[34m(txt)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_clean_token\u001b[39m(token):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m      4\u001b[39m         token.is_punct \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m      5\u001b[39m         token.is_space \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m      6\u001b[39m         token.is_stop \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mlen\u001b[39m(token) == \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tokens = \u001b[43mesp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m ret = []\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokenizer.pyx:160\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokenizer.pyx:196\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._tokenize_affixes\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokenizer.pyx:400\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._tokenize\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\tokenizer.pyx:480\u001b[39m, in \u001b[36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\vocab.pyx:205\u001b[39m, in \u001b[36mspacy.vocab.Vocab.get\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\vocab.pyx:234\u001b[39m, in \u001b[36mspacy.vocab.Vocab._new_lexeme\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\lang\\lex_attrs.py:145\u001b[39m, in \u001b[36mlower\u001b[39m\u001b[34m(string)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlower\u001b[39m(string: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstring\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tokenize(plain_path, tokens_full_path, tokens_clean_path, tokenizer_esp_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh5kn_mP_qZF"
   },
   "source": [
    "Una vez que se tienen los archivos con los tokens, deseamos detectar conceptos dentro de ellos. Estos conceptos formarán luego nuestro vocabulario.\n",
    "\n",
    "Se utilizará un algoritmo de ventana deslizante para esto.\n",
    "\n",
    "La función `get_candidate_concepts` analiza los tokens \"clean\" de cada archivo original, detectando y contando todas las secuencias de estos de longitud entre 1 y el tamaño de la ventana. A estas secuencias se les llama \"conceptos candidatos\".\n",
    "\n",
    "Por otro lado, `make_vocabs` toma los conceptos candidatos y, por cada longitud de estos (del 1 al tamaño de la ventana), calcula la frecuencia promedio. Esta frecuencia promedio permite filtrar los conceptos candidatos, tomando como \"conceptos definitivos\" aquellos cuya frecuencia se encuentre entre ciertos valores, obtenidos al multiplicar por la frecuencia promedio.\n",
    "\n",
    "Los conceptos definitivos pasan a formar el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fMtkg6yC_5o4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_candidate_concepts(tokens_clean_path, window_size):\n",
    "\n",
    "    tokens_clean_files = get_filenames(tokens_clean_path)\n",
    "\n",
    "    candidates = {}\n",
    "\n",
    "    banned_tokens = []\n",
    "\n",
    "    with open(\"./data/banned.txt\", \"rb\") as bf:\n",
    "        banned_tokens = bf.read().decode(\"utf-8\").split(\"\\r\\n\")\n",
    "\n",
    "    for f in tokens_clean_files:\n",
    "        print(\"\\033[94mDetectando conceptos en archivo: \" + f + \"\\033[0m\")\n",
    "\n",
    "        tokens = []\n",
    "\n",
    "        with open(tokens_clean_path + \"/\" + f + \".txt\", \"rb\") as tf:\n",
    "            tokens = tf.read().decode(\"utf-8\").split(\"\\r\\n\")\n",
    "\n",
    "        for i in range(len(tokens) - window_size):\n",
    "            window = tokens[i:i+window_size]\n",
    "            \n",
    "            arrays = [window[0:r] for r in range(1,window_size+1)]\n",
    "\n",
    "            for arr in arrays:\n",
    "                arr = [s.lower() for s in arr]\n",
    "\n",
    "                if arr[-1] in banned_tokens:\n",
    "                    break\n",
    "\n",
    "                if any(re.search(r'(^[0-9\\.\\,]+$)|(-$)|(^.\\.$)|[0-9]{1,2}:[0-9]{1,2}|[0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4}', s) for s in arr):\n",
    "                    continue\n",
    "\n",
    "                if len(arr) != len(set(arr)):\n",
    "                    continue\n",
    "                \n",
    "                t = tuple(arr)\n",
    "                if (not t in candidates):\n",
    "                    candidates[t] = 0\n",
    "                candidates[t] += 1\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def make_vocabs(vocabularies_path, candidates, window_size, freq_ranges):\n",
    "    \n",
    "    pathlib.Path(vocabularies_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    base = len(get_filenames(vocabularies_path))\n",
    "\n",
    "    candidates_count = [0 for ws in range(window_size)]\n",
    "    candidates_freqs_acum = [0 for ws in range(window_size)]\n",
    "    for tokens, freq in candidates.items():\n",
    "        candidates_count[len(tokens)-1] += 1\n",
    "        candidates_freqs_acum[len(tokens)-1] += freq\n",
    "\n",
    "    avg_candidates_freq = [candidates_freqs_acum[i] / candidates_count[i] for i in range(len(candidates_count))]\n",
    "\n",
    "\n",
    "    for i, freq_range in enumerate(freq_ranges):\n",
    "        print(\"\\033[94mGenerando vocabulario: \" + str(base + i) + \" (\" + str(freq_range) + \")\\033[0m\")\n",
    "        vocabulary = []\n",
    "\n",
    "        for tokens, freq in candidates.items():\n",
    "            c = len(tokens)\n",
    "            use_nth = c <= len(freq_range)\n",
    "            ix = c-1 if use_nth else -1\n",
    "\n",
    "            min_freq = freq_range[ix][0]\n",
    "            max_freq = freq_range[ix][1]\n",
    "\n",
    "            freq_rel_avg = freq / avg_candidates_freq[c-1]\n",
    "\n",
    "            if (freq_rel_avg >= min_freq and freq_rel_avg <= max_freq):\n",
    "                vocabulary.append(tokens)\n",
    "\n",
    "        with open(vocabularies_path + \"/vocab_\" + str(base + i) + \".txt\", \"wb\") as cf:\n",
    "            for concept in vocabulary:\n",
    "                cf.write((str(concept) + \"\\n\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mDetectando conceptos en archivo: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 2 - SO - Tanenbaum\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Ergonomia - 4°9°\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[94mDetectando conceptos en archivo: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "window_size = 4\n",
    "\n",
    "candidates = get_candidate_concepts(tokens_clean_path, window_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para armar el vocabulario, se deben armar los rangos de frecuencias.\n",
    "\n",
    "Cada rango de frecuencias se convertirá en un vocabulario.\n",
    "\n",
    "Un rango de frecuencias es una secuencia de duplas, donde la enésima dupla contiene los factores para las frecuencias mínimas y máximas para los conceptos de longitud n.\n",
    "\n",
    "Los factores para las frecuencias mínimas y máximas son multiplicados por la frecuencia promedio de los conceptos de la longitud correspondiente para determinar el mínimo y máximo.\n",
    "\n",
    "A modo de ejemplo:\n",
    "* Si se tiene un concepto candidato de dos tokens (longitud 2)\n",
    "* La segunda dupla en un rango de frecuencias es (50, 100)\n",
    "* En promedio, los conceptos de longitud 2 tienen frecuencia (absoluta) de 3 (es decir, en promedio se encontró tres veces a los conceptos de dos tokens)\n",
    "\n",
    "Entonces el concepto candidato se volverá definitivo si y solo sí su frecuencia se encuentra entre 3 * 50 = 150 y 3 * 100 = 300.\n",
    "\n",
    "Si bien los rangos de frecuencias pueden ser armados por prueba y error, al analizar un poco más a fondo se puede llegar a la conclusión de que los conceptos candidatos de n+1 tokens contienen 2 tokens de longitud n.\n",
    "\n",
    "Como ejemplo, el concepto candidato (protocolo, tcp, ip) (de longitud 3) contiene a estos dos conceptos candidatos de longitud 2: (protocolo, tcp) y (tcp, ip).\n",
    "\n",
    "En base a esta observación, se concluye que es conveniente, para facilitar el proceso de descubrimiento empírico del rango de frecuencias, dejar fija la relación entre las duplas correspondientes a conceptos candidatos de diferentes longitudes, dividiendo siempre el rango a la mitad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mGenerando vocabulario: 8 ([(64, 160), (48, 120), (32, 80)])\u001b[0m\n",
      "\u001b[94mGenerando vocabulario: 9 ([(64, 200), (48, 150), (32, 100)])\u001b[0m\n",
      "\u001b[94mGenerando vocabulario: 10 ([(64, 240), (48, 180), (32, 120)])\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "div_by_2 = lambda start_min, start_max: [(start_min*(4-i), start_max*(4-i)) for i in range(0,3)]\n",
    "\n",
    "freq_ranges = [\n",
    "    #[(67, 134),(63, 318), (44, 268), (1.5, 1.8)]\n",
    "    #[(126, 636),(64, 318), (32, 159), (16, 80)]\n",
    "    div_by_2(16,60)\n",
    "]\n",
    "\n",
    "make_vocabs(vocabularies_path, candidates, window_size, freq_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waCeLFNPFJbO"
   },
   "source": [
    "En este punto, ya hemos seleccionado conjuntos de tokens que suelen aparecer cerca.\n",
    "\n",
    "Estos conjuntos serán los conceptos, y pasarán a formar nuestro vocabulario.\n",
    "\n",
    "Ahora, se debe tokenizar nuevamente los textos planos, utilizando los conceptos.\n",
    "\n",
    "Para esto, se recorrerá cada archivo con los tokens completos (limpios o no) mediante una ventana deslizante de un tamaño proporcional al utilizado para detectar conceptos (no igual, dado que en la detección de conceptos solo había tokens limpios).\n",
    "\n",
    "Los resultados de este proceso son escritos a archivos dentro de /data/tokens_concepts. Estos archivos consisten en secuencias de números enteros. Los positivos (o 0) corresponden al índice de un concepto en el vocabulario, mientras que los negativos indican la cantidad de tokens no reconocidos (\\<unk\\>) entre medio. Esto se permite ahorrar mucho espacio y tiempo de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_vocabulary(vocabularies_path, vocabulary):\n",
    "    with open(vocabularies_path + \"/\" + vocabulary + \".txt\", \"rb\") as cf:\n",
    "        lines = cf.read().decode(\"utf-8\").split(\"\\n\")[:-1]\n",
    "        return [ast.literal_eval(l) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTamaño del vocabulario:441\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "vocabularies_path = \"./data/vocabularies\"\n",
    "selected_vocabulary = \"vocab_1\"\n",
    "\n",
    "vocabulary = get_vocabulary(vocabularies_path, selected_vocabulary)\n",
    "\n",
    "print(\"\\033[92mTamaño del vocabulario:\" + str(len(vocabulary)) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "RQzvFvNPHcfL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tokenize_by_concepts(tokens_full_path, tokens_concepts_path, vocabulary, window_size, window_size_extension_factor = 3):\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    window_size_large = math.floor(window_size * window_size_extension_factor)\n",
    "\n",
    "    pathlib.Path(tokens_concepts_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    tokens_full_files = get_filenames(tokens_full_path)\n",
    "\n",
    "    for f in tokens_full_files:\n",
    "        print(\"\\033[94mTokenizando por conceptos: \" + f + \"\\033[0m\")\n",
    "        metrics[f] = {}\n",
    "        found_concepts = 0\n",
    "\n",
    "        recent_concepts = {}\n",
    "\n",
    "        with open(tokens_full_path + \"/\" + f + \".txt\", \"rb\") as pf:\n",
    "            tokens = pf.read().decode(\"utf-8\").split(\"\\n\")\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "\n",
    "            metrics[f][\"tokens\"] = len(tokens)\n",
    "\n",
    "            with open(tokens_concepts_path + \"/\" + f + \".txt\", \"wb\") as tnf:\n",
    "\n",
    "                unks = 0\n",
    "\n",
    "                for i in range(len(tokens) - window_size_large):\n",
    "                    window = tokens[i:i+window_size_large]\n",
    "                    for k, v in recent_concepts.items():\n",
    "                        if v > 0:\n",
    "                            recent_concepts[k] -= 1\n",
    "\n",
    "                    unks += 1\n",
    "                \n",
    "                    for ix, concept in enumerate(vocabulary):\n",
    "                        if (ix in recent_concepts and recent_concepts[ix] > 0):\n",
    "                            continue\n",
    "\n",
    "                        curr_word_ix = 0\n",
    "                        curr_word = concept[curr_word_ix]\n",
    "\n",
    "                        found = False\n",
    "\n",
    "                        for token in window:\n",
    "                            if(token == curr_word):\n",
    "                                curr_word_ix += 1\n",
    "                                if (len(concept) <= curr_word_ix):\n",
    "                                    found = True\n",
    "                                    break\n",
    "                                curr_word = concept[curr_word_ix]\n",
    "\n",
    "                        if found:\n",
    "                            if unks > 0:\n",
    "                                tnf.write((\"-\" + str(unks) + \" \").encode(\"utf-8\"))\n",
    "                                unks = 0\n",
    "                            tnf.write((str(ix) + \" \").encode(\"utf-8\"))\n",
    "                            recent_concepts[ix] = window_size_large\n",
    "                            found_concepts += 1\n",
    "\n",
    "                        \n",
    "        metrics[f][\"concepts\"] = found_concepts\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mTokenizando por conceptos: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 2 - SO - Tanenbaum\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Ergonomia - 4°9°\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[94mTokenizando por conceptos: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "window_size_extension_factor = 3\n",
    "\n",
    "tokenize_by_concepts_metrics = tokenize_by_concepts(tokens_full_path, tokens_concepts_path, vocabulary, window_size, window_size_extension_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "Archivo 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321:\n",
      "\ttokens: 94709\n",
      "\tconcepts: 3867\n",
      "Archivo 1 - AyED - cpp según yo pero en pedo:\n",
      "\ttokens: 2658\n",
      "\tconcepts: 68\n",
      "Archivo 1 - AyED - cpp según yo:\n",
      "\ttokens: 2554\n",
      "\tconcepts: 75\n",
      "Archivo 1 - AyED - Unidad3 (7929):\n",
      "\ttokens: 5728\n",
      "\tconcepts: 175\n",
      "Archivo 1 - AyED - Unidad4 (7930):\n",
      "\ttokens: 2146\n",
      "\tconcepts: 116\n",
      "Archivo 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928):\n",
      "\ttokens: 3520\n",
      "\tconcepts: 128\n",
      "Archivo 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org):\n",
      "\ttokens: 198363\n",
      "\tconcepts: 4064\n",
      "Archivo 1 - SyO - 01 Evolucion de las estructuras:\n",
      "\ttokens: 4413\n",
      "\tconcepts: 177\n",
      "Archivo 1 - SyO - 02 Gestion por procesos UNCuyo:\n",
      "\ttokens: 9477\n",
      "\tconcepts: 306\n",
      "Archivo 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2:\n",
      "\ttokens: 12599\n",
      "\tconcepts: 380\n",
      "Archivo 1 - SyO - 04 gestion-por-procesos:\n",
      "\ttokens: 4530\n",
      "\tconcepts: 175\n",
      "Archivo 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q:\n",
      "\ttokens: 750\n",
      "\tconcepts: 46\n",
      "Archivo 1 - SyO - 1)La Información en la Empresa:\n",
      "\ttokens: 4308\n",
      "\tconcepts: 135\n",
      "Archivo 1 - SyO - 2)Recopilacion de la informacion:\n",
      "\ttokens: 15749\n",
      "\tconcepts: 446\n",
      "Archivo 1 - SyO - Analisis FODA:\n",
      "\ttokens: 2874\n",
      "\tconcepts: 101\n",
      "Archivo 1 - SyO - Bertoglio - Teoria de sistemas:\n",
      "\ttokens: 8938\n",
      "\tconcepts: 177\n",
      "Archivo 1 - SyO - BPMN 2.0 Manual de referencia:\n",
      "\ttokens: 100986\n",
      "\tconcepts: 2844\n",
      "Archivo 1 - SyO - Gutierrez Gomez - Teoría general de sistemas:\n",
      "\ttokens: 53785\n",
      "\tconcepts: 2067\n",
      "Archivo 1 - SyO - Kendall y Kendall pag. 1 - 13:\n",
      "\ttokens: 11976\n",
      "\tconcepts: 543\n",
      "Archivo 1 - SyO - La organizacion como sistema abierto- Cap 5:\n",
      "\ttokens: 3681\n",
      "\tconcepts: 172\n",
      "Archivo 1 - SyO - Ponce - FODA:\n",
      "\ttokens: 9758\n",
      "\tconcepts: 250\n",
      "Archivo 1 - SyO - Resumen SyO U1:\n",
      "\ttokens: 1398\n",
      "\tconcepts: 68\n",
      "Archivo 1 - SyO - Resumen SyO U2:\n",
      "\ttokens: 3615\n",
      "\tconcepts: 175\n",
      "Archivo 1 - SyO - Resumen SyO U3:\n",
      "\ttokens: 8088\n",
      "\tconcepts: 348\n",
      "Archivo 1 - SyO - Resumen SyO U4:\n",
      "\ttokens: 1232\n",
      "\tconcepts: 81\n",
      "Archivo 1 - SyO - Resumen SyO U5:\n",
      "\ttokens: 2886\n",
      "\tconcepts: 153\n",
      "Archivo 1 - SyO - Teoria de sistemas:\n",
      "\ttokens: 3781\n",
      "\tconcepts: 124\n",
      "Archivo 2 - AS - AS Presentación de UML (a):\n",
      "\ttokens: 10654\n",
      "\tconcepts: 538\n",
      "Archivo 2 - AS - Scrum Manager - Historias de Usuario:\n",
      "\ttokens: 13336\n",
      "\tconcepts: 592\n",
      "Archivo 2 - AS - scrum_manager:\n",
      "\ttokens: 37170\n",
      "\tconcepts: 1468\n",
      "Archivo 2 - AS - U8 Metodologías Agiles parte a :\n",
      "\ttokens: 5722\n",
      "\tconcepts: 2\n",
      "Archivo 2 - SO - ResumenSO:\n",
      "\ttokens: 42427\n",
      "\tconcepts: 1520\n",
      "Archivo 2 - SO - Stallings:\n",
      "\ttokens: 499260\n",
      "\tconcepts: 28486\n",
      "Archivo 2 - SO - Tanenbaum:\n",
      "\ttokens: 726029\n",
      "\tconcepts: 35765\n",
      "Archivo 3 - AP - AdProy_2_Trabajo en Equipo_2022:\n",
      "\ttokens: 30693\n",
      "\tconcepts: 1141\n",
      "Archivo 3 - AP - respuestas:\n",
      "\ttokens: 1\n",
      "\tconcepts: 0\n",
      "Archivo 3 - BD - caselli_manual-de-base-de-datos-2019:\n",
      "\ttokens: 33382\n",
      "\tconcepts: 1530\n",
      "Archivo 3 - BD - Guía 1:\n",
      "\ttokens: 12192\n",
      "\tconcepts: 557\n",
      "Archivo 3 - BD - Guía 2:\n",
      "\ttokens: 10374\n",
      "\tconcepts: 350\n",
      "Archivo 3 - CD - capitulo2:\n",
      "\ttokens: 11236\n",
      "\tconcepts: 451\n",
      "Archivo 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings:\n",
      "\ttokens: 487752\n",
      "\tconcepts: 31057\n",
      "Archivo 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion:\n",
      "\ttokens: 579531\n",
      "\tconcepts: 33515\n",
      "Archivo 3 - DS - Actor. Definicion. Clasificacion (1):\n",
      "\ttokens: 1665\n",
      "\tconcepts: 34\n",
      "Archivo 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados:\n",
      "\ttokens: 4804\n",
      "\tconcepts: 124\n",
      "Archivo 3 - DS - Libro UML y Patrones - Larman:\n",
      "\ttokens: 276607\n",
      "\tconcepts: 10254\n",
      "Archivo 3 - DS - MerFNConceptos:\n",
      "\ttokens: 2663\n",
      "\tconcepts: 88\n",
      "Archivo 4 - AS - Analisis PEST:\n",
      "\ttokens: 1129\n",
      "\tconcepts: 49\n",
      "Archivo 4 - AS - Backups_raids:\n",
      "\ttokens: 1688\n",
      "\tconcepts: 40\n",
      "Archivo 4 - AS - Benchmark en Tecnología:\n",
      "\ttokens: 306\n",
      "\tconcepts: 2\n",
      "Archivo 4 - AS - Benchmark:\n",
      "\ttokens: 1727\n",
      "\tconcepts: 71\n",
      "Archivo 4 - AS - Chiavenato-cap8:\n",
      "\ttokens: 12679\n",
      "\tconcepts: 376\n",
      "Archivo 4 - AS - chiavenato-cap9:\n",
      "\ttokens: 29634\n",
      "\tconcepts: 1072\n",
      "Archivo 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.:\n",
      "\ttokens: 396998\n",
      "\tconcepts: 10880\n",
      "Archivo 4 - AS - EL PROCESO ADMINISTRATIVO:\n",
      "\ttokens: 1870\n",
      "\tconcepts: 57\n",
      "Archivo 4 - AS - Elaboracion_de_programas_de_capacitacion:\n",
      "\ttokens: 13013\n",
      "\tconcepts: 506\n",
      "Archivo 4 - AS - Ergonomia - 4°9°:\n",
      "\ttokens: 850\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Ergonomía 4k10:\n",
      "\ttokens: 813\n",
      "\tconcepts: 5\n",
      "Archivo 4 - AS - Gestión CPD:\n",
      "\ttokens: 878\n",
      "\tconcepts: 13\n",
      "Archivo 4 - AS - Habilidades Blandas 4k9:\n",
      "\ttokens: 1387\n",
      "\tconcepts: 29\n",
      "Archivo 4 - AS - Implementación de un Data Center:\n",
      "\ttokens: 1820\n",
      "\tconcepts: 18\n",
      "Archivo 4 - AS - Ingeniería Social 4k9:\n",
      "\ttokens: 1017\n",
      "\tconcepts: 13\n",
      "Archivo 4 - AS - Ingeniería Social:\n",
      "\ttokens: 1099\n",
      "\tconcepts: 11\n",
      "Archivo 4 - AS - Inteligencia Emocional - 4°10°:\n",
      "\ttokens: 3215\n",
      "\tconcepts: 82\n",
      "Archivo 4 - AS - Inteligencia Emocional - 4°9°:\n",
      "\ttokens: 2053\n",
      "\tconcepts: 43\n",
      "Archivo 4 - AS - Planes-de-Contingencia:\n",
      "\ttokens: 1044\n",
      "\tconcepts: 2\n",
      "Archivo 4 - AS - Resumen Global Administración de Sistemas de Información:\n",
      "\ttokens: 3345\n",
      "\tconcepts: 119\n",
      "Archivo 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS:\n",
      "\ttokens: 1060\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Sniffers_y_escaneo_de_puertos:\n",
      "\ttokens: 1049\n",
      "\tconcepts: 4\n",
      "Archivo 4 - AS - Teletrabajo-4k10:\n",
      "\ttokens: 826\n",
      "\tconcepts: 0\n",
      "Archivo 4 - AS - Teletrabajo-4k9:\n",
      "\ttokens: 821\n",
      "\tconcepts: 18\n",
      "Archivo 4 - AS - Trello-4°9°:\n",
      "\ttokens: 967\n",
      "\tconcepts: 7\n",
      "Archivo 4 - AS - WLAN TT:\n",
      "\ttokens: 740\n",
      "\tconcepts: 1\n",
      "Archivo 4 - AS - WLAN:\n",
      "\ttokens: 1463\n",
      "\tconcepts: 15\n",
      "Archivo 4 - ICS - 2.-principiosingenieriasoftware:\n",
      "\ttokens: 4669\n",
      "\tconcepts: 134\n",
      "Archivo 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American:\n",
      "\ttokens: 11027\n",
      "\tconcepts: 207\n",
      "Archivo 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx:\n",
      "\ttokens: 2049\n",
      "\tconcepts: 10\n",
      "Archivo 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software:\n",
      "\ttokens: 11292\n",
      "\tconcepts: 497\n",
      "Archivo 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software:\n",
      "\ttokens: 1401\n",
      "\tconcepts: 60\n",
      "Archivo 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia:\n",
      "\ttokens: 12858\n",
      "\tconcepts: 808\n",
      "Archivo 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD:\n",
      "\ttokens: 714\n",
      "\tconcepts: 35\n",
      "Archivo 4 - IO - U5 - ADMINISTRACION DE PROYECTOS:\n",
      "\ttokens: 3592\n",
      "\tconcepts: 74\n",
      "Archivo 4 - IO - U6_1-GESTION DE INVENTARIOS:\n",
      "\ttokens: 2187\n",
      "\tconcepts: 51\n",
      "Archivo 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC:\n",
      "\ttokens: 785\n",
      "\tconcepts: 35\n",
      "Archivo 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC:\n",
      "\ttokens: 1\n",
      "\tconcepts: 0\n",
      "Archivo 4 - RD - Kurose-Ross:\n",
      "\ttokens: 551157\n",
      "\tconcepts: 30323\n",
      "Archivo 4 - RD - Resumen Redes de Datos:\n",
      "\ttokens: 23109\n",
      "\tconcepts: 1542\n",
      "Archivo 4 - RD - Tanenbaum:\n",
      "\ttokens: 562810\n",
      "\tconcepts: 27680\n",
      "Archivo 4 - S - Resumen 2do Parcial:\n",
      "\ttokens: 3315\n",
      "\tconcepts: 100\n",
      "Archivo 4 - TA - Guía 1. Sistemas de control automático:\n",
      "\ttokens: 12194\n",
      "\tconcepts: 390\n",
      "Archivo 4 - UXUI - Diseño UX UNIDAD 1:\n",
      "\ttokens: 25410\n",
      "\tconcepts: 814\n",
      "Archivo 5 - GG - Resumen Cambio Organizacional:\n",
      "\ttokens: 454\n",
      "\tconcepts: 28\n",
      "Archivo 5 - GG - Resumen Parcial 26-5:\n",
      "\ttokens: 1754\n",
      "\tconcepts: 46\n",
      "Archivo 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8:\n",
      "\ttokens: 28138\n",
      "\tconcepts: 1180\n",
      "Archivo 5 - SSI - Clase U2-3 Disposicion ONTI 2015:\n",
      "\ttokens: 6834\n",
      "\tconcepts: 304\n",
      "Archivo 5 - SSI - DE641-2021_Anexo:\n",
      "\ttokens: 5277\n",
      "\tconcepts: 152\n",
      "Archivo 5 - SSI - Disposicion 1-2015_PSI:\n",
      "\ttokens: 57193\n",
      "\tconcepts: 2186\n",
      "Archivo 5 - SSI - Guia_apoyo_SGSI:\n",
      "\ttokens: 14303\n",
      "\tconcepts: 578\n",
      "Archivo 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad:\n",
      "\ttokens: 9045\n",
      "\tconcepts: 432\n",
      "Archivo 5 - SSI - M5-Privacidad:\n",
      "\ttokens: 25474\n",
      "\tconcepts: 847\n",
      "Archivo 5 - SSI - Magerit_v3_libro1_metodo:\n",
      "\ttokens: 66988\n",
      "\tconcepts: 3411\n",
      "Archivo 5 - SSI - Magerit_v3_libro2_catálogo de elementos:\n",
      "\ttokens: 28138\n",
      "\tconcepts: 1180\n",
      "Archivo 5 - SSI - Magerit_v3_libro3_guía de técnicas:\n",
      "\ttokens: 19468\n",
      "\tconcepts: 733\n",
      "Archivo 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI:\n",
      "\ttokens: 6694\n",
      "\tconcepts: 248\n",
      "Archivo 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información:\n",
      "\ttokens: 11618\n",
      "\tconcepts: 324\n",
      "Archivo 5 - SSI - Resumen U2 - Disposición ONTI 1 2015:\n",
      "\ttokens: 4546\n",
      "\tconcepts: 201\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[92m\")\n",
    "for archivo, item in tokenize_by_concepts_metrics.items():\n",
    "    print(\"Archivo \" + archivo + \":\")\n",
    "    for nombre, valor in item.items():\n",
    "        print(\"\\t\" + nombre + \": \" + str(valor))\n",
    "print(\"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyqXH3y2IKST"
   },
   "source": [
    "Teniendo ya las secuencias de números, se busca detectar qué conceptos se encuentran relacionados entre sí; esto es, se encuentran cerca en el texto.\n",
    "\n",
    "Para poder armar el dataset mediante Skip-Gram, se requieren tres elementos:\n",
    "* Concepto central\n",
    "* Conceptos en el contexto\n",
    "* Ejemplos negativos de conceptos (que no se encuentran en el centro).\n",
    "\n",
    "El objetivo de los ejemplos negativos es evitar que la red neuronal que se entrenará piense que siempre todos los conceptos están en el contexto de otros.\n",
    "\n",
    "La generación de los ejemplos negativos será realizado mediante un muestreo, para lo cual primero se debe calcular la frecuencia relativa de cada token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_concepts_freqs(tokens_concepts_path):\n",
    "    freq_abs = {}\n",
    "\n",
    "    tokens_concepts_files = get_filenames(tokens_concepts_path)\n",
    "\n",
    "    for file in tokens_concepts_files:\n",
    "        with open(f\"{tokens_concepts_path}/{file}.txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "\n",
    "            nums = txt.split(\" \") # Lista con cada número en el archivo\n",
    "            for num in nums:\n",
    "                if (num == \"\" or num[0] == \"-\"):\n",
    "                    continue\n",
    "\n",
    "                token_ix = int(num)\n",
    "                if not token_ix in freq_abs:\n",
    "                    freq_abs[token_ix] = 0\n",
    "\n",
    "                freq_abs[token_ix] += 1\n",
    "\n",
    "    total_tokens = 0\n",
    "    for token, freq in freq_abs.items():\n",
    "        total_tokens += freq\n",
    "\n",
    "    freq_rel = {}\n",
    "    for token, freq in freq_abs.items():\n",
    "        freq_rel[token] = freq/total_tokens\n",
    "\n",
    "    return freq_abs, freq_rel\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A su vez, se definirá una clase auxiliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomGenerator:\n",
    "  \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
    "  def __init__(self, sampling_weights):\n",
    "    # Exclude\n",
    "    self.population = list(range(1, len(sampling_weights) + 1))\n",
    "    self.sampling_weights = sampling_weights\n",
    "    self.candidates = []\n",
    "    self.i = 0\n",
    "\n",
    "  def draw(self):\n",
    "    if self.i == len(self.candidates):\n",
    "      # Cache `k` random sampling results\n",
    "      self.candidates = random.choices(\n",
    "          self.population, self.sampling_weights, k=10000)\n",
    "      self.i = 0\n",
    "    self.i += 1\n",
    "    return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hecho esto, podemos finalmente armar nuestro dataset. El mismo retornará (mediante get_item()) un centro, su contexto y sus ejemplos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "class ISIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, load_file):\n",
    "        with open(load_file, \"rb\") as lf:\n",
    "            self.data = pickle.load(lf)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase del dataset definida, sin embargo, es solo un wrapper que carga el contenido de un archivo. Estos archivos son generados mediante el algoritmo `make_datasets`. Esta función tiene la capacidad de generar velozmente múltiples datasets variando el tamaño de la ventana.\n",
    "\n",
    "Para un único tamaño de ventana, el algoritmo es el siguiente:\n",
    "1. Por cada archivo, se itera para centrarse en cada concepto.\n",
    "2. Se analiza hacia el lado izquierdo del concepto para detectar conceptos en el contexto, hasta agotar la mitad del tamaño de la ventana.\n",
    "3. Se analiza hacia el lado derecho del concepto, de forma análoga al izquierdo.\n",
    "4. Se generan ejemplos negativos (garantizado que sean distintos a los conceptos del contexto). La cantidad de ejemplos negativos es K veces la de conceptos en el contexto.\n",
    "5. Se almacenan los centros, sus contextos y ejemplos negativos en un diccionario, que a su vez es guardado en un archivo.\n",
    "\n",
    "Si se genera más de un dataset con tamaños de ventana distintos, en primer lugar se ordenan estos tamaños de menor a mayor, y luego cambian los pasos 2. y 3.:\n",
    "1. Se analiza hacia el lado izquierdo del concepto, por el primer tamaño de ventana (el más pequeño)\n",
    "2. Se analiza hacia el lado izquierdo, desde el primer tamaño de ventana hasta el segundo. El proceso se repite hasta haber analizado todos los tamaños.\n",
    "3. Se analiza hacia el lado derecho, de manera análoga.\n",
    "4. Dado que el contexto de la ventana n siempre contiene al de la ventana n-1, se los une recursivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "1JampbY-K6fZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_datasets(tokens_concepts_path, datasets_path, vocabulary, window_sizes, K):\n",
    "\n",
    "    data = [[] for i in range(0, len(window_sizes))]\n",
    "\n",
    "    _, freq_rel = get_tokens_concepts_freqs(tokens_concepts_path)\n",
    "\n",
    "    sampling_weights = [freq_rel[concept]**0.75 if concept in freq_rel else 0 for concept in range(0, len(vocabulary))]\n",
    "    generator = RandomGenerator(sampling_weights)\n",
    "\n",
    "    pathlib.Path(datasets_path).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    tokens_concepts_files = get_filenames(tokens_concepts_path)\n",
    "    window_sizes.sort()\n",
    "\n",
    "    for file in tokens_concepts_files:\n",
    "        print(\"\\033[94mArmando dataset con: \" + file + \"\\033[0m\")\n",
    "\n",
    "        with open(f\"{tokens_concepts_path}/{file}.txt\", \"rb\") as pf:\n",
    "            txt = pf.read().decode(\"utf-8\")\n",
    "\n",
    "            nums = txt.split(\" \") # Lista con cada número en el archivo\n",
    "\n",
    "            for ix, num in enumerate(nums):\n",
    "                if (num == \"\" or num[0] == \"-\"):\n",
    "                    continue\n",
    "\n",
    "                token_ix = int(num)\n",
    "\n",
    "                context = []\n",
    "                \n",
    "                curr_ws_ix = 0\n",
    "\n",
    "                c = 0\n",
    "                i = 1\n",
    "                \n",
    "                while curr_ws_ix < len(window_sizes): # Buscar conceptos hacia atrás\n",
    "                    curr_ws = window_sizes[curr_ws_ix]\n",
    "                    curr_wr = curr_ws // 2\n",
    "                    c += curr_wr\n",
    "\n",
    "                    context.append([])\n",
    "\n",
    "                    while c >= 0: \n",
    "                        if (ix - i < 0): # Si se acabó el archivo, dejar de buscar\n",
    "                            break\n",
    "                        \n",
    "                        val = nums[ix - i]\n",
    "                        if (val == \"\"): \n",
    "                            val = \"-0\"\n",
    "                        if (val[0] == \"-\"):\n",
    "                            val = val.lstrip(\"-\")\n",
    "                            unks = int(val)\n",
    "                            c = c - unks\n",
    "                        else:\n",
    "                            concept = int(val)\n",
    "                            context[curr_ws_ix].append(concept)\n",
    "                            c = c - 1\n",
    "                        i = i + 1\n",
    "\n",
    "                    curr_ws_ix += 1\n",
    "\n",
    "                curr_ws_ix = 0\n",
    "\n",
    "                c = 0\n",
    "                i = 1\n",
    "                l = len(nums)\n",
    "\n",
    "                while curr_ws_ix < len(window_sizes): # Buscar conceptos hacia adelante\n",
    "                    curr_ws = window_sizes[curr_ws_ix]\n",
    "                    curr_wr = curr_ws // 2\n",
    "                    c += curr_wr\n",
    "\n",
    "                    while c >= 0: \n",
    "                        if (ix + i < l): # Si se acabó el archivo, dejar de buscar\n",
    "                            break\n",
    "                        \n",
    "                        val = nums[ix + i]\n",
    "                        if (val == \"\"): \n",
    "                            val = \"-0\"\n",
    "                        if (val[0] == \"-\"):\n",
    "                            val = val.lstrip(\"-\")\n",
    "                            unks = int(val)\n",
    "                            c = c - unks\n",
    "                        else:\n",
    "                            concept = int(val)\n",
    "                            context[curr_ws_ix].append(concept)\n",
    "                            c = c - 1\n",
    "                        i = i + 1\n",
    "\n",
    "                    curr_ws_ix += 1\n",
    "                \n",
    "                # Juntar contextos de tamaños de ventana más grandes con otros más pequeños\n",
    "                context_aux = []\n",
    "                context_acc = []\n",
    "\n",
    "                for sub_ctx in context:\n",
    "                    context_acc += sub_ctx\n",
    "                    context_aux.append(context_acc[:])\n",
    "                \n",
    "                context = context_aux\n",
    "\n",
    "                for ctx_ix, ctx in enumerate(context):\n",
    "                    negatives = []\n",
    "                    \n",
    "                    while len(negatives) < len(ctx) * K:\n",
    "                        neg = generator.draw()\n",
    "                        if neg not in context:\n",
    "                            negatives.append(neg)\n",
    "\n",
    "                    if (len(ctx) > 0):\n",
    "                        data[ctx_ix].append({\n",
    "                            \"center\": token_ix,\n",
    "                            \"context\": ctx,\n",
    "                            \"negatives\": negatives\n",
    "                        })\n",
    "    \n",
    "    for ws_ix, ws in enumerate(window_sizes):\n",
    "        with open(datasets_path + \"/dataset-\" + str(ws) + \".pkl\", \"wb\") as lf:\n",
    "            pickle.dump(data[ws_ix], lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder probar distintas alternativas, se generó distintos datasets (almacenados en /data/datasets), con distintos tamaños de ventana y cantidad de ejemplos negativos (K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mArmando dataset con: 1 - AC - LibroArquitecturadeComputadorasSantiagoPerez090321\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - AyED - cpp según yo pero en pedo\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - AyED - cpp según yo\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - AyED - Unidad3 (7929)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - AyED - Unidad4 (7930)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - AyED - Unidades 1 y 2 (cód. fotoc. 7928)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - MD - Matemáticas discretas by Ramóne Espinosa Armenta (z-lib.org)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 01 Evolucion de las estructuras\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 02 Gestion por procesos UNCuyo\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 03 Gestion por procesos, indicaroes y estandares para unidades de informacion - Cap 1 y 2\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 04 gestion-por-procesos\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 05 Arquitectura_empresarial_que_es_y_para_q\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 1)La Información en la Empresa\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - 2)Recopilacion de la informacion\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Analisis FODA\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Bertoglio - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - BPMN 2.0 Manual de referencia\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Gutierrez Gomez - Teoría general de sistemas\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Kendall y Kendall pag. 1 - 13\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - La organizacion como sistema abierto- Cap 5\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Ponce - FODA\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Resumen SyO U1\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Resumen SyO U2\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Resumen SyO U3\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Resumen SyO U4\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Resumen SyO U5\u001b[0m\n",
      "\u001b[94mArmando dataset con: 1 - SyO - Teoria de sistemas\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - AS - AS Presentación de UML (a)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - AS - Scrum Manager - Historias de Usuario\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - AS - scrum_manager\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - AS - U8 Metodologías Agiles parte a \u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - SO - ResumenSO\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - SO - Stallings\u001b[0m\n",
      "\u001b[94mArmando dataset con: 2 - SO - Tanenbaum\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - AP - AdProy_2_Trabajo en Equipo_2022\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - AP - respuestas\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - BD - caselli_manual-de-base-de-datos-2019\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - BD - Guía 1\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - BD - Guía 2\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - CD - capitulo2\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - CD - Comunicaciones y Redes de Computadores,7ma Edición - William Stallings\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - CD - sistemas-de-comunicaciones-electronicas-tomasi-4ta-edicion\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - DS - Actor. Definicion. Clasificacion (1)\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - DS - Eje 1. Metodología y conceptos teóricos aplicados\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - DS - Libro UML y Patrones - Larman\u001b[0m\n",
      "\u001b[94mArmando dataset con: 3 - DS - MerFNConceptos\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Analisis PEST\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Backups_raids\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Benchmark en Tecnología\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Benchmark\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Chiavenato-cap8\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - chiavenato-cap9\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Comportamiento_organizacional._La_dina_mica_en_las_organizaciones.\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - EL PROCESO ADMINISTRATIVO\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Elaboracion_de_programas_de_capacitacion\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Ergonomia - 4°9°\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Ergonomía 4k10\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Gestión CPD\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Habilidades Blandas 4k9\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Implementación de un Data Center\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Ingeniería Social 4k9\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Ingeniería Social\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Inteligencia Emocional - 4°10°\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Inteligencia Emocional - 4°9°\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Planes-de-Contingencia\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Resumen Global Administración de Sistemas de Información\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - SNIFFERS Y ESCANEO DE PUERTOS\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Sniffers_y_escaneo_de_puertos\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Teletrabajo-4k10\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Teletrabajo-4k9\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - Trello-4°9°\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - WLAN TT\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - AS - WLAN\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - 2.-principiosingenieriasoftware\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - 2020-Scrum-Guide-Spanish-Latin-South-American\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - Facu_MiniParcial 2_U 4 y 5.1.docx\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - Preguntas 1er Parcial Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - Preguntas Unidad 3 Ingeniería y Calidad de Software\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - ICS - SoftwareDesign_PrincipiosyPatrones-Autentia\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - IO - Breve Resumen ANALISIS SENSIBILIDAD\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - IO - U5 - ADMINISTRACION DE PROYECTOS\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - IO - U6_1-GESTION DE INVENTARIOS\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - IO - U6_2_1-INVENTARIO - Introduccion ANALISIS ABC\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - IO - U6_2_2-INVENTARIO - Articulo ANALISIS ABC\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - RD - Kurose-Ross\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - RD - Resumen Redes de Datos\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - RD - Tanenbaum\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - S - Resumen 2do Parcial\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - TA - Guía 1. Sistemas de control automático\u001b[0m\n",
      "\u001b[94mArmando dataset con: 4 - UXUI - Diseño UX UNIDAD 1\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - GG - Resumen Cambio Organizacional\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - GG - Resumen Parcial 26-5\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - 2012_Magerit_v3_libro2_catalogo-de-elementos_es_NIPO_630-12-171-8\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Clase U2-3 Disposicion ONTI 2015\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - DE641-2021_Anexo\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Disposicion 1-2015_PSI\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Guia_apoyo_SGSI\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - guia_ciberseguridad_gestion_riesgos_metad\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - M5-Privacidad\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Magerit_v3_libro1_metodo\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Magerit_v3_libro2_catálogo de elementos\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Magerit_v3_libro3_guía de técnicas\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Metodologia GR compatible Disposición 1-2015 ONTI\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Resolución Nº RESOL-2022-87-APN-SIGEN - Normas de Control Interno para Tecnología de la Información\u001b[0m\n",
      "\u001b[94mArmando dataset con: 5 - SSI - Resumen U2 - Disposición ONTI 1 2015\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "full_window_sizes = [50, 100, 200, 300]\n",
    "K = 5\n",
    "\n",
    "make_datasets(tokens_concepts_path, datasets_path, vocabulary, full_window_sizes, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra un ejemplo de cómo cargar uno de los datasets generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241310\n"
     ]
    }
   ],
   "source": [
    "dataset = ISIDataset(datasets_path + \"/dataset-100.pkl\")\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-QSj_DQK63i"
   },
   "source": [
    "A partir de este Dataset se generará un DataLoader.\n",
    "\n",
    "Un requisito que debe cumplir es que se modifique los lotes para que estos tengan la misma longitud (cosa que no pasa debido a las diferentes cantidades de conceptos en el contexto de cada concepto central y, por ende de ejemplos negativos). Para solventar esto se utiliza la función `collate_batch`.\n",
    "\n",
    "Una alternativa a esto sería que el método get_item del dataset ya devolviera los datos en este formato. Por claridad, sin embargo, se optó por separar esto en la función indicada.\n",
    "\n",
    "Un aspecto importante es que en los vocabularios generados no se incluye un \"concepto\" de relleno, necesario para esta función. Es por esto que se optó por usar como índice para este \"pseudo-concepto\" el tamaño del vocabulario (es decir, el índice del último concepto más 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(data):\n",
    "  max_len = max(len(d[\"context\"]) + len(d[\"negatives\"]) for d in data)\n",
    "  centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "  for d in data:\n",
    "    center = d[\"center\"]\n",
    "    context = d[\"context\"]\n",
    "    negative = d[\"negatives\"]\n",
    "    centers += [center]\n",
    "    cur_len = len(context) + len(negative)\n",
    "    contexts_negatives += [context + negative + [len(vocabulary)] * (max_len - cur_len)]\n",
    "    masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "    labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "  return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
    "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "ec-elIZ3LhXY"
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u6Gf_hIMYnm"
   },
   "source": [
    "A continuación, se armará la estructura de la red neuronal mediante skipgram, utilizando capas Embedding de pytorch.\n",
    "\n",
    "Nótese una vez más la indicación de cuál concepto corresponde al relleno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "VjNlBeIxM1my"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocabulary, embed_size):\n",
    "        super().__init__()\n",
    "        self.central_embedding = nn.Embedding(num_embeddings=len(vocabulary)+1,\n",
    "                                embedding_dim=embed_size, padding_idx=len(vocabulary))\n",
    "        self.context_embedding = nn.Embedding(num_embeddings=len(vocabulary)+1,\n",
    "                                embedding_dim=embed_size, padding_idx=len(vocabulary))\n",
    "\n",
    "    def forward(self, center, contexts_and_negatives):\n",
    "        v = self.central_embedding(center)\n",
    "        u = self.context_embedding(contexts_and_negatives)\n",
    "        pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOVN22WTM1-j"
   },
   "source": [
    "Como función de pérdida, se utilizará entropía cruzada binaria (Sigmoidea). Esto es así pues requerimos clasificar dos conceptos según si están o no relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "Dcyjlt4YND8Q"
   },
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module):\n",
    "    # Binary cross-entropy loss with masking\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0f_w07NNEZ9"
   },
   "source": [
    "Se optó por abarcar todo el entrenamiento en una misma función. La misma incluye la inicialización de variables y el ciclo de entrenamiento en sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "E3vBgobNNbKZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(net, data_iter, lr, num_epochs, device):\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    L = 0\n",
    "    N = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start, num_batches = time.time(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch]\n",
    "\n",
    "            pred = net(center, context_negative)\n",
    "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
    "                     / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            L += l.sum()\n",
    "            N += l.numel()\n",
    "        print(f'loss {L / N:.3f}, '\n",
    "          f'{N / (time.time() - start):.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLUJawwxNbkC"
   },
   "source": [
    "Se generó una función auxiliar para el entrenamiento por medio de GPU, en caso de estar disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "S_yUK2giNmTR"
   },
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o86EEabpNsqR"
   },
   "source": [
    "Finalmente, se realizó el entrenamiento. Esto se realizó con cada dataset, almacenando los parámetros resultantes de cada uno (variando tamaños de los embeddings y de lote) en /data/models.\n",
    "\n",
    "La siguiente función permite entrenar con múltiples tamaños de embedding y de lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple(datasets_path, models_path, lr, num_epochs, embed_sizes, batch_sizes):\n",
    "    datasets_files = get_filenames(datasets_path)\n",
    "\n",
    "    pathlib.Path(models_path).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "    for f in datasets_files:\n",
    "        ds = ISIDataset(datasets_path + \"/\" + f + \".pkl\")\n",
    "        for bs in batch_sizes:\n",
    "            for es in embed_sizes:\n",
    "                print(\"\\033[94mEntrenando modelo \" + f + \"-\" + str(bs) + \"-\" + str(es) + \"\\033[0m\")\n",
    "                dl = torch.utils.data.DataLoader(ds, bs, shuffle=True, collate_fn=collate_batch)\n",
    "                isinet = SkipGram(vocabulary, es)\n",
    "                train(isinet, dl, lr, num_epochs, try_gpu())\n",
    "                torch.save(isinet.state_dict(), models_path + \"/\" + f + \"-\" + str(bs) + \"-\" + str(es) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QimpOHcN2XJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mEntrenando modelo dataset-100-256-256\u001b[0m\n",
      "loss 0.355, 13760.1 tokens/sec on cpu\n",
      "loss 0.334, 27285.5 tokens/sec on cpu\n",
      "loss 0.326, 41083.0 tokens/sec on cpu\n",
      "loss 0.321, 53759.6 tokens/sec on cpu\n",
      "loss 0.318, 64441.9 tokens/sec on cpu\n",
      "loss 0.316, 75401.6 tokens/sec on cpu\n",
      "loss 0.314, 90294.8 tokens/sec on cpu\n",
      "loss 0.313, 108025.6 tokens/sec on cpu\n",
      "loss 0.312, 122884.8 tokens/sec on cpu\n",
      "loss 0.311, 140116.1 tokens/sec on cpu\n",
      "\u001b[94mEntrenando modelo dataset-100-256-512\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[188]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m dl = torch.utils.data.DataLoader(ds, bs, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_batch)\n\u001b[32m     18\u001b[39m isinet = SkipGram(vocabulary, es)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43misinet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m torch.save(isinet.state_dict(), models_path + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + f + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(bs) + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(es) + \u001b[33m\"\u001b[39m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[182]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(net, data_iter, lr, num_epochs, device)\u001b[39m\n\u001b[32m     18\u001b[39m pred = net(center, context_negative)\n\u001b[32m     19\u001b[39m l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n\u001b[32m     20\u001b[39m          / mask.sum(axis=\u001b[32m1\u001b[39m) * mask.shape[\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     23\u001b[39m L += l.sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lr = 0.002\n",
    "num_epochs = 10\n",
    "\n",
    "embed_sizes = [256, 512]\n",
    "batch_sizes = [256]\n",
    "\n",
    "train_multiple(datasets_path, models_path, lr, num_epochs, embed_sizes, batch_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FATzuHy4N5Bo"
   },
   "source": [
    "Para verificar la funcionalidad final que buscamos en el proyecto, se planteó la siguiente función, que devuelve los k conceptos cuyos embeddings se encuentran más cerca a cierto concepto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "mUEb4ATSOFNs"
   },
   "outputs": [],
   "source": [
    "def get_related_concepts(concept_ix, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[torch.tensor(concept_ix)]\n",
    "\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
    "                                      torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "\n",
    "    related = []\n",
    "    for i in topk[1:]:\n",
    "        related.append(vocabulary[i])\n",
    "    return related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinet = SkipGram(vocabulary, 256)\n",
    "isinet.load_state_dict(torch.load(models_path + '/dataset-100-256-256.pt',weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U7YBaVVOGVX"
   },
   "source": [
    "Algunos ejemplos de la misma serían:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adehXDcCOF_D"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('datos', 'digitales'),\n",
       " ('modulación',),\n",
       " ('banda', 'voz'),\n",
       " ('velocidad', 'transmisión'),\n",
       " ('frecuencias',),\n",
       " ('comunicación', 'datos'),\n",
       " ('ruido',),\n",
       " ('errores',),\n",
       " ('señales',),\n",
       " ('circuito',)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('transmisión', 'datos')\n",
    "get_related_concepts(80, 10, isinet.central_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('número', 'secuencia'),\n",
       " ('entidad', 'transporte'),\n",
       " ('tamaño', 'ventana'),\n",
       " ('transporte',),\n",
       " ('bytes',),\n",
       " ('confirmación', 'recepción'),\n",
       " ('datagrama', 'ip'),\n",
       " ('número', 'puerto'),\n",
       " ('capa', 'transporte'),\n",
       " ('tamaño',)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('segmento',)\n",
    "get_related_concepts(72, 10, isinet.central_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('control', 'congestión'),\n",
       " ('tamaño', 'ventana'),\n",
       " ('datos', 'fiable'),\n",
       " ('conexión', 'tcp'),\n",
       " ('capa', 'transporte'),\n",
       " ('transferencia', 'datos', 'fiable'),\n",
       " ('tasa', 'transferencia'),\n",
       " ('transporte',),\n",
       " ('protocolo', 'transporte'),\n",
       " ('tcp', 'udp')]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ('control', 'congestión', 'tcp')\n",
    "get_related_concepts(376, 10, isinet.central_embedding)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkOFf8bDH8q9ApCjhlpGB6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
